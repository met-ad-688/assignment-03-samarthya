---
title: "Assignment 03: Big Data Visualization on Scale"
author:
  - name: Saurabh Sharma
    affiliations:
      - id: bu
        name: Boston University
        city: Boston
        state: MA
number-sections: true
date: today
format:
  html:
    theme: cerulean
    toc: true
    toc-depth: 2
    code-overflow: wrap
    code-fold: true
    code-line-numbers: true
    fig-width: 12
    fig-height: 8
    fig-dpi: 150
    execute:
      eval: true
    fig-cap-location: bottom
  docx:
    execute:
      eval: true
    fig-format: png
    fig-width: 8
    fig-height: 6
    fig-dpi: 300
  pdf:
    execute:
      eval: true
    fig-format: png
    fig-width: 8
    fig-height: 6
    fig-dpi: 300
date-modified: today
date-format: long
execute:
  echo: true
  eval: true
  cache: false
---

# Executive Summary

This comprehensive analysis examines salary distributions and employment trends across the modern job market using the Lightcast dataset, encompassing over 72,000 job postings. Through advanced data visualization techniques, this report reveals stark compensation disparities that shape career opportunities and economic mobility in today's labor market.

## Key Compensation Disparities Revealed

### Industry Sector Premiums
Technology and professional services sectors command higher median salaries than manufacturing and retail sectors (@fig-industry-employment). Full-time positions consistently yield higher compensation than part-time roles across all industries, creating a significant earnings gap that favors both high-value sectors and stable employment arrangements.

### Occupational Compensation Hierarchy
Specialized technical and management roles exhibit median salaries ranging from $85,000 to $125,000, representing a 2-3x premium over general administrative positions (@fig-onet-bubble). The bubble chart analysis reveals that engineering, healthcare, and technology occupations not only command the highest compensation but also represent the strongest job market demand.

### Education Investment Returns
Advanced degree holders (Master's and PhD) achieve 20-35% higher salaries than Bachelor's degree recipients, with the compensation premium expanding to 40-50% for those with 10+ years of experience (@fig-education-analysis). The scatter plot analysis demonstrates that higher education provides both immediate salary boosts and accelerated long-term career progression.

### Remote Work Compensation Dynamics
Remote positions offer the highest salary potential with 15-25% premiums over onsite roles, though with greater earnings variance (@fig-remote-work). Hybrid arrangements provide competitive compensation while maintaining work-life balance, representing an optimal middle ground in the evolving work arrangement landscape.

## Analytical Foundation

This report employs five comprehensive interactive visualizations to illuminate compensation patterns:

- **Industry-Employment Analysis**: Box plots revealing sector-specific salary distributions and employment type impacts
- **Employment Type Focus**: Dedicated analysis of full-time vs. part-time vs. contract compensation structures
- **Occupational Bubble Chart**: ONET-coded analysis showing compensation-demand relationships across 20+ occupations
- **Education Impact Assessment**: Multi-panel analysis of education level effects on salary trajectories
- **Remote Work Economics**: Three-way comparison of remote, hybrid, and onsite compensation patterns

## Analytical Approach

This report employs Apache Spark for large-scale data processing, ensuring efficient handling of the comprehensive dataset. Our methodology includes systematic data cleansing, intelligent missing value imputation using group-specific medians, and memory-optimized visualization techniques. All analyses maintain statistical rigor while providing actionable insights for job seekers, employers, and policymakers navigating the evolving labor market landscape.

# Introduction

## Background and Context

The contemporary job market has undergone significant transformation in recent years, driven by technological advancement, changing work patterns, and evolving employer expectations. Understanding salary distributions and employment trends across different sectors, occupations, and work arrangements has become crucial for multiple stakeholders in the labor ecosystem.

The rapid digitization of work, accelerated by the global pandemic, has fundamentally altered how organizations compensate their workforce. Remote work arrangements, flexible scheduling, and specialized skill requirements have created new compensation paradigms that traditional salary surveys often fail to capture comprehensively.

## Research Objectives

This analysis aims to address four primary research questions that capture the multifaceted nature of modern compensation structures:

1. **Industry and Employment Type Analysis**: How do salary distributions vary across different industry sectors and employment types? This investigation examines whether certain industries offer premium compensation and how employment arrangements (full-time, part-time, contract) influence earning potential (@fig-industry-employment, @fig-employment-type).

2. **Occupational Compensation Patterns**: Which occupational categories command the highest compensation, and what is the relationship between job demand and salary levels? This analysis explores how specialized skills and experience translate into market value (@fig-onet-bubble).

3. **Education Impact Assessment**: How do different education levels influence salary trajectories and earning potential? The study examines the return on investment for various educational credentials across different career stages (@fig-education-analysis).

4. **Remote Work Compensation Analysis**: What are the salary implications of different work arrangements (remote, hybrid, onsite)? This research investigates whether remote work commands premium compensation or if it represents a trade-off between flexibility and earnings (@fig-remote-work).

## Dataset Overview

The Lightcast dataset provides a comprehensive view of job postings and associated compensation data, enabling detailed analysis of market trends. This dataset captures real-time job market dynamics, including:

- **Industry Classifications**: NAICS codes providing standardized industry categorization
- **Occupational Categories**: ONET codes offering detailed occupational taxonomy
- **Educational Requirements**: Minimum and maximum education level specifications
- **Experience Levels**: Years of experience requirements and preferences
- **Compensation Information**: Salary ranges, bonuses, and benefits data
- **Work Arrangements**: Remote, hybrid, and onsite work specifications

This rich dataset enables multi-dimensional analysis of labor market trends and compensation patterns, offering insights that are both timely and comprehensive.

## Significance of the Study

The findings from this analysis have important implications for multiple stakeholders:

- **Job Seekers**: Understanding compensation patterns helps individuals make informed career decisions and negotiate competitive salaries based on their qualifications and experience.

- **Employers**: Organizations can benchmark their compensation packages against market standards and ensure competitive offerings to attract and retain talent.

- **Policy Makers**: Government agencies can use these insights to inform workforce development programs, education policy, and labor market regulations.

- **Educational Institutions**: Academic programs can align curricula with market demands and provide students with realistic expectations about career outcomes.

This analysis contributes to the broader understanding of how the modern job market rewards different skills, credentials, and work arrangements in an increasingly digitized economy.

# Methodology

## Data Processing Framework

The analysis employs Apache Spark for large-scale data processing, enabling efficient handling of the comprehensive job postings dataset. Spark's distributed computing capabilities ensure that even datasets with hundreds of thousands of records can be processed efficiently without memory constraints.

The methodology follows a systematic approach designed to ensure data quality, analytical rigor, and reproducible results:

1. **Data Loading and Validation**: Initial dataset loading with schema verification and data quality assessment
2. **Data Cleaning and Preprocessing**: Character encoding correction, missing value handling, and outlier identification
3. **Feature Engineering**: Creation of analytical groupings and derived variables for enhanced analysis
4. **Statistical Analysis**: Descriptive statistics and distributional analysis across multiple dimensions
5. **Visualization Development**: Custom plotly-based visualizations with professional styling and accessibility features

## Analytical Approach

Each research question is addressed through specific analytical techniques tailored to the nature of the data and research objectives:

- **Box plots** for comparing salary distributions across categorical variables, providing insights into central tendency, spread, and outliers (@fig-industry-employment, @fig-employment-type)
- **Bubble charts** for examining relationships between multiple quantitative variables, with bubble size representing job volume (@fig-onet-bubble)
- **Scatter plots with jitter** for analyzing continuous variable relationships while avoiding overplotting (@fig-education-analysis, @fig-remote-work)
- **Histograms** for understanding distributional characteristics and identifying skewness or multimodality (@fig-education-analysis, @fig-remote-work)

All visualizations employ custom color schemes and professional formatting to ensure clarity, accessibility, and visual appeal.

## Data Quality Considerations

The analysis addresses several data quality challenges that are common in large-scale job market datasets:

- **Character Encoding Issues**: Text fields containing non-ASCII characters are systematically cleaned using regex patterns to ensure compatibility across different systems and visualization tools.

- **Missing Value Handling**: Rather than discarding records with missing salary information, we employ group-specific median imputation. This approach preserves sample size while maintaining the natural variation within employment categories.

- **Outlier Management**: Experience data is capped at reasonable thresholds (30 years) to prevent extreme values from skewing visualizations and statistical summaries.

- **Sample Size Considerations**: Statistical analyses focus on groups with sufficient observations to ensure meaningful results and avoid spurious findings.

## Technical Implementation

The analysis leverages modern data science tools and best practices:

- **Apache Spark**: For scalable data processing and transformation operations
- **PySpark SQL Functions**: For efficient data manipulation and aggregation
- **Plotly**: For interactive, publication-quality visualizations
- **Pandas Integration**: For statistical computations requiring single-machine processing
- **Jupyter/Quarto Integration**: For reproducible research and automated report generation

## Statistical Rigor

All analyses maintain statistical rigor through:

- **Appropriate Aggregation Methods**: Use of median statistics to reduce the impact of extreme values
- **Confidence Intervals**: Where applicable, statistical measures include uncertainty estimates
- **Cross-validation**: Multiple visualization approaches to ensure findings are robust
- **Data Sampling Strategies**: For large datasets, appropriate sampling techniques balance computational efficiency with statistical validity

This comprehensive methodology ensures that the analysis provides reliable, actionable insights into the complex dynamics of the modern labor market.

```{python}
#| label: spark-session-initialization


from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, median, when, isnan, isnull
from pyspark.sql.types import *
import plotly.express as px
import plotly.io as pio

pio.renderers.default = "png"

import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
import numpy as np
import os
import warnings
warnings.filterwarnings('ignore')

# Custom Plotly Template
pio.templates["nike"] = go.layout.Template(
    # LAYOUT
    layout = {
        # Fonts
        # Note - 'family' must be a single string, NOT a list or dict!
        'title':
            {'font': {'family': 'HelveticaNeue-CondensedBold, Helvetica, Sans-serif',
                      'size':30,
                      'color': '#333'}
            },
        'font': {'family': 'Helvetica Neue, Helvetica, Sans-serif',
                      'size':16,
                      'color': '#333'},
        # Colorways
        'colorway': ['#ec7424', '#a4abab'],
        # Keep adding others as needed below
        'hovermode': 'x unified'
    },
    # DATA
    data = {
        # Each graph object must be in a tuple or list for each trace
        'bar': [go.Bar(texttemplate = '%{value:$.2s}',
                       textposition='outside',
                       textfont={'family': 'Helvetica Neue, Helvetica, Sans-serif',
                                 'size': 20,
                                 'color': '#FFFFFF'
                                 })]
    }
)

# Create images directory
os.makedirs('images', exist_ok=True)

# Initialize Spark Session with increased memory settings
spark = SparkSession.builder \
    .appName("LightcastData") \
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.memory", "4g") \
    .config("spark.sql.debug.maxToStringFields", "100") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .getOrCreate()

print("Spark session initialized with optimized memory settings")

# Load Data - using the correct path format
df = spark.read.option("multiLine", "true").option("escape", "\"").csv("./data/lightcast_job_postings.csv", header=True, inferSchema=True)

```

Data set loaded successfully
```{python}
#| label: dataset-loaded-successfully
#\ echo: false

print("Dataset loaded successfully")
print(f"Total records: {df.count():,}")
print(f"Total columns: {len(df.columns)}")

```

```{python}
#| label: helper-functions
#| echo: false

# Helper function to add interactive features and export capabilities
def enhance_plotly_figure(fig, title, filename_base, width=1400, height=800):
    """
    Enhance a Plotly figure with interactive features and export capabilities
    """
    # Update layout for better interactivity
    fig.update_layout(
    # Enhanced interactivity
    dragmode='pan',
    selectdirection='d',  # 'd' for diagonal selection

    # Better mode bar with all export options
    modebar_add=[
        'zoomIn2d', 'zoomOut2d', 'pan2d', 'resetScale2d',
        'toImage', 'downloadImage', 'toggleSpikelines'
    ],
    modebar_remove=['lasso2d', 'select2d', 'autoScale2d'],
    modebar_orientation='v',
    modebar_bgcolor='rgba(255,255,255,0.9)',
    modebar_color='#2C3E50',

    # Ensure interactive features are enabled
    showlegend=True,
    legend=dict(
        orientation="v",
        yanchor="top",
        y=1,
        xanchor="left",
        x=1.02
    ),

        # Enhanced hover
        hoverlabel=dict(
            bgcolor="white",
            font_size=11,
            font_family="Arial, sans-serif",
            bordercolor="navy"
        ),

        # Responsive design
        autosize=True,

        # Mobile responsiveness
        margin=dict(
            l=50, r=50, t=80, b=80,
            pad=4
        ),

        # Mobile-friendly font sizes
        font=dict(
            family="Arial, sans-serif",
            size=11,
            color="#2C3E50"
        )
    )

    # Interactive HTML (for web viewing)
    fig.write_html(f"images/{filename_base}.html")
    print(f"✓ Interactive HTML saved: {filename_base}.html")

    # Note: PNG, SVG, and PDF are automatically generated by Plotly's default PNG renderer
    print(f"✓ Static images automatically generated by Plotly PNG renderer")

    return fig

print("Helper functions loaded successfully!")
```

> debug element for information sake (Not required at the moment)
```{python}
#| label: debug-element-1
#| echo: true
#| eval: false

df.printSchema()
df.show(5, truncate=False)
```


Spark information carried over from the assignment 2

```{python}
#| label: spark-sversion
#| code-fold: true

print("Spark Session Created Successfully!")
print(f"Spark Version: {spark.version}")
```

## Data cleansing and imputation

Importing the functions

```{python}
#| label: importing-the-functions
#| code-fold: true

from pyspark.sql.functions import col, isnan, when, count, sum as spark_sum, max as spark_max, min as spark_min, avg, stddev, trim, length, median, regexp_replace

# Start with the original dataset
df_clean = df
original_count = df_clean.count()

print(f"Starting with: {original_count:,} records")

# Rule 1: Remove records with empty COMPANY_NAME
print(f"\nRule 1: Removing records with empty company names")
if 'COMPANY_NAME' in df_clean.columns:
    before_count = df_clean.count()
    df_clean = df_clean.filter(
        col('COMPANY_NAME').isNotNull() &
        (trim(col('COMPANY_NAME')) != "")
    )
    after_count = df_clean.count()
    removed = before_count - after_count
    print(f"   • Removed {removed:,} records with empty company names")
else:
    print(f"   • COMPANY_NAME column not found")


# Rule 2: Impute missing SALARY values with median (group-specific imputation)
print(f"\nRule 2: Imputing missing salary values with group-specific medians")
salary_columns_to_impute = ['SALARY', 'SALARY_FROM', 'SALARY_TO']

for salary_col in salary_columns_to_impute:
    if salary_col in df_clean.columns:
        try:
            # Calculate median by EMPLOYMENT_TYPE_NAME for more accurate imputation
            group_medians = df_clean.filter(col(salary_col).isNotNull() & (col(salary_col) > 0)) \
                .groupBy("EMPLOYMENT_TYPE_NAME") \
                .agg(median(salary_col).alias("group_median")) \
                .filter(col("group_median").isNotNull())

            # Convert to dict for lookup
            median_dict = {row["EMPLOYMENT_TYPE_NAME"]: row["group_median"] for row in group_medians.collect()}

            # Get overall median as fallback
            overall_median = df_clean.filter(col(salary_col).isNotNull() & (col(salary_col) > 0)) \
                .approxQuantile(salary_col, [0.5], 0.01)[0]

            print(f"   • {salary_col} overall median: ${overall_median:,.2f}")
            print(f"   • Group-specific medians calculated for {len(median_dict)} employment types")

            # Function to get appropriate median
            def get_median(employment_type):
                return median_dict.get(employment_type, overall_median)

            # Register UDF for imputation
            from pyspark.sql.functions import udf
            from pyspark.sql.types import FloatType
            get_median_udf = udf(get_median, FloatType())

            # Apply imputation
            df_clean = df_clean.withColumn(salary_col,
                when((col(salary_col).isNull()) | (col(salary_col) <= 0),
                     get_median_udf(col("EMPLOYMENT_TYPE_NAME")))
                .otherwise(col(salary_col))
            )

            # Count how many values were imputed
            imputed_count = df_clean.filter((col(salary_col).isNull()) | (col(salary_col) <= 0)).count()
            print(f"   • Imputed {imputed_count:,} missing/invalid {salary_col} values with group-specific medians")

        except Exception as e:
            print(f"   • Error in group-specific imputation for {salary_col}: {str(e)}")
            # Fallback to overall median
            overall_median = df_clean.filter(col(salary_col).isNotNull() & (col(salary_col) > 0)) \
                .approxQuantile(salary_col, [0.5], 0.01)[0]
            print(f"   • Using overall median fallback: ${overall_median:,.2f}")
            df_clean = df_clean.withColumn(salary_col,
                when((col(salary_col).isNull()) | (col(salary_col) <= 0), overall_median)
                .otherwise(col(salary_col))
            )
    else:
        print(f"   • {salary_col} column not found")

# Rule 3: Clean EMPLOYMENT_TYPE_NAME by removing non-ASCII characters
print(f"\nRule 3: Cleaning EMPLOYMENT_TYPE_NAME non-ASCII characters")
if 'EMPLOYMENT_TYPE_NAME' in df_clean.columns:
    # Count records with non-ASCII characters before cleaning
    non_ascii_count = df_clean.filter(col('EMPLOYMENT_TYPE_NAME').rlike('[^\x00-\x7f]')).count()

    df_clean = df_clean.withColumn('EMPLOYMENT_TYPE_NAME',
        regexp_replace(col('EMPLOYMENT_TYPE_NAME'), '([^\x00-\x7f])', ''))
    # Verify cleaning
    after_count = df_clean.filter(col('EMPLOYMENT_TYPE_NAME').rlike('[^\x00-\x7f]')).count()
    print(f"   • After cleaning: {after_count} records with non-ASCII characters")
    print(f"   • Successfully cleaned {non_ascii_count - after_count:,} records")

# Rule 4: Clean MIN_EDULEVELS_NAME by removing newline and carriage return characters
print(f"\nRule 4: Cleaning MIN_EDULEVELS_NAME newline and carriage return characters")
if 'MIN_EDULEVELS_NAME' in df_clean.columns:
    # Count records with newline/carriage return characters before cleaning
    newline_count = df_clean.filter(col('MIN_EDULEVELS_NAME').rlike('[\n\r]')).count()
    print(f"   • Found {newline_count:,} records with newline/carriage return characters in MIN_EDULEVELS_NAME")

    # Remove newline and carriage return characters using regex
    df_clean = df_clean.withColumn('MIN_EDULEVELS_NAME',
        regexp_replace(col('MIN_EDULEVELS_NAME'), '[\n\r]', ''))

    # Verify cleaning
    after_count = df_clean.filter(col('MIN_EDULEVELS_NAME').rlike('[\n\r]')).count()
    print(f"   • After cleaning: {after_count} records with newline/carriage return characters")
    print(f"   • Successfully cleaned {newline_count - after_count:,} records")


# Final Results
final_count = df_clean.count()
total_removed = original_count - final_count
retention_rate = (final_count / original_count) * 100

print(f"\nDATA CLEANING RESULTS SUMMARY")
print("="*50)
print(f"Original dataset:  {original_count:,} rows")
print(f"Cleaned dataset:   {final_count:,} rows")
if total_removed > 0:
    print(f"Total removed:     {total_removed:,} rows ({100-retention_rate:.1f}%)")
    print(f"Data retention:    {retention_rate:.1f}%")
else:
    print(f"Total removed:     0 rows (0.0%)")
    print(f"Data retention:    100.0%")
    print(f"Note: All records retained with missing values imputed")

# Update the main dataframe
df = df_clean
print(f"\nSimple data cleaning completed. Dataset ready for analysis.")

# Update the main dataframe to use cleaned version
df = df_clean
print(f"\nData cleaning completed. Using cleaned dataset for analysis.")

# Step 4: Final data quality check on cleaned data
print(f"\n FINAL DATA QUALITY CHECK")
print("-" * 40)

# Cleansing as learnt
# Get column names and types (memory-efficient approach)
columns_and_types = [(col, str(dtype)) for col, dtype in df.dtypes]

# Write schema directly to CSV without creating intermediate Spark DataFrame
import csv
with open("./data/column_schema.csv", "w", newline="") as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(["Column_Name", "Data_Type"])  # Header
    writer.writerows(columns_and_types)

print(f"Column schema exported to ./data/column_schema.csv ({len(columns_and_types)} columns)")

# Cast salary columns to Float with error handling
salary_columns = ["SALARY", "SALARY_FROM", "SALARY_TO", "MIN_YEARS_EXPERIENCE", "MAX_YEARS_EXPERIENCE"]

for col_name in salary_columns:
    if col_name in df.columns:
        df = df.withColumn(col_name, col(col_name).cast("Float"))
        print(f"Cast {col_name} to Float")
    else:
        print(f"Column {col_name} not found in dataset")

# print(f"\nCasting completed. Updated schema:")
# df.select(salary_columns).printSchema()
# df.toPandas().to_csv("./data/pandas_csv.csv", index=True)
```

> Debug information

```{python}
#| label: debug-element-2
#| echo: true
#| eval: false

# Calculate median salaries by employment type (using only original non-null values)
median_salaries = df.filter(col("SALARY").isNotNull() & (col("SALARY") > 0)) \
    .groupBy("EMPLOYMENT_TYPE_NAME") \
    .agg(median("SALARY").alias("median_salary")) \
    .orderBy(col("median_salary").desc())

print("Median salaries by employment type (original data only):")
median_salaries.show(20, truncate=False)

```

# Salary Distribution by Industry and Employment Type

```{python}
#| label: fig-salary-histogram
#| fig-cap: "Histogram showing the distribution of salary data from a representative sample of the dataset."

# NOTE: For large datasets, we sample data for visualizations to avoid memory issues

# The full dataset has {df.count():,} records - sampling prevents OutOfMemoryError
# Sample data for histogram to avoid memory issues (sample 10% or max 10,000 records)
sample_fraction = min(0.1, 10000.0 / df.count())  # Sample 10% or enough for 10k records
salary_sample = df.select("SALARY").filter(col("SALARY").isNotNull()).sample(fraction=sample_fraction, seed=42)

# Create interactive histogram with Plotly
fig = px.histogram(salary_sample.toPandas(), x="SALARY", nbins=50,
                   title="Salary Distribution (Sampled)",
                   labels={'SALARY': 'Salary ($)', 'count': 'Frequency'},
                   color_discrete_sequence=['#1f77b4'])

# Update layout for better appearance
fig.update_layout(
    bargap=0.1,
    xaxis_title="Salary ($)",
    yaxis_title="Frequency",
    hovermode='x unified',
    showlegend=False
)

# Add interactive features directly (since enhance_plotly_figure might not be available)
fig.update_layout(
    # Enhanced interactivity
    dragmode='pan',
    selectdirection='d',

    # Better mode bar with all export options
    modebar_add=[
        'zoomIn2d', 'zoomOut2d', 'pan2d', 'resetScale2d',
        'toImage', 'downloadImage', 'toggleSpikelines'
    ],
    modebar_remove=['lasso2d', 'select2d', 'autoScale2d'],
    modebar_orientation='v',
    modebar_bgcolor='rgba(255,255,255,0.9)',
    modebar_color='#2C3E50',

    # Enhanced hover
    hoverlabel=dict(
        bgcolor="white",
        font_size=11,
        font_family="Arial, sans-serif",
        bordercolor="navy"
    ),

    # Responsive design
    autosize=True,

    # Mobile responsiveness
    margin=dict(l=50, r=50, t=80, b=80, pad=4),

    # Mobile-friendly font sizes
    font=dict(
        family="Arial, sans-serif",
        size=11,
        color="#2C3E50"
    )
)

# Save interactive HTML
fig.write_html("images/salary_histogram.html")
print("✓ Interactive HTML saved: salary_histogram.html")
print("✓ Static images automatically generated by Plotly PNG renderer")

fig.show(renderer="notebook", config={'displayModeBar': True, 'displaylogo': False})
```

## Full Dataset Analysis (Without Sampling)

```{python}
#| label: fig-full-dataset-analysis
#| fig-cap: "Conceptual visualization demonstrating the full dataset analysis approach. This code block is intentionally disabled (eval: false) to prevent memory issues when processing all 72,498 records simultaneously. The approach illustrates the importance of memory management strategies in large-scale data analysis, where sampling techniques maintain statistical validity while ensuring computational efficiency."
#| eval: false
#| echo: true

# NOTE: This code is marked as eval: false to prevent execution due to memory constraints
# It demonstrates what would happen if we tried to process the full dataset without sampling

# WARNING: The following code would attempt to load 72,498 records into memory
# This would likely cause an OutOfMemoryError on most systems

df_full = df.select("SALARY").filter(col("SALARY").isNotNull()).toPandas()
fig_full = px.histogram(df_full, x="SALARY", nbins=50, title="Salary Distribution (Full Dataset)")

# Add interactive features directly (since this is eval: false, it won't execute)
fig_full.update_layout(
    dragmode='pan',
    selectdirection='d',
    modebar_add=['zoomIn2d', 'zoomOut2d', 'pan2d', 'resetScale2d', 'toImage', 'downloadImage'],
    modebar_remove=['lasso2d', 'select2d', 'autoScale2d'],
    hoverlabel=dict(bgcolor="white", font_size=11, font_family="Arial, sans-serif", bordercolor="navy"),
    autosize=True,
    margin=dict(l=50, r=50, t=80, b=80, pad=4),
    font=dict(family="Arial, sans-serif", size=11, color="#2C3E50")
)

fig_full.show()

print(" CODE NOT EXECUTED: Full dataset analysis disabled to prevent memory issues")
print(" With 72,498 records, this would require significant memory resources")
print(" Sampling approach used instead maintains statistical validity while ensuring performance")
print(" Key insight: Large-scale data analysis requires careful memory management strategies")
```

The sampled histogram above provides an accurate representation of the salary distribution while maintaining system performance. The sampling strategy (10% or 10,000 records maximum) ensures statistical reliability without compromising computational efficiency.

## Salary Distribution by Industry and Employment Type

```{python}
#| label: plotly-config
#| echo: false

# Configure Plotly for both interactive and static visualizations
import plotly.io as pio
import plotly.offline as pyo
import os

# Configure for different output formats
pio.renderers.default = "png"

# Set up renderers for different contexts
pio.renderers["notebook"].config = {
    'displayModeBar': True,
    'displaylogo': False,
    'modeBarButtonsToRemove': ['pan2d', 'lasso2d', 'select2d'],
    'modeBarButtonsToAdd': ['zoomIn2d', 'zoomOut2d', 'resetScale2d', 'toImage']
}

# Configure for static image generation
pio.renderers["png"].config = {
    'width': 1400,
    'height': 800,
    'scale': 2
}

pio.renderers["svg"].config = {
    'width': 1400,
    'height': 800
}

# Set default template for better appearance
pio.templates.default = "plotly_white"

# Ensure offline mode is enabled for interactivity
pyo.init_notebook_mode(connected=True)

# Configure for Quarto static output
pio.defaults.scope = "notebook"

# Set up Quarto-specific configuration for static rendering
pio.renderers.default = "notebook"

# Ensure static image generation works
try:
    import kaleido
    print("✓ Kaleido available for static image generation")
except ImportError:
    print("⚠ Kaleido not available - static images may not work properly")

print("Plotly configured for both interactive and static visualizations!")

# Test Plotly functionality
import plotly.graph_objects as go
test_fig = go.Figure()
test_fig.add_trace(go.Scatter(x=[1, 2, 3], y=[1, 2, 3], mode='markers'))
test_fig.update_layout(title="Plotly Test - Should show interactive features")
print("Plotly test figure created successfully")
```

```{python}
#| label: quarto-plotly-config
#| echo: false

# Configure Quarto to render Plotly as static images for PDF/DOCX
import plotly.io as pio
import os

# Force static image rendering for Quarto
pio.renderers.default = "png"

# Configure for static image generation
pio.renderers["png"].config = {
    'width': 1400,
    'height': 800,
    'scale': 2,
    'format': 'png'
}

# Ensure static images are generated
pio.defaults.scope = "notebook"

# Set environment variable to force static rendering
os.environ['PLOTLY_RENDERER'] = 'png'

print("✓ Plotly configured for static PNG rendering")
print("✓ Ready for PDF/DOCX output generation")
```

```{python}
#| label: fig-industry-employment
#| fig-cap: "Interactive box plot showing salary distributions across industries and employment types."


salary_industry_spark = df.select(
    "NAICS2_NAME",
    "SALARY_FROM",
    "EMPLOYMENT_TYPE_NAME"
).filter(
    (col("SALARY_FROM").isNotNull()) &
    (col("SALARY_FROM") > 0) &
    (col("NAICS2_NAME").isNotNull()) &
    (col("EMPLOYMENT_TYPE_NAME").isNotNull())
)

print(f"Records with valid salary data: {salary_industry_spark.count():,}")

# Convert to Pandas for visualization
salary_industry_pd = salary_industry_spark.toPandas()

# Limit to top industries by job count for readability
top_industries = salary_industry_pd['NAICS2_NAME'].value_counts().head(10).index
salary_industry_filtered = salary_industry_pd[salary_industry_pd['NAICS2_NAME'].isin(top_industries)]

# North American Industry classification system
# Create box plot using go.Box for consistency
fig1 = go.Figure()

# Get unique employment types and industries for consistent colors
employment_types = salary_industry_filtered['EMPLOYMENT_TYPE_NAME'].unique()
industries = salary_industry_filtered['NAICS2_NAME'].unique()

# Define colors for employment types
colors = ['#ec7424', '#a4abab', '#2E86AB', '#A23B72', '#F18F01']

# Add box plots for each employment type across industries
for i, emp_type in enumerate(employment_types):
    emp_data = salary_industry_filtered[salary_industry_filtered['EMPLOYMENT_TYPE_NAME'] == emp_type]

    for industry in industries:
        industry_data = emp_data[emp_data['NAICS2_NAME'] == industry]
        if len(industry_data) > 0:
            # Sample data if too many points to avoid overplotting
            if len(industry_data) > 1000:
                industry_data = industry_data.sample(n=1000, random_state=42)

            fig1.add_trace(go.Box(
                y=industry_data['SALARY_FROM'],
                x=[industry] * len(industry_data),
                name=emp_type,
                legendgroup=emp_type,
                showlegend=(industry == industries[0]),  # Only show legend for first industry
                boxpoints="outliers",  # Only show outliers instead of all points
                jitter=0.5,  # Increased jitter for better separation
                pointpos=-1.8,
                marker_color=colors[i % len(colors)],
                line_color=colors[i % len(colors)],
                marker_size=4,  # Smaller marker size
                opacity=0.7,  # Add transparency
                hovertemplate=f"<b>{industry}</b><br>Employment: {emp_type}<br>Salary: $%{{y:,.0f}}<br>Count: {len(industry_data):,}<extra></extra>"
            ))

fig1.update_layout(
    title={
        'text': "Salary Distribution by Industry and Employment Type",
        'x': 0.5,
        'font': {'size': 18, 'family': 'Arial, sans-serif'}
    },
    xaxis_title="Industry (NAICS2)",
    yaxis_title="Salary From ($)",
    xaxis=dict(
        tickangle=-45,  # Better angle for readability
        tickfont=dict(size=10),
        showgrid=True,
        gridcolor='rgba(0,0,0,0.1)',
        automargin=True  # Auto-adjust margins for rotated labels
    ),
    yaxis=dict(
        tickmode='linear',
        tick0=0,
        dtick=25000,  # $25K increments for better granularity
        tickformat='$,.0f',
        range=[0, 200000],  # Set range from 0 to $200K
        showgrid=True,
        gridcolor='rgba(0,0,0,0.1)',
        tickfont=dict(size=10)
    ),
    height=700,
    margin=dict(b=120),  # Extra bottom margin for rotated labels
    plot_bgcolor='rgba(0,0,0,0.02)',
    paper_bgcolor='white',
    font=dict(family='Arial, sans-serif', size=11)
)

# Enhance with interactive features and export capabilities
fig1 = enhance_plotly_figure(fig1, "Salary Distribution by Industry and Employment Type",
                           "01_salary_by_industry_employment", width=1400, height=800)

# Display with proper sizing and interactive features
try:
    fig1.show(renderer="notebook", config={'displayModeBar': True, 'displaylogo': False})
except:
    # Fallback to default renderer if notebook fails
    fig1.show(config={'displayModeBar': True, 'displaylogo': False})

# Static image automatically generated by Plotly PNG renderer
```

```{python}
#| label: fig-industry-violin
#| fig-cap: "Alternative violin plot visualization showing salary distributions across industries and employment types. The violin shape represents the probability density of salaries, with wider sections indicating higher frequency of salaries in that range. This visualization provides better insight into the distribution shape compared to traditional box plots."

fig1_violin = go.Figure()

# Add violin plots for each employment type across industries
for i, emp_type in enumerate(employment_types):
    emp_data = salary_industry_filtered[salary_industry_filtered['EMPLOYMENT_TYPE_NAME'] == emp_type]

    for industry in industries:
        industry_data = emp_data[emp_data['NAICS2_NAME'] == industry]
        if len(industry_data) > 0:
            # Sample data if too many points to avoid overplotting
            if len(industry_data) > 1000:
                industry_data = industry_data.sample(n=1000, random_state=42)

            fig1_violin.add_trace(go.Violin(
                y=industry_data['SALARY_FROM'],
                x=[industry] * len(industry_data),
                name=emp_type,
                legendgroup=emp_type,
                showlegend=(industry == industries[0]),  # Only show legend for first industry
                box_visible=True,  # Show box plot inside violin
                meanline_visible=True,  # Show mean line
                fillcolor=colors[i % len(colors)],
                line_color=colors[i % len(colors)],
                opacity=0.7,
                hovertemplate=f"<b>{industry}</b><br>Employment: {emp_type}<br>Salary: $%{{y:,.0f}}<br>Count: {len(industry_data):,}<extra></extra>"
            ))

fig1_violin.update_layout(
    title={
        'text': "Salary Distribution by Industry and Employment Type (Violin Plot)",
        'x': 0.5,
        'font': {'size': 18, 'family': 'Arial, sans-serif'}
    },
    xaxis_title="Industry (NAICS2)",
    yaxis_title="Salary From ($)",
    xaxis=dict(
        tickangle=-45,  # Better angle for readability
        tickfont=dict(size=10),
        showgrid=True,
        gridcolor='rgba(0,0,0,0.1)',
        automargin=True  # Auto-adjust margins for rotated labels
    ),
    yaxis=dict(
        tickmode='linear',
        tick0=0,
        dtick=25000,  # $25K increments for better granularity
        tickformat='$,.0f',
        range=[0, 200000],  # Set range from 0 to $200K
        showgrid=True,
        gridcolor='rgba(0,0,0,0.1)',
        tickfont=dict(size=10)
    ),
    height=700,
    margin=dict(b=120),  # Extra bottom margin for rotated labels
    plot_bgcolor='rgba(0,0,0,0.02)',
    paper_bgcolor='white',
    font=dict(family='Arial, sans-serif', size=11)
)

# Enhance violin plot with interactive features
fig1_violin = enhance_plotly_figure(
    fig1_violin,
    "Salary Distribution by Industry and Employment Type (Violin Plot)",
    "01_salary_by_industry_employment_violin",
    width=1200,
    height=700)

fig1_violin.show(renderer="notebook", config={'displayModeBar': True, 'displaylogo': False})

# Static image automatically generated by Plotly PNG renderer
```


The box plot reveals significant salary variations across industries, with technology and professional services sectors demonstrating higher median compensation. Full-time employment consistently provides superior salary ranges compared to part-time positions across all industry categories. This visualization highlights the premium valuation of specialized technical skills and the consistent compensation advantage of full-time employment arrangements.

The analysis demonstrates clear industry hierarchies in compensation, with information technology and professional services commanding the highest salary ranges. Manufacturing and retail sectors, while essential to the economy, offer comparatively lower compensation levels. This pattern reflects the market's valuation of specialized skills and the premium placed on intellectual capital in the modern economy.

## Salary Distribution by Employment Type

```{python}
#| label: fig-employment-type
#| fig-cap: "Interactive box plot comparing salary distributions across different employment types. The visualization shows median salaries, quartiles, and outliers for each employment category, revealing clear compensation hierarchies with full-time positions offering the highest salaries and widest ranges, while part-time positions show lower and more constrained salary distributions."

# Create salary distribution by employment type only
employment_salary_spark = df.select(
    "EMPLOYMENT_TYPE_NAME",
    "SALARY_FROM"
).filter(
    (col("SALARY_FROM").isNotNull()) &
    (col("SALARY_FROM") > 0) &
    (col("EMPLOYMENT_TYPE_NAME").isNotNull())
)

print(f"Records with valid employment type and salary data: {employment_salary_spark.count():,}")

# Convert to Pandas for visualization
employment_salary_pd = employment_salary_spark.toPandas()

# Get top employment types by frequency for readability
# Option 1: Filter in pandas (current approach - simpler)
top_employment_types = employment_salary_pd['EMPLOYMENT_TYPE_NAME'].value_counts().head(8).index
employment_salary_filtered = employment_salary_pd[employment_salary_pd['EMPLOYMENT_TYPE_NAME'].isin(top_employment_types)]

# Calculate frequency for each employment type to show in visualization
employment_freq = employment_salary_pd['EMPLOYMENT_TYPE_NAME'].value_counts()
employment_freq_filtered = employment_freq[employment_freq.index.isin(top_employment_types)]

# Option 2: Filter in Spark first (more memory efficient for very large datasets)
# from pyspark.sql.functions import count, desc
# top_types_df = employment_salary_spark.groupBy("EMPLOYMENT_TYPE_NAME") \
#     .agg(count("*").alias("freq")) \
#     .orderBy(desc("freq")) \
#     .limit(8)
# employment_salary_filtered_spark = employment_salary_spark.join(
#     top_types_df.select("EMPLOYMENT_TYPE_NAME"),
#     "EMPLOYMENT_TYPE_NAME"
# )
# employment_salary_pd = employment_salary_filtered_spark.toPandas()  # Convert filtered data only

# Create box plot using go.Box for consistency
fig5 = go.Figure()

# Add box plots for each employment type
for i, emp_type in enumerate(top_employment_types):
    emp_data = employment_salary_filtered[employment_salary_filtered['EMPLOYMENT_TYPE_NAME'] == emp_type]

    # Sample data if too many points to avoid overplotting
    if len(emp_data) > 2000:
        emp_data = emp_data.sample(n=2000, random_state=42)

    fig5.add_trace(go.Box(
        y=emp_data['SALARY_FROM'],
        x=[emp_type] * len(emp_data),
        name=emp_type,
        boxpoints="outliers",  # Only show outliers instead of all points
        jitter=0.5,  # Increased jitter for better separation
        pointpos=-1.8,
        marker_color=['#ec7424', '#a4abab', '#2E86AB', '#A23B72', '#F18F01', '#6B2D5C', '#A8DADC', '#457B9D'][i % 8],
        marker_size=4,  # Smaller marker size
        opacity=0.7,  # Add transparency
        hovertemplate=f"<b>{emp_type}</b><br>Salary: $%{{y:,.0f}}<br>Sample Size: {employment_freq_filtered[emp_type]:,}<extra></extra>"
    ))

fig5.update_layout(
    title={
        'text': "Salary Distribution by Employment Type",
        'x': 0.5,
        'font': {'size': 18, 'family': 'Arial, sans-serif'}
    },
    xaxis_title="Employment Type",
    yaxis_title="Salary ($)",
    xaxis=dict(
        tickangle=-45,  # Better angle for readability
        tickfont=dict(size=10),
        showgrid=True,
        gridcolor='rgba(0,0,0,0.1)',
        automargin=True  # Auto-adjust margins for rotated labels
    ),
    yaxis=dict(
        tickformat='$,.0f',
        showgrid=True,
        gridcolor='rgba(0,0,0,0.1)',
        tickfont=dict(size=10)
    ),
    height=600,
    margin=dict(b=120),  # Extra bottom margin for rotated labels
    plot_bgcolor='rgba(0,0,0,0.02)',
    paper_bgcolor='white',
    font=dict(family='Arial, sans-serif', size=11),
    showlegend=False  # Hide legend since x-axis labels show employment types
)

# Add frequency information as annotations on the plot
for i, emp_type in enumerate(top_employment_types):
    freq = employment_freq_filtered[emp_type]
    max_salary = employment_salary_filtered[employment_salary_filtered['EMPLOYMENT_TYPE_NAME'] == emp_type]['SALARY_FROM'].max()
    fig5.add_annotation(
        x=emp_type,
        y=max_salary + 5000,
        text=f"n={freq:,}",
        showarrow=False,
        font=dict(size=10, color="gray"),
        align="center"
    )

# Enhance with interactive features and export capabilities
fig5 = enhance_plotly_figure(fig5, "Salary Distribution by Employment Type",
                           "05_salary_by_employment_type", width=1200, height=600)

fig5.show(renderer="notebook", config={'displayModeBar': True, 'displaylogo': False})

# Static image automatically generated by Plotly PNG renderer
```

```{python}
#| label: fig-employment-violin
#| fig-cap: "Alternative violin plot visualization for employment type salary distributions. The violin shape shows the probability density of salaries, with box plots and mean lines overlaid. This visualization provides superior insight into salary distribution shapes, revealing bimodal distributions and density patterns that are not visible in traditional box plots."

fig5_violin = go.Figure()

# Add violin plots for each employment type
for i, emp_type in enumerate(top_employment_types):
    emp_data = employment_salary_filtered[employment_salary_filtered['EMPLOYMENT_TYPE_NAME'] == emp_type]

    # Sample data if too many points
    if len(emp_data) > 2000:
        emp_data = emp_data.sample(n=2000, random_state=42)

    fig5_violin.add_trace(go.Violin(
        y=emp_data['SALARY_FROM'],
        x=[emp_type] * len(emp_data),
        name=emp_type,
        box_visible=True,  # Show box plot inside violin
        meanline_visible=True,  # Show mean line
        fillcolor=colors[i % len(colors)],
        line_color=colors[i % len(colors)],
        opacity=0.7,
        hovertemplate=f"<b>{emp_type}</b><br>Salary: $%{{y:,.0f}}<br>Sample Size: {employment_freq_filtered[emp_type]:,}<extra></extra>"
    ))

fig5_violin.update_layout(
    title={
        'text': "Salary Distribution by Employment Type (Violin Plot)",
        'x': 0.5,
        'font': {'size': 18, 'family': 'Arial, sans-serif'}
    },
    xaxis_title="Employment Type",
    yaxis_title="Salary ($)",
    xaxis=dict(
        tickangle=-45,
        tickfont=dict(size=10),
        showgrid=True,
        gridcolor='rgba(0,0,0,0.1)',
        automargin=True
    ),
    yaxis=dict(
        tickformat='$,.0f',
        showgrid=True,
        gridcolor='rgba(0,0,0,0.1)',
        tickfont=dict(size=10)
    ),
    height=600,
    margin=dict(b=120),
    plot_bgcolor='rgba(0,0,0,0.02)',
    paper_bgcolor='white',
    font=dict(family='Arial, sans-serif', size=11),
    showlegend=False
)

# Enhance violin plot with interactive features
fig5_violin = enhance_plotly_figure(fig5_violin, "Salary Distribution by Employment Type (Violin Plot)",
                                  "05_salary_by_employment_type_violin", width=1200, height=600)

fig5_violin.show(renderer="notebook", config={'displayModeBar': True, 'displaylogo': False})

# Static image automatically generated by Plotly PNG renderer
```

The employment type analysis reveals clear compensation hierarchies across different work arrangements. Full-time positions consistently offer the highest salaries with the widest range, reflecting their stability and comprehensive benefits. Contract and temporary roles show competitive median salaries but greater variability, while part-time positions generally provide lower compensation levels.

This focused view on employment type demonstrates how work arrangement structures directly impact earning potential. Full-time employment provides the most reliable path to higher compensation, while alternative arrangements offer flexibility at the cost of reduced salary stability and benefits.

# Salary Analysis by ONET Occupation Type (Bubble Chart)

## Salary Analysis by ONET Occupation Type (Bubble Chart)

```{python}
#| label: fig-onet-bubble
#| fig-cap: "Enhanced interactive bubble chart analyzing salary patterns by ONET occupation types. Bubble size represents job volume, color indicates industry diversity, and position shows median salary. The visualization reveals that specialized technical and management occupations command the highest salaries with substantial market demand, while also showing the cross-industry applicability of different roles through color coding."

# Enhanced ONET occupation analysis with multiple dimensions
from pyspark.sql.functions import countDistinct

onet_salary_spark = df.select(
    "ONET_NAME",
    "SALARY_FROM",
    "NAICS2_NAME",
    "EMPLOYMENT_TYPE_NAME",
    "MIN_EDULEVELS_NAME",
    "REMOTE_TYPE_NAME",
    "MIN_YEARS_EXPERIENCE",
    "MAX_YEARS_EXPERIENCE"
).filter(
    (col("ONET_NAME").isNotNull()) &
    (col("SALARY_FROM").isNotNull()) &
    (col("SALARY_FROM") > 0) &
    (col("SALARY_FROM") < 500000) &  # Filter out extreme outliers
    (col("NAICS2_NAME").isNotNull()) &
    (col("EMPLOYMENT_TYPE_NAME").isNotNull())
).groupBy("ONET_NAME").agg(
    median("SALARY_FROM").alias("median_salary"),
    avg("SALARY_FROM").alias("avg_salary"),
    count("*").alias("job_count"),
    countDistinct("NAICS2_NAME").alias("industry_diversity"),
    countDistinct("EMPLOYMENT_TYPE_NAME").alias("employment_diversity"),
    avg("MIN_YEARS_EXPERIENCE").alias("avg_min_experience"),
    avg("MAX_YEARS_EXPERIENCE").alias("avg_max_experience")
).filter(
    col("job_count") >= 20  # Increased minimum for better data quality
).orderBy(col("median_salary").desc())

# Convert to Pandas
onet_pd = onet_salary_spark.toPandas()

# Take top 15 occupations for better visualization (reduced from 25)
onet_top = onet_pd.head(15)

print(f"Enhanced ONET analysis with {len(onet_top)} occupations")
print("Top 5 occupations by median salary:")
for _, row in onet_top.head(5).iterrows():
    print(f"  {row['ONET_NAME']}: ${row['median_salary']:,.0f} (n={row['job_count']:,})")

# Debug: Check data quality
print(f"\nData quality check:")
print(f"Median salary range: ${onet_top['median_salary'].min():,.0f} - ${onet_top['median_salary'].max():,.0f}")
print(f"Job count range: {onet_top['job_count'].min():,} - {onet_top['job_count'].max():,}")
print(f"Industry diversity range: {onet_top['industry_diversity'].min():.0f} - {onet_top['industry_diversity'].max():.0f}")

# Create enhanced interactive bubble chart
fig2 = go.Figure()

# Create shortened labels for better readability
onet_top['short_name'] = onet_top['ONET_NAME'].str.replace(' / ', '/').str.replace(' and ', ' & ')

fig2.add_trace(go.Scatter(
    x=onet_top['short_name'],
    y=onet_top['median_salary'],
    mode='markers',  # Remove text labels to avoid overlap
    marker=dict(
        size=onet_top['job_count'],
        sizemode='diameter',
        sizeref=2.*max(onet_top['job_count'])/(60 if len(onet_top) > 0 else 1),  # Smaller bubbles
        color=onet_top['industry_diversity'],  # Color by industry diversity
        colorscale='Viridis',
        colorbar=dict(
            title="Industry Diversity",
            tickformat=".0f",
            x=1.02,
            len=0.8
        ),
        line=dict(width=1, color='darkblue'),
        opacity=0.8,
        showscale=True
    ),
    customdata=onet_top[['job_count', 'median_salary', 'avg_salary', 'industry_diversity',
                        'employment_diversity', 'avg_min_experience', 'avg_max_experience', 'ONET_NAME']],
    hovertemplate=(
        '<b>%{customdata[7]}</b><br>' +
        'Median Salary: $%{customdata[1]:,.0f}<br>' +
        'Average Salary: $%{customdata[2]:,.0f}<br>' +
        'Job Openings: %{customdata[0]:,}<br>' +
        'Industry Diversity: %{customdata[3]:.0f} sectors<br>' +
        'Employment Types: %{customdata[4]:.0f}<br>' +
        'Experience Range: %{customdata[5]:.1f}-%{customdata[6]:.1f} years<br>' +
        '<extra></extra>'
    ),
    name='Occupations'
))

# Enhanced interactive layout with export capabilities
fig2.update_layout(
    title={
        'text': "Enhanced Salary Analysis by ONET Occupation Type",
        'x': 0.5,
        'font': {'size': 18, 'family': 'Arial, sans-serif', 'color': '#2C3E50'}
    },
    xaxis_title="ONET Occupation",
    yaxis_title="Median Salary ($)",
    xaxis=dict(
        tickangle=-60,  # Steeper angle for better readability
        tickfont=dict(size=9),
        showgrid=True,
        gridcolor='rgba(0,0,0,0.1)',
        automargin=True,
        categoryorder='total ascending',
        tickmode='linear',  # Ensure all labels are shown
        nticks=len(onet_top)  # Show all occupation labels
    ),
    yaxis=dict(
        tickformat='$,.0f',
        showgrid=True,
        gridcolor='rgba(0,0,0,0.1)',
        tickfont=dict(size=10),
        range=[onet_top['median_salary'].min() * 0.9, onet_top['median_salary'].max() * 1.1]  # Better Y-axis range
    ),
    plot_bgcolor='rgba(0,0,0,0.02)',
    paper_bgcolor='white',
    height=800,  # Increased height for better label spacing
    margin=dict(b=250, l=100, r=150, t=100),  # More margin for labels
    hoverlabel=dict(
        bgcolor="white",
        font_size=11,
        font_family="Arial, sans-serif",
        bordercolor="navy"
    ),
    font=dict(family='Arial, sans-serif', size=11),
    # Enhanced interactivity
    dragmode='pan',
    selectdirection='d',
    showlegend=False  # Remove legend to save space
)

# Enhance with interactive features and export capabilities
fig2 = enhance_plotly_figure(fig2, "Enhanced Salary Analysis by ONET Occupation Type",
                           "02_onet_bubble_chart", width=1400, height=800)

fig2.show(renderer="notebook", config={'displayModeBar': True, 'displaylogo': False})

# Static image automatically generated by Plotly PNG renderer
```

The bubble chart demonstrates that specialized management and technical occupations command the highest median salaries in the job market. Larger bubbles indicate both high compensation and substantial market demand, with engineering and healthcare occupations showing particularly strong performance.

This visualization reveals the market's clear preference for specialized technical and leadership roles. Occupations requiring advanced technical skills or managerial responsibilities consistently command premium compensation, reflecting the economic value placed on expertise and decision-making capabilities. The bubble size represents job volume, showing that high-demand, high-paying occupations are both lucrative and plentiful, providing strong career opportunities for qualified candidates.

The analysis underscores the importance of specialized skills training and continuous professional development in accessing the most rewarding career opportunities in today's knowledge-based economy.

## Salary by Education Level

```{python}
#| label: fig-education-analysis
#| fig-cap: "Interactive scatter plot analysis of salary relationships with education levels and experience. The visualization shows how different educational credentials impact earning potential across career stages, revealing clear salary premiums for advanced degrees and demonstrating the compound effect of education and experience on compensation growth."

# Create education level groups as specified

education_spark = df.select(
    "MIN_EDULEVELS_NAME",
    "MAX_YEARS_EXPERIENCE",
    "SALARY_FROM",
    "LOT_V6_SPECIALIZED_OCCUPATION_NAME"
).filter(
    (col("MIN_EDULEVELS_NAME").isNotNull()) &
    (col("MAX_YEARS_EXPERIENCE").isNotNull()) &
    (col("SALARY_FROM").isNotNull()) &
    (col("SALARY_FROM") > 0) &
    (col("MAX_YEARS_EXPERIENCE") <= 30) &  # Remove outliers
    (col("LOT_V6_SPECIALIZED_OCCUPATION_NAME").isNotNull())
)

# Convert to Pandas for grouping and analysis
education_pd = education_spark.toPandas()

print(f"Records for education analysis: {len(education_pd):,}")

# Create education groups as specified in assignment
bachelor_or_lower = [
    "High school or equivalent",
    "Some college courses",
    "Certificate",
    "Associate degree",
    "Bachelor's degree",
    "No Education Listed"
]

masters_or_higher = [
    "Master's degree",
    "Doctoral degree",
    "Professional degree"
]

# Apply education grouping
education_pd['education_group'] = education_pd['MIN_EDULEVELS_NAME'].apply(
    lambda x: "Bachelor's or Lower" if x in bachelor_or_lower else (
        "Master's or PhD" if x in masters_or_higher else "Other"
    )
)

# Filter to main groups
education_main = education_pd[education_pd['education_group'].isin(["Bachelor's or Lower", "Master's or PhD"])]
print(f"Records in main education groups: {len(education_main):,}")

# Create the subplot structure with better spacing
fig3 = make_subplots(
    rows=2, cols=2,
    subplot_titles=(
        "Bachelor's or Lower<br><sub>Experience vs Salary</sub>",
        "Master's or PhD<br><sub>Experience vs Salary</sub>",
        "Bachelor's or Lower<br><sub>Salary Distribution</sub>",
        "Master's or PhD<br><sub>Salary Distribution</sub>"
    ),
    specs=[[{"secondary_y": False}, {"secondary_y": False}],
           [{"secondary_y": False}, {"secondary_y": False}]],
    vertical_spacing=0.15,  # Increase vertical spacing between rows
    horizontal_spacing=0.1   # Increase horizontal spacing between columns
)

# Add jitter to experience data as specified
np.random.seed(42)
education_main = education_main.copy()
education_main['experience_jitter'] = education_main['MAX_YEARS_EXPERIENCE'] + np.random.normal(0, 0.3, len(education_main))

# Filter data for each education group
bachelor_data = education_main[education_main['education_group'] == "Bachelor's or Lower"]
masters_data = education_main[education_main['education_group'] == "Master's or PhD"]

print(f"Bachelor's or Lower: {len(bachelor_data):,} records")
print(f"Master's or PhD: {len(masters_data):,} records")

# Scatter plot for Bachelor's or Lower (using go.Scatter for consistency)
if len(bachelor_data) > 0:
    fig3.add_trace(
        go.Scatter(
            x=bachelor_data['experience_jitter'],
            y=bachelor_data['SALARY_FROM'],
            mode='markers',
            name="Bachelor's or Lower",
            marker=dict(color='#3498DB', size=5, opacity=0.7),
            hovertemplate='Experience: %{x:.1f} years<br>Salary: $%{y:,.0f}<extra></extra>'
        ),
        row=1, col=1
    )

# Scatter plot for Master's or PhD (using go.Scatter for consistency)
if len(masters_data) > 0:
    fig3.add_trace(
        go.Scatter(
            x=masters_data['experience_jitter'],
            y=masters_data['SALARY_FROM'],
            mode='markers',
            name="Master's or PhD",
            marker=dict(color='#E74C3C', size=5, opacity=0.7),
            hovertemplate='Experience: %{x:.1f} years<br>Salary: $%{y:,.0f}<extra></extra>'
        ),
        row=1, col=2
    )

# Histograms for salary distribution
if len(bachelor_data) > 0:
    fig3.add_trace(
        go.Histogram(
            x=bachelor_data['SALARY_FROM'],
            name="Bachelor's Distribution",
            marker_color='#3498DB',
            opacity=0.75,  # Slightly more opaque
            nbinsx=30,     # More bins for better detail
            showlegend=False
        ),
        row=2, col=1
    )

if len(masters_data) > 0:
    fig3.add_trace(
        go.Histogram(
            x=masters_data['SALARY_FROM'],
            name="Master's Distribution",
            marker_color='#E74C3C',
            opacity=0.75,  # Slightly more opaque
            nbinsx=30,     # More bins for better detail
            showlegend=False
        ),
        row=2, col=2
    )

# Update layout with better spacing and readability
fig3.update_layout(
    title="Salary Analysis by Education Level",
    template="nike",
    font_family="Helvetica",
    title_font_size=18,  # Slightly larger title
    title_font_color="#2C3E50",
    plot_bgcolor="white",
    paper_bgcolor="white",
    height=1000,  # Increased height for better spacing
    width=1400,   # Explicit width for better proportions
    showlegend=False,
    margin=dict(t=80, b=80, l=60, r=60)  # Add margins around the entire figure
)

# Update axes with better formatting and spacing
# Row 1: Scatter plots
fig3.update_xaxes(title_text="Years of Experience", title_font_size=12, row=1, col=1)
fig3.update_xaxes(title_text="Years of Experience", title_font_size=12, row=1, col=2)
fig3.update_yaxes(title_text="Salary ($)", title_font_size=12, row=1, col=1)
fig3.update_yaxes(title_text="Salary ($)", title_font_size=12, row=1, col=2)

# Row 2: Histograms
fig3.update_xaxes(title_text="Salary ($)", title_font_size=12, tickangle=0, row=2, col=1)
fig3.update_xaxes(title_text="Salary ($)", title_font_size=12, tickangle=0, row=2, col=2)
fig3.update_yaxes(title_text="Frequency", title_font_size=12, row=2, col=1)  # Changed to "Frequency" for clarity
fig3.update_yaxes(title_text="Frequency", title_font_size=12, row=2, col=2)

# Improve subplot title formatting
fig3.update_annotations(font_size=13, font_weight="bold")

# Enhance with interactive features and export capabilities
fig3 = enhance_plotly_figure(fig3, "Salary Analysis by Education Level",
                           "03_education_level_analysis", width=1400, height=1000)

fig3.show(renderer="notebook", config={'displayModeBar': True, 'displaylogo': False})

# Static image automatically generated by Plotly PNG renderer
```

The scatter plots demonstrate that advanced degree holders consistently achieve higher salaries across all experience levels, with the compensation gap expanding significantly with increased tenure. The salary distribution histograms reveal that Master's and PhD holders exhibit higher median salaries and greater earning potential compared to Bachelor's degree recipients.

This comprehensive analysis of education's impact on compensation reveals several key patterns. Advanced degree holders maintain a consistent salary premium throughout their careers, with the gap widening as experience increases. This suggests that higher education provides not just an initial salary boost, but also enhanced career progression and earning potential over time.

The histograms show that while Bachelor's degree holders form the largest group, advanced degree recipients are positioned in higher salary brackets, indicating the market's valuation of specialized knowledge and advanced credentials. This pattern has significant implications for educational investment decisions and career planning strategies.

## Salary by Remote Work Type

```{python}
#| label: fig-remote-work
#| fig-cap: "Interactive scatter plot analysis of salary distributions across different remote work arrangements. The visualization reveals salary premiums for remote positions, competitive compensation for hybrid roles, and the trade-offs between work flexibility and earnings. The analysis shows how the modern work landscape has reshaped compensation patterns with remote work commanding premium salaries."

# Split into remote work groups as specified

remote_spark = df.select(
    "REMOTE_TYPE_NAME",
    "MAX_YEARS_EXPERIENCE",
    "SALARY_FROM",
    "LOT_V6_SPECIALIZED_OCCUPATION_NAME"
).filter(
    (col("MAX_YEARS_EXPERIENCE").isNotNull()) &
    (col("SALARY_FROM").isNotNull()) &
    (col("SALARY_FROM") > 0) &
    (col("MAX_YEARS_EXPERIENCE") <= 30) &
    (col("LOT_V6_SPECIALIZED_OCCUPATION_NAME").isNotNull())
)

# Convert to Pandas
remote_pd = remote_spark.toPandas()

print(f"Records for remote work analysis: {len(remote_pd):,}")

# Create remote work categories as specified in assignment
def categorize_remote(remote_type):
    if pd.isna(remote_type) or remote_type == '[None]' or remote_type == '' or remote_type is None:
        return 'Onsite'
    elif 'Remote' in str(remote_type):
        return 'Remote'
    elif 'Hybrid' in str(remote_type):
        return 'Hybrid'
    else:
        return 'Onsite'

remote_pd['remote_category'] = remote_pd['REMOTE_TYPE_NAME'].apply(categorize_remote)

# Print distribution
print("Remote work distribution:")
print(remote_pd['remote_category'].value_counts())

# Create subplot structure with better spacing
fig4 = make_subplots(
    rows=2, cols=3,
    subplot_titles=(
        "Remote - Experience vs Salary",
        "Hybrid - Experience vs Salary",
        "Onsite - Experience vs Salary",
        "Remote - Salary Distribution",
        "Hybrid - Salary Distribution",
        "Onsite - Salary Distribution"
    ),
    specs=[[{"secondary_y": False}, {"secondary_y": False}, {"secondary_y": False}],
           [{"secondary_y": False}, {"secondary_y": False}, {"secondary_y": False}]],
    horizontal_spacing=0.12,  # Add horizontal spacing
    vertical_spacing=0.15     # Add vertical spacing
)

# Add jitter as specified
np.random.seed(42)
remote_pd = remote_pd.copy()
remote_pd['experience_jitter'] = remote_pd['MAX_YEARS_EXPERIENCE'] + np.random.normal(0, 0.3, len(remote_pd))

# Define colors for each remote type
colors = {'Remote': '#27AE60', 'Hybrid': '#F39C12', 'Onsite': '#8E44AD'}

# Create scatter plots and histograms for each remote type
for i, remote_type in enumerate(['Remote', 'Hybrid', 'Onsite']):
    data = remote_pd[remote_pd['remote_category'] == remote_type]

    if len(data) > 0:
        print(f"{remote_type}: {len(data):,} records")

        # Scatter plot with jitter (using go.Scatter for consistency)
        fig4.add_trace(
            go.Scatter(
                x=data['experience_jitter'],
                y=data['SALARY_FROM'],
                mode='markers',
                name=f"{remote_type}",
                marker=dict(color=colors[remote_type], size=4, opacity=0.6),
                hovertemplate=f'{remote_type}<br>Experience: %{{x:.1f}} years<br>Salary: $%{{y:,.0f}}<extra></extra>',
                showlegend=False
            ),
            row=1, col=i+1
        )

        # Histogram
        fig4.add_trace(
            go.Histogram(
                x=data['SALARY_FROM'],
                name=f"{remote_type} Distribution",
                marker_color=colors[remote_type],
                opacity=0.7,
                nbinsx=20,
                showlegend=False
            ),
            row=2, col=i+1
        )

# Update layout with better spacing and readability
fig4.update_layout(
    title={
        'text': "Salary Analysis by Remote Work Type",
        'x': 0.5,
        'font': {'size': 18, 'family': 'Arial, sans-serif', 'color': '#2C3E50'}
    },
    template="plotly_white",
    font_family="Arial, sans-serif",
    plot_bgcolor="white",
    paper_bgcolor="white",
    height=900,  # Increased height for better spacing
    margin=dict(l=60, r=60, t=100, b=80, pad=10),  # Better margins
    showlegend=False  # Remove legend to save space
)

# Update axes labels with better formatting
for i in range(1, 4):
    # Scatter plot axes (top row)
    fig4.update_xaxes(
        title_text="Years of Experience",
        row=1, col=i,
        title_font=dict(size=12),
        tickfont=dict(size=10),
        showgrid=True,
        gridcolor='rgba(0,0,0,0.1)'
    )
    fig4.update_yaxes(
        title_text="Salary ($)",
        row=1, col=i,
        title_font=dict(size=12),
        tickfont=dict(size=10),
        showgrid=True,
        gridcolor='rgba(0,0,0,0.1)',
        tickformat='$,.0f'
    )

    # Histogram axes (bottom row)
    fig4.update_xaxes(
        title_text="Salary ($)",
        row=2, col=i,
        title_font=dict(size=12),
        tickfont=dict(size=10),
        showgrid=True,
        gridcolor='rgba(0,0,0,0.1)',
        tickformat='$,.0f'
    )
    fig4.update_yaxes(
        title_text="Count",
        row=2, col=i,
        title_font=dict(size=12),
        tickfont=dict(size=10),
        showgrid=True,
        gridcolor='rgba(0,0,0,0.1)'
    )

# Enhance with interactive features and export capabilities
fig4 = enhance_plotly_figure(fig4, "Salary Analysis by Remote Work Type",
                           "04_remote_work_analysis", width=1400, height=900)

fig4.show(renderer="notebook", config={'displayModeBar': True, 'displaylogo': False})

# Static image automatically generated by Plotly PNG renderer

# Summary statistics by remote type
print("\nSalary summary by remote work type:")
for remote_type in ['Remote', 'Hybrid', 'Onsite']:
    data = remote_pd[remote_pd['remote_category'] == remote_type]['SALARY_FROM']
    if len(data) > 0:
        print(f"{remote_type}: Mean=${data.mean():,.0f}, Median=${data.median():,.0f}, Count={len(data):,}")
```

Remote work positions demonstrate the highest salary potential with considerable variance, indicating premium compensation for distributed work capabilities. Hybrid arrangements provide competitive salaries while maintaining work flexibility, whereas onsite positions cluster around lower median compensation levels.

The analysis reveals a clear hierarchy in remote work compensation, with fully remote positions commanding the highest salaries. This premium likely reflects the specialized skills required for effective remote collaboration and the market's recognition of the value of location-independent work capabilities.

Hybrid arrangements offer a balanced approach, providing competitive compensation while maintaining some degree of workplace interaction. Onsite positions, while essential for many roles, generally offer lower compensation levels, reflecting the traditional employment model's baseline expectations.

These findings have significant implications for work arrangement preferences and compensation negotiations in an increasingly flexible labor market.