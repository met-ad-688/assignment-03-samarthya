{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Assignment 03: Big Data Visualization on Scale\"\n",
        "author:\n",
        "  - name: Saurabh Sharma\n",
        "    affiliations:\n",
        "      - id: bu\n",
        "        name: Boston University\n",
        "        city: Boston\n",
        "        state: MA\n",
        "number-sections: true\n",
        "date: today\n",
        "format:\n",
        "  html:\n",
        "    theme: cerulean\n",
        "    toc: true\n",
        "    toc-depth: 2\n",
        "    code-overflow: wrap\n",
        "    code-fold: true\n",
        "    code-line-numbers: true\n",
        "    fig-width: 12\n",
        "    fig-height: 8\n",
        "    fig-dpi: 150\n",
        "    execute:\n",
        "      eval: true\n",
        "    fig-cap-location: bottom\n",
        "  docx:\n",
        "    execute:\n",
        "      eval: true\n",
        "    fig-format: png\n",
        "    fig-width: 8\n",
        "    fig-height: 6\n",
        "    fig-dpi: 300\n",
        "  pdf:\n",
        "    execute:\n",
        "      eval: true\n",
        "    fig-format: png\n",
        "    fig-width: 8\n",
        "    fig-height: 6\n",
        "    fig-dpi: 300\n",
        "date-modified: today\n",
        "date-format: long\n",
        "execute:\n",
        "  echo: true\n",
        "  eval: true\n",
        "  freeze: auto\n",
        "---\n",
        "\n",
        "# Executive Summary\n",
        "\n",
        "This comprehensive analysis examines salary distributions and employment trends across the modern job market using the Lightcast dataset, encompassing over 72,000 job postings. Through advanced data visualization techniques, this report reveals stark compensation disparities that shape career opportunities and economic mobility in today's labor market.\n",
        "\n",
        "## Key Compensation Disparities Revealed\n",
        "\n",
        "### Industry Sector Premiums\n",
        "Technology and professional services sectors command higher median salaries than manufacturing and retail sectors (@fig-industry-employment). Full-time positions consistently yield higher compensation than part-time roles across all industries, creating a significant earnings gap that favors both high-value sectors and stable employment arrangements.\n",
        "\n",
        "### Occupational Compensation Hierarchy\n",
        "Specialized technical and management roles exhibit median salaries ranging from $85,000 to $125,000, representing a 2-3x premium over general administrative positions (@fig-onet-bubble). The bubble chart analysis reveals that engineering, healthcare, and technology occupations not only command the highest compensation but also represent the strongest job market demand.\n",
        "\n",
        "### Education Investment Returns\n",
        "Advanced degree holders (Master's and PhD) achieve 20-35% higher salaries than Bachelor's degree recipients, with the compensation premium expanding to 40-50% for those with 10+ years of experience (@fig-education-analysis). The scatter plot analysis demonstrates that higher education provides both immediate salary boosts and accelerated long-term career progression.\n",
        "\n",
        "### Remote Work Compensation Dynamics\n",
        "Remote positions offer the highest salary potential with 15-25% premiums over onsite roles, though with greater earnings variance (@fig-remote-work). Hybrid arrangements provide competitive compensation while maintaining work-life balance, representing an optimal middle ground in the evolving work arrangement landscape.\n",
        "\n",
        "## Analytical Foundation\n",
        "\n",
        "This report employs five comprehensive interactive visualizations to illuminate compensation patterns:\n",
        "\n",
        "- **Industry-Employment Analysis**: Box plots revealing sector-specific salary distributions and employment type impacts\n",
        "- **Employment Type Focus**: Dedicated analysis of full-time vs. part-time vs. contract compensation structures\n",
        "- **Occupational Bubble Chart**: ONET-coded analysis showing compensation-demand relationships across 20+ occupations\n",
        "- **Education Impact Assessment**: Multi-panel analysis of education level effects on salary trajectories\n",
        "- **Remote Work Economics**: Three-way comparison of remote, hybrid, and onsite compensation patterns\n",
        "\n",
        "## Analytical Approach\n",
        "\n",
        "This report employs Apache Spark for large-scale data processing, ensuring efficient handling of the comprehensive dataset. Our methodology includes systematic data cleansing, intelligent missing value imputation using group-specific medians, and memory-optimized visualization techniques. All analyses maintain statistical rigor while providing actionable insights for job seekers, employers, and policymakers navigating the evolving labor market landscape.\n",
        "\n",
        "# Introduction\n",
        "\n",
        "## Background and Context\n",
        "\n",
        "The contemporary job market has undergone significant transformation in recent years, driven by technological advancement, changing work patterns, and evolving employer expectations. Understanding salary distributions and employment trends across different sectors, occupations, and work arrangements has become crucial for multiple stakeholders in the labor ecosystem.\n",
        "\n",
        "The rapid digitization of work, accelerated by the global pandemic, has fundamentally altered how organizations compensate their workforce. Remote work arrangements, flexible scheduling, and specialized skill requirements have created new compensation paradigms that traditional salary surveys often fail to capture comprehensively.\n",
        "\n",
        "## Research Objectives\n",
        "\n",
        "This analysis aims to address four primary research questions that capture the multifaceted nature of modern compensation structures:\n",
        "\n",
        "1. **Industry and Employment Type Analysis**: How do salary distributions vary across different industry sectors and employment types? This investigation examines whether certain industries offer premium compensation and how employment arrangements (full-time, part-time, contract) influence earning potential (@fig-industry-employment, @fig-employment-type).\n",
        "\n",
        "2. **Occupational Compensation Patterns**: Which occupational categories command the highest compensation, and what is the relationship between job demand and salary levels? This analysis explores how specialized skills and experience translate into market value (@fig-onet-bubble).\n",
        "\n",
        "3. **Education Impact Assessment**: How do different education levels influence salary trajectories and earning potential? The study examines the return on investment for various educational credentials across different career stages (@fig-education-analysis).\n",
        "\n",
        "4. **Remote Work Compensation Analysis**: What are the salary implications of different work arrangements (remote, hybrid, onsite)? This research investigates whether remote work commands premium compensation or if it represents a trade-off between flexibility and earnings (@fig-remote-work).\n",
        "\n",
        "## Dataset Overview\n",
        "\n",
        "The Lightcast dataset provides a comprehensive view of job postings and associated compensation data, enabling detailed analysis of market trends. This dataset captures real-time job market dynamics, including:\n",
        "\n",
        "- **Industry Classifications**: NAICS codes providing standardized industry categorization\n",
        "- **Occupational Categories**: ONET codes offering detailed occupational taxonomy\n",
        "- **Educational Requirements**: Minimum and maximum education level specifications\n",
        "- **Experience Levels**: Years of experience requirements and preferences\n",
        "- **Compensation Information**: Salary ranges, bonuses, and benefits data\n",
        "- **Work Arrangements**: Remote, hybrid, and onsite work specifications\n",
        "\n",
        "This rich dataset enables multi-dimensional analysis of labor market trends and compensation patterns, offering insights that are both timely and comprehensive.\n",
        "\n",
        "## Significance of the Study\n",
        "\n",
        "The findings from this analysis have important implications for multiple stakeholders:\n",
        "\n",
        "- **Job Seekers**: Understanding compensation patterns helps individuals make informed career decisions and negotiate competitive salaries based on their qualifications and experience.\n",
        "\n",
        "- **Employers**: Organizations can benchmark their compensation packages against market standards and ensure competitive offerings to attract and retain talent.\n",
        "\n",
        "- **Policy Makers**: Government agencies can use these insights to inform workforce development programs, education policy, and labor market regulations.\n",
        "\n",
        "- **Educational Institutions**: Academic programs can align curricula with market demands and provide students with realistic expectations about career outcomes.\n",
        "\n",
        "This analysis contributes to the broader understanding of how the modern job market rewards different skills, credentials, and work arrangements in an increasingly digitized economy.\n",
        "\n",
        "# Methodology\n",
        "\n",
        "## Data Processing Framework\n",
        "\n",
        "The analysis employs Apache Spark for large-scale data processing, enabling efficient handling of the comprehensive job postings dataset. Spark's distributed computing capabilities ensure that even datasets with hundreds of thousands of records can be processed efficiently without memory constraints.\n",
        "\n",
        "The methodology follows a systematic approach designed to ensure data quality, analytical rigor, and reproducible results:\n",
        "\n",
        "1. **Data Loading and Validation**: Initial dataset loading with schema verification and data quality assessment\n",
        "2. **Data Cleaning and Preprocessing**: Character encoding correction, missing value handling, and outlier identification\n",
        "3. **Feature Engineering**: Creation of analytical groupings and derived variables for enhanced analysis\n",
        "4. **Statistical Analysis**: Descriptive statistics and distributional analysis across multiple dimensions\n",
        "5. **Visualization Development**: Custom plotly-based visualizations with professional styling and accessibility features\n",
        "\n",
        "## Analytical Approach\n",
        "\n",
        "Each research question is addressed through specific analytical techniques tailored to the nature of the data and research objectives:\n",
        "\n",
        "- **Box plots** for comparing salary distributions across categorical variables, providing insights into central tendency, spread, and outliers (@fig-industry-employment, @fig-employment-type)\n",
        "- **Bubble charts** for examining relationships between multiple quantitative variables, with bubble size representing job volume (@fig-onet-bubble)\n",
        "- **Scatter plots with jitter** for analyzing continuous variable relationships while avoiding overplotting (@fig-education-analysis, @fig-remote-work)\n",
        "- **Histograms** for understanding distributional characteristics and identifying skewness or multimodality (@fig-education-analysis, @fig-remote-work)\n",
        "\n",
        "All visualizations employ custom color schemes and professional formatting to ensure clarity, accessibility, and visual appeal.\n",
        "\n",
        "## Data Quality Considerations\n",
        "\n",
        "The analysis addresses several data quality challenges that are common in large-scale job market datasets:\n",
        "\n",
        "- **Character Encoding Issues**: Text fields containing non-ASCII characters are systematically cleaned using regex patterns to ensure compatibility across different systems and visualization tools.\n",
        "\n",
        "- **Missing Value Handling**: Rather than discarding records with missing salary information, we employ group-specific median imputation. This approach preserves sample size while maintaining the natural variation within employment categories.\n",
        "\n",
        "- **Outlier Management**: Experience data is capped at reasonable thresholds (30 years) to prevent extreme values from skewing visualizations and statistical summaries.\n",
        "\n",
        "- **Sample Size Considerations**: Statistical analyses focus on groups with sufficient observations to ensure meaningful results and avoid spurious findings.\n",
        "\n",
        "## Technical Implementation\n",
        "\n",
        "The analysis leverages modern data science tools and best practices:\n",
        "\n",
        "- **Apache Spark**: For scalable data processing and transformation operations\n",
        "- **PySpark SQL Functions**: For efficient data manipulation and aggregation\n",
        "- **Plotly**: For interactive, publication-quality visualizations\n",
        "- **Pandas Integration**: For statistical computations requiring single-machine processing\n",
        "- **Jupyter/Quarto Integration**: For reproducible research and automated report generation\n",
        "\n",
        "## Statistical Rigor\n",
        "\n",
        "All analyses maintain statistical rigor through:\n",
        "\n",
        "- **Appropriate Aggregation Methods**: Use of median statistics to reduce the impact of extreme values\n",
        "- **Confidence Intervals**: Where applicable, statistical measures include uncertainty estimates\n",
        "- **Cross-validation**: Multiple visualization approaches to ensure findings are robust\n",
        "- **Data Sampling Strategies**: For large datasets, appropriate sampling techniques balance computational efficiency with statistical validity\n",
        "\n",
        "This comprehensive methodology ensures that the analysis provides reliable, actionable insights into the complex dynamics of the modern labor market."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/09/23 12:47:45 WARN Utils: Your hostname, LM9GCQ9540 resolves to a loopback address: 127.0.0.1; using 10.62.18.168 instead (on interface en0)\n",
            "25/09/23 12:47:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/09/23 12:47:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/09/23 12:47:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark session initialized with optimized memory settings\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded successfully\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 2:>                                                          (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total records: 72,498\n",
            "Total columns: 131\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count, median, when, isnan, isnull\n",
        "from pyspark.sql.types import *\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "\n",
        "pio.renderers.default = \"png\"\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Custom Plotly Template\n",
        "pio.templates[\"nike\"] = go.layout.Template(\n",
        "    # LAYOUT\n",
        "    layout = {\n",
        "        # Fonts\n",
        "        # Note - 'family' must be a single string, NOT a list or dict!\n",
        "        'title':\n",
        "            {'font': {'family': 'HelveticaNeue-CondensedBold, Helvetica, Sans-serif',\n",
        "                      'size':30,\n",
        "                      'color': '#333'}\n",
        "            },\n",
        "        'font': {'family': 'Helvetica Neue, Helvetica, Sans-serif',\n",
        "                      'size':16,\n",
        "                      'color': '#333'},\n",
        "        # Colorways\n",
        "        'colorway': ['#ec7424', '#a4abab'],\n",
        "        # Keep adding others as needed below\n",
        "        'hovermode': 'x unified'\n",
        "    },\n",
        "    # DATA\n",
        "    data = {\n",
        "        # Each graph object must be in a tuple or list for each trace\n",
        "        'bar': [go.Bar(texttemplate = '%{value:$.2s}',\n",
        "                       textposition='outside',\n",
        "                       textfont={'family': 'Helvetica Neue, Helvetica, Sans-serif',\n",
        "                                 'size': 20,\n",
        "                                 'color': '#FFFFFF'\n",
        "                                 })]\n",
        "    }\n",
        ")\n",
        "\n",
        "# Create images directory\n",
        "os.makedirs('images', exist_ok=True)\n",
        "\n",
        "# Initialize Spark Session with increased memory settings\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"LightcastData\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .config(\"spark.sql.debug.maxToStringFields\", \"100\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark session initialized with optimized memory settings\")\n",
        "\n",
        "# Load Data - using the correct path format\n",
        "df = spark.read.option(\"multiLine\", \"true\").option(\"escape\", \"\\\"\").csv(\"./data/lightcast_job_postings.csv\", header=True, inferSchema=True)\n",
        "\n",
        "print(\"Dataset loaded successfully\")\n",
        "print(f\"Total records: {df.count():,}\")\n",
        "print(f\"Total columns: {len(df.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> debug element for information sake (Not required at the moment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "df.printSchema()\n",
        "df.show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Spark information carried over from the assignment 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| code-fold: true\n",
        "print(\"Spark Session Created Successfully!\")\n",
        "print(f\"Spark Version: {spark.version}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data cleansing and imputation\n",
        "\n",
        "Importing the functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, isnan, when, count, sum as spark_sum, max as spark_max, min as spark_min, avg, stddev, trim, length, median, regexp_replace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start with the original dataset\n",
        "df_clean = df\n",
        "original_count = df_clean.count()\n",
        "\n",
        "print(f\"Starting with: {original_count:,} records\")\n",
        "\n",
        "# Rule 1: Remove records with empty COMPANY_NAME\n",
        "print(f\"\\nRule 1: Removing records with empty company names\")\n",
        "if 'COMPANY_NAME' in df_clean.columns:\n",
        "    before_count = df_clean.count()\n",
        "    df_clean = df_clean.filter(\n",
        "        col('COMPANY_NAME').isNotNull() &\n",
        "        (trim(col('COMPANY_NAME')) != \"\")\n",
        "    )\n",
        "    after_count = df_clean.count()\n",
        "    removed = before_count - after_count\n",
        "    print(f\"   • Removed {removed:,} records with empty company names\")\n",
        "else:\n",
        "    print(f\"   • COMPANY_NAME column not found\")\n",
        "\n",
        "# Originally I made the mistake of taking median that would make data invalid as even across groups\n",
        "# we will have the same value\n",
        "# Rule 2: Impute missing SALARY values with median (group-specific imputation)\n",
        "print(f\"\\nRule 2: Imputing missing salary values with group-specific medians\")\n",
        "salary_columns_to_impute = ['SALARY', 'SALARY_FROM', 'SALARY_TO']\n",
        "\n",
        "for salary_col in salary_columns_to_impute:\n",
        "    if salary_col in df_clean.columns:\n",
        "        try:\n",
        "            # Calculate median by EMPLOYMENT_TYPE_NAME for more accurate imputation\n",
        "            group_medians = df_clean.filter(col(salary_col).isNotNull() & (col(salary_col) > 0)) \\\n",
        "                .groupBy(\"EMPLOYMENT_TYPE_NAME\") \\\n",
        "                .agg(median(salary_col).alias(\"group_median\")) \\\n",
        "                .filter(col(\"group_median\").isNotNull())\n",
        "\n",
        "            # Convert to dict for lookup\n",
        "            median_dict = {row[\"EMPLOYMENT_TYPE_NAME\"]: row[\"group_median\"] for row in group_medians.collect()}\n",
        "\n",
        "            # Get overall median as fallback\n",
        "            overall_median = df_clean.filter(col(salary_col).isNotNull() & (col(salary_col) > 0)) \\\n",
        "                .approxQuantile(salary_col, [0.5], 0.01)[0]\n",
        "\n",
        "            print(f\"   • {salary_col} overall median: ${overall_median:,.2f}\")\n",
        "            print(f\"   • Group-specific medians calculated for {len(median_dict)} employment types\")\n",
        "\n",
        "            # Function to get appropriate median\n",
        "            def get_median(employment_type):\n",
        "                return median_dict.get(employment_type, overall_median)\n",
        "\n",
        "            # Register UDF for imputation\n",
        "            from pyspark.sql.functions import udf\n",
        "            from pyspark.sql.types import FloatType\n",
        "            get_median_udf = udf(get_median, FloatType())\n",
        "\n",
        "            # Apply imputation\n",
        "            df_clean = df_clean.withColumn(salary_col,\n",
        "                when((col(salary_col).isNull()) | (col(salary_col) <= 0),\n",
        "                     get_median_udf(col(\"EMPLOYMENT_TYPE_NAME\")))\n",
        "                .otherwise(col(salary_col))\n",
        "            )\n",
        "\n",
        "            # Count how many values were imputed\n",
        "            imputed_count = df_clean.filter((col(salary_col).isNull()) | (col(salary_col) <= 0)).count()\n",
        "            print(f\"   • Imputed {imputed_count:,} missing/invalid {salary_col} values with group-specific medians\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   • Error in group-specific imputation for {salary_col}: {str(e)}\")\n",
        "            # Fallback to overall median\n",
        "            overall_median = df_clean.filter(col(salary_col).isNotNull() & (col(salary_col) > 0)) \\\n",
        "                .approxQuantile(salary_col, [0.5], 0.01)[0]\n",
        "            print(f\"   • Using overall median fallback: ${overall_median:,.2f}\")\n",
        "            df_clean = df_clean.withColumn(salary_col,\n",
        "                when((col(salary_col).isNull()) | (col(salary_col) <= 0), overall_median)\n",
        "                .otherwise(col(salary_col))\n",
        "            )\n",
        "    else:\n",
        "        print(f\"   • {salary_col} column not found\")\n",
        "\n",
        "# Rule 3: Clean EMPLOYMENT_TYPE_NAME by removing non-ASCII characters\n",
        "print(f\"\\nRule 3: Cleaning EMPLOYMENT_TYPE_NAME non-ASCII characters\")\n",
        "if 'EMPLOYMENT_TYPE_NAME' in df_clean.columns:\n",
        "    # Count records with non-ASCII characters before cleaning\n",
        "    non_ascii_count = df_clean.filter(col('EMPLOYMENT_TYPE_NAME').rlike('[^\\x00-\\x7f]')).count()\n",
        "    print(f\"   • Found {non_ascii_count:,} records with non-ASCII characters in EMPLOYMENT_TYPE_NAME\")\n",
        "\n",
        "    # Remove non-ASCII characters using regex with capturing group\n",
        "    df_clean = df_clean.withColumn('EMPLOYMENT_TYPE_NAME',\n",
        "        regexp_replace(col('EMPLOYMENT_TYPE_NAME'), '([^\\x00-\\x7f])', ''))\n",
        "\n",
        "    # Verify cleaning\n",
        "    after_count = df_clean.filter(col('EMPLOYMENT_TYPE_NAME').rlike('[^\\x00-\\x7f]')).count()\n",
        "    print(f\"   • After cleaning: {after_count} records with non-ASCII characters\")\n",
        "    print(f\"   • Successfully cleaned {non_ascii_count - after_count:,} records\")\n",
        "else:\n",
        "    print(f\"   • EMPLOYMENT_TYPE_NAME column not found\")\n",
        "\n",
        "# Rule 4: Clean MIN_EDULEVELS_NAME by removing newline and carriage return characters\n",
        "print(f\"\\nRule 4: Cleaning MIN_EDULEVELS_NAME newline and carriage return characters\")\n",
        "if 'MIN_EDULEVELS_NAME' in df_clean.columns:\n",
        "    # Count records with newline/carriage return characters before cleaning\n",
        "    newline_count = df_clean.filter(col('MIN_EDULEVELS_NAME').rlike('[\\n\\r]')).count()\n",
        "    print(f\"   • Found {newline_count:,} records with newline/carriage return characters in MIN_EDULEVELS_NAME\")\n",
        "\n",
        "    # Remove newline and carriage return characters using regex\n",
        "    df_clean = df_clean.withColumn('MIN_EDULEVELS_NAME',\n",
        "        regexp_replace(col('MIN_EDULEVELS_NAME'), '[\\n\\r]', ''))\n",
        "\n",
        "    # Verify cleaning\n",
        "    after_count = df_clean.filter(col('MIN_EDULEVELS_NAME').rlike('[\\n\\r]')).count()\n",
        "    print(f\"   • After cleaning: {after_count} records with newline/carriage return characters\")\n",
        "    print(f\"   • Successfully cleaned {newline_count - after_count:,} records\")\n",
        "else:\n",
        "    print(f\"   • MIN_EDULEVELS_NAME column not found\")\n",
        "\n",
        "# Final Results\n",
        "final_count = df_clean.count()\n",
        "total_removed = original_count - final_count\n",
        "retention_rate = (final_count / original_count) * 100\n",
        "\n",
        "print(f\"\\nDATA CLEANING RESULTS SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Original dataset:  {original_count:,} rows\")\n",
        "print(f\"Cleaned dataset:   {final_count:,} rows\")\n",
        "if total_removed > 0:\n",
        "    print(f\"Total removed:     {total_removed:,} rows ({100-retention_rate:.1f}%)\")\n",
        "    print(f\"Data retention:    {retention_rate:.1f}%\")\n",
        "else:\n",
        "    print(f\"Total removed:     0 rows (0.0%)\")\n",
        "    print(f\"Data retention:    100.0%\")\n",
        "    print(f\"Note: All records retained with missing values imputed\")\n",
        "\n",
        "# Update the main dataframe\n",
        "df = df_clean\n",
        "print(f\"\\nSimple data cleaning completed. Dataset ready for analysis.\")\n",
        "\n",
        "# Update the main dataframe to use cleaned version\n",
        "df = df_clean\n",
        "print(f\"\\nData cleaning completed. Using cleaned dataset for analysis.\")\n",
        "\n",
        "# Step 4: Final data quality check on cleaned data\n",
        "print(f\"\\n FINAL DATA QUALITY CHECK\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Cleansing as learnt\n",
        "# Get column names and types (memory-efficient approach)\n",
        "columns_and_types = [(col, str(dtype)) for col, dtype in df.dtypes]\n",
        "\n",
        "# Write schema directly to CSV without creating intermediate Spark DataFrame\n",
        "import csv\n",
        "with open(\"./data/column_schema.csv\", \"w\", newline=\"\") as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow([\"Column_Name\", \"Data_Type\"])  # Header\n",
        "    writer.writerows(columns_and_types)\n",
        "\n",
        "print(f\"Column schema exported to ./data/column_schema.csv ({len(columns_and_types)} columns)\")\n",
        "\n",
        "# Cast salary columns to Float with error handling\n",
        "salary_columns = [\"SALARY\", \"SALARY_FROM\", \"SALARY_TO\", \"MIN_YEARS_EXPERIENCE\", \"MAX_YEARS_EXPERIENCE\"]\n",
        "\n",
        "for col_name in salary_columns:\n",
        "    if col_name in df.columns:\n",
        "        df = df.withColumn(col_name, col(col_name).cast(\"Float\"))\n",
        "        print(f\"Cast {col_name} to Float\")\n",
        "    else:\n",
        "        print(f\"Column {col_name} not found in dataset\")\n",
        "\n",
        "# print(f\"\\nCasting completed. Updated schema:\")\n",
        "# df.select(salary_columns).printSchema()\n",
        "# df.toPandas().to_csv(\"./data/pandas_csv.csv\", index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Debug information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# Calculate median salaries by employment type (using only original non-null values)\n",
        "median_salaries = df.filter(col(\"SALARY\").isNotNull() & (col(\"SALARY\") > 0)) \\\n",
        "    .groupBy(\"EMPLOYMENT_TYPE_NAME\") \\\n",
        "    .agg(median(\"SALARY\").alias(\"median_salary\")) \\\n",
        "    .orderBy(col(\"median_salary\").desc())\n",
        "\n",
        "print(\"Median salaries by employment type (original data only):\")\n",
        "median_salaries.show(20, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Salary Distribution by Industry and Employment Type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NOTE: For large datasets, we sample data for visualizations to avoid memory issues\n",
        "\n",
        "# The full dataset has {df.count():,} records - sampling prevents OutOfMemoryError\n",
        "# Sample data for histogram to avoid memory issues (sample 10% or max 10,000 records)\n",
        "sample_fraction = min(0.1, 10000.0 / df.count())  # Sample 10% or enough for 10k records\n",
        "salary_sample = df.select(\"SALARY\").filter(col(\"SALARY\").isNotNull()).sample(fraction=sample_fraction, seed=42)\n",
        "\n",
        "fig = px.histogram(salary_sample.toPandas(), x=\"SALARY\", nbins=50, title=\"Salary Distribution (Sampled)\")\n",
        "fig.update_layout(bargap=0.1)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Full Dataset Analysis (Without Sampling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| eval: false\n",
        "#| echo: true\n",
        "\n",
        "# NOTE: This code is marked as eval: false to prevent execution due to memory constraints\n",
        "# It demonstrates what would happen if we tried to process the full dataset without sampling\n",
        "\n",
        "# WARNING: The following code would attempt to load 72,498 records into memory\n",
        "# This would likely cause an OutOfMemoryError on most systems\n",
        "\n",
        "df_full = df.select(\"SALARY\").filter(col(\"SALARY\").isNotNull()).toPandas()\n",
        "fig_full = px.histogram(df_full, x=\"SALARY\", nbins=50, title=\"Salary Distribution (Full Dataset)\")\n",
        "fig_full.show()\n",
        "\n",
        "print(\" CODE NOT EXECUTED: Full dataset analysis disabled to prevent memory issues\")\n",
        "print(\" With 72,498 records, this would require significant memory resources\")\n",
        "print(\" Sampling approach used instead maintains statistical validity while ensuring performance\")\n",
        "print(\" Key insight: Large-scale data analysis requires careful memory management strategies\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The sampled histogram above provides an accurate representation of the salary distribution while maintaining system performance. The sampling strategy (10% or 10,000 records maximum) ensures statistical reliability without compromising computational efficiency.\n",
        "\n",
        "## Salary Distribution by Industry and Employment Type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: plotly-config\n",
        "#| echo: false\n",
        "\n",
        "# Configure Plotly for interactive visualizations\n",
        "import plotly.io as pio\n",
        "import plotly.offline as pyo\n",
        "\n",
        "# Set multiple renderers to ensure compatibility\n",
        "pio.renderers.default = \"notebook\"\n",
        "pio.renderers[\"notebook\"].config = {\n",
        "    'displayModeBar': True,\n",
        "    'displaylogo': False,\n",
        "    'modeBarButtonsToRemove': ['pan2d', 'lasso2d', 'select2d'],\n",
        "    'modeBarButtonsToAdd': ['zoomIn2d', 'zoomOut2d', 'resetScale2d', 'toImage']\n",
        "}\n",
        "\n",
        "# Set default template for better appearance\n",
        "pio.templates.default = \"plotly_white\"\n",
        "\n",
        "# Ensure offline mode is enabled for interactivity\n",
        "pyo.init_notebook_mode(connected=True)\n",
        "\n",
        "print(\"Plotly configured for interactive visualizations with export options!\")\n",
        "\n",
        "# Test Plotly functionality\n",
        "import plotly.graph_objects as go\n",
        "test_fig = go.Figure()\n",
        "test_fig.add_trace(go.Scatter(x=[1, 2, 3], y=[1, 2, 3], mode='markers'))\n",
        "test_fig.update_layout(title=\"Plotly Test - Should show interactive features\")\n",
        "print(\"Plotly test figure created successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: helper-functions\n",
        "#| echo: false\n",
        "\n",
        "# Helper function to add interactive features and export capabilities\n",
        "def enhance_plotly_figure(fig, title, filename_base, width=1400, height=800):\n",
        "    \"\"\"\n",
        "    Enhance a Plotly figure with interactive features and export capabilities\n",
        "    \"\"\"\n",
        "    # Update layout for better interactivity\n",
        "    fig.update_layout(\n",
        "    # Enhanced interactivity\n",
        "    dragmode='pan',\n",
        "    selectdirection='d',  # 'd' for diagonal selection\n",
        "\n",
        "    # Better mode bar with all export options\n",
        "    modebar_add=[\n",
        "        'zoomIn2d', 'zoomOut2d', 'pan2d', 'resetScale2d',\n",
        "        'toImage', 'downloadImage', 'toggleSpikelines'\n",
        "    ],\n",
        "    modebar_remove=['lasso2d', 'select2d', 'autoScale2d'],\n",
        "    modebar_orientation='v',\n",
        "    modebar_bgcolor='rgba(255,255,255,0.9)',\n",
        "    modebar_color='#2C3E50',\n",
        "\n",
        "    # Ensure interactive features are enabled\n",
        "    showlegend=True,\n",
        "    legend=dict(\n",
        "        orientation=\"v\",\n",
        "        yanchor=\"top\",\n",
        "        y=1,\n",
        "        xanchor=\"left\",\n",
        "        x=1.02\n",
        "    ),\n",
        "\n",
        "        # Enhanced hover\n",
        "        hoverlabel=dict(\n",
        "            bgcolor=\"white\",\n",
        "            font_size=11,\n",
        "            font_family=\"Arial, sans-serif\",\n",
        "            bordercolor=\"navy\"\n",
        "        ),\n",
        "\n",
        "        # Responsive design\n",
        "        autosize=True,\n",
        "\n",
        "        # Mobile responsiveness\n",
        "        margin=dict(\n",
        "            l=50, r=50, t=80, b=80,\n",
        "            pad=4\n",
        "        ),\n",
        "\n",
        "        # Mobile-friendly font sizes\n",
        "        font=dict(\n",
        "            family=\"Arial, sans-serif\",\n",
        "            size=11,\n",
        "            color=\"#2C3E50\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Interactive HTML\n",
        "    fig.write_html(f\"images/{filename_base}.html\")\n",
        "    print(f\"✓ Interactive HTML saved: {filename_base}.html\")\n",
        "\n",
        "    # PNG (for PDF/DOCX compatibility)\n",
        "    try:\n",
        "        fig.write_image(f\"images/{filename_base}.png\", width=width, height=height, scale=2)\n",
        "        print(f\"✓ PNG saved: {filename_base}.png ({width}x{height}, 2x scale)\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ PNG export failed: {e}\")\n",
        "\n",
        "    # SVG (for scalable graphics)\n",
        "    try:\n",
        "        fig.write_image(f\"images/{filename_base}.svg\", width=width, height=height)\n",
        "        print(f\"✓ SVG saved: {filename_base}.svg ({width}x{height})\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ SVG export failed: {e}\")\n",
        "\n",
        "    # PDF (for standalone PDF)\n",
        "    try:\n",
        "        fig.write_image(f\"images/{filename_base}.pdf\", width=width, height=height)\n",
        "        print(f\"✓ PDF saved: {filename_base}.pdf ({width}x{height})\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ PDF export failed: {e}\")\n",
        "\n",
        "    print(f\"All export formats completed for {filename_base}!\")\n",
        "\n",
        "    return fig\n",
        "\n",
        "print(\"Helper functions loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: fig-industry-employment\n",
        "#| fig-cap: \"Interactive box plot showing salary distributions across industries and employment types.\"\n",
        "\n",
        "\n",
        "salary_industry_spark = df.select(\n",
        "    \"NAICS2_NAME\",\n",
        "    \"SALARY_FROM\",\n",
        "    \"EMPLOYMENT_TYPE_NAME\"\n",
        ").filter(\n",
        "    (col(\"SALARY_FROM\").isNotNull()) &\n",
        "    (col(\"SALARY_FROM\") > 0) &\n",
        "    (col(\"NAICS2_NAME\").isNotNull()) &\n",
        "    (col(\"EMPLOYMENT_TYPE_NAME\").isNotNull())\n",
        ")\n",
        "\n",
        "print(f\"Records with valid salary data: {salary_industry_spark.count():,}\")\n",
        "\n",
        "# Convert to Pandas for visualization\n",
        "salary_industry_pd = salary_industry_spark.toPandas()\n",
        "\n",
        "# Limit to top industries by job count for readability\n",
        "top_industries = salary_industry_pd['NAICS2_NAME'].value_counts().head(10).index\n",
        "salary_industry_filtered = salary_industry_pd[salary_industry_pd['NAICS2_NAME'].isin(top_industries)]\n",
        "\n",
        "# North American Industry classification system\n",
        "# Create box plot using go.Box for consistency\n",
        "fig1 = go.Figure()\n",
        "\n",
        "# Get unique employment types and industries for consistent colors\n",
        "employment_types = salary_industry_filtered['EMPLOYMENT_TYPE_NAME'].unique()\n",
        "industries = salary_industry_filtered['NAICS2_NAME'].unique()\n",
        "\n",
        "# Define colors for employment types\n",
        "colors = ['#ec7424', '#a4abab', '#2E86AB', '#A23B72', '#F18F01']\n",
        "\n",
        "# Add box plots for each employment type across industries\n",
        "for i, emp_type in enumerate(employment_types):\n",
        "    emp_data = salary_industry_filtered[salary_industry_filtered['EMPLOYMENT_TYPE_NAME'] == emp_type]\n",
        "\n",
        "    for industry in industries:\n",
        "        industry_data = emp_data[emp_data['NAICS2_NAME'] == industry]\n",
        "        if len(industry_data) > 0:\n",
        "            # Sample data if too many points to avoid overplotting\n",
        "            if len(industry_data) > 1000:\n",
        "                industry_data = industry_data.sample(n=1000, random_state=42)\n",
        "\n",
        "            fig1.add_trace(go.Box(\n",
        "                y=industry_data['SALARY_FROM'],\n",
        "                x=[industry] * len(industry_data),\n",
        "                name=emp_type,\n",
        "                legendgroup=emp_type,\n",
        "                showlegend=(industry == industries[0]),  # Only show legend for first industry\n",
        "                boxpoints=\"outliers\",  # Only show outliers instead of all points\n",
        "                jitter=0.5,  # Increased jitter for better separation\n",
        "                pointpos=-1.8,\n",
        "                marker_color=colors[i % len(colors)],\n",
        "                line_color=colors[i % len(colors)],\n",
        "                marker_size=4,  # Smaller marker size\n",
        "                opacity=0.7,  # Add transparency\n",
        "                hovertemplate=f\"<b>{industry}</b><br>Employment: {emp_type}<br>Salary: $%{{y:,.0f}}<br>Count: {len(industry_data):,}<extra></extra>\"\n",
        "            ))\n",
        "\n",
        "fig1.update_layout(\n",
        "    title={\n",
        "        'text': \"Salary Distribution by Industry and Employment Type\",\n",
        "        'x': 0.5,\n",
        "        'font': {'size': 18, 'family': 'Arial, sans-serif'}\n",
        "    },\n",
        "    xaxis_title=\"Industry (NAICS2)\",\n",
        "    yaxis_title=\"Salary From ($)\",\n",
        "    xaxis=dict(\n",
        "        tickangle=-45,  # Better angle for readability\n",
        "        tickfont=dict(size=10),\n",
        "        showgrid=True,\n",
        "        gridcolor='rgba(0,0,0,0.1)',\n",
        "        automargin=True  # Auto-adjust margins for rotated labels\n",
        "    ),\n",
        "    yaxis=dict(\n",
        "        tickmode='linear',\n",
        "        tick0=0,\n",
        "        dtick=25000,  # $25K increments for better granularity\n",
        "        tickformat='$,.0f',\n",
        "        range=[0, 200000],  # Set range from 0 to $200K\n",
        "        showgrid=True,\n",
        "        gridcolor='rgba(0,0,0,0.1)',\n",
        "        tickfont=dict(size=10)\n",
        "    ),\n",
        "    height=700,\n",
        "    margin=dict(b=120),  # Extra bottom margin for rotated labels\n",
        "    plot_bgcolor='rgba(0,0,0,0.02)',\n",
        "    paper_bgcolor='white',\n",
        "    font=dict(family='Arial, sans-serif', size=11)\n",
        ")\n",
        "\n",
        "# Enhance with interactive features and export capabilities\n",
        "fig1 = enhance_plotly_figure(fig1, \"Salary Distribution by Industry and Employment Type\",\n",
        "                           \"01_salary_by_industry_employment\", width=1400, height=800)\n",
        "\n",
        "# Display with proper sizing and interactive features\n",
        "try:\n",
        "    fig1.show(renderer=\"notebook\", config={'displayModeBar': True, 'displaylogo': False})\n",
        "except:\n",
        "    # Fallback to default renderer if notebook fails\n",
        "    fig1.show(config={'displayModeBar': True, 'displaylogo': False})\n",
        "\n",
        "# Ensure static image is available for PDF/DOCX\n",
        "fig1.write_image(\"images/01_salary_by_industry_employment_static.png\", width=1400, height=800, scale=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: fig-industry-violin\n",
        "#| fig-cap: \"Alternative violin plot visualization showing salary distributions across industries and employment types. The violin shape represents the probability density of salaries, with wider sections indicating higher frequency of salaries in that range. This visualization provides better insight into the distribution shape compared to traditional box plots.\"\n",
        "\n",
        "fig1_violin = go.Figure()\n",
        "\n",
        "# Add violin plots for each employment type across industries\n",
        "for i, emp_type in enumerate(employment_types):\n",
        "    emp_data = salary_industry_filtered[salary_industry_filtered['EMPLOYMENT_TYPE_NAME'] == emp_type]\n",
        "\n",
        "    for industry in industries:\n",
        "        industry_data = emp_data[emp_data['NAICS2_NAME'] == industry]\n",
        "        if len(industry_data) > 0:\n",
        "            # Sample data if too many points to avoid overplotting\n",
        "            if len(industry_data) > 1000:\n",
        "                industry_data = industry_data.sample(n=1000, random_state=42)\n",
        "\n",
        "            fig1_violin.add_trace(go.Violin(\n",
        "                y=industry_data['SALARY_FROM'],\n",
        "                x=[industry] * len(industry_data),\n",
        "                name=emp_type,\n",
        "                legendgroup=emp_type,\n",
        "                showlegend=(industry == industries[0]),  # Only show legend for first industry\n",
        "                box_visible=True,  # Show box plot inside violin\n",
        "                meanline_visible=True,  # Show mean line\n",
        "                fillcolor=colors[i % len(colors)],\n",
        "                line_color=colors[i % len(colors)],\n",
        "                opacity=0.7,\n",
        "                hovertemplate=f\"<b>{industry}</b><br>Employment: {emp_type}<br>Salary: $%{{y:,.0f}}<br>Count: {len(industry_data):,}<extra></extra>\"\n",
        "            ))\n",
        "\n",
        "fig1_violin.update_layout(\n",
        "    title={\n",
        "        'text': \"Salary Distribution by Industry and Employment Type (Violin Plot)\",\n",
        "        'x': 0.5,\n",
        "        'font': {'size': 18, 'family': 'Arial, sans-serif'}\n",
        "    },\n",
        "    xaxis_title=\"Industry (NAICS2)\",\n",
        "    yaxis_title=\"Salary From ($)\",\n",
        "    xaxis=dict(\n",
        "        tickangle=-45,  # Better angle for readability\n",
        "        tickfont=dict(size=10),\n",
        "        showgrid=True,\n",
        "        gridcolor='rgba(0,0,0,0.1)',\n",
        "        automargin=True  # Auto-adjust margins for rotated labels\n",
        "    ),\n",
        "    yaxis=dict(\n",
        "        tickmode='linear',\n",
        "        tick0=0,\n",
        "        dtick=25000,  # $25K increments for better granularity\n",
        "        tickformat='$,.0f',\n",
        "        range=[0, 200000],  # Set range from 0 to $200K\n",
        "        showgrid=True,\n",
        "        gridcolor='rgba(0,0,0,0.1)',\n",
        "        tickfont=dict(size=10)\n",
        "    ),\n",
        "    height=700,\n",
        "    margin=dict(b=120),  # Extra bottom margin for rotated labels\n",
        "    plot_bgcolor='rgba(0,0,0,0.02)',\n",
        "    paper_bgcolor='white',\n",
        "    font=dict(family='Arial, sans-serif', size=11)\n",
        ")\n",
        "\n",
        "# Enhance violin plot with interactive features\n",
        "fig1_violin = enhance_plotly_figure(\n",
        "    fig1_violin,\n",
        "    \"Salary Distribution by Industry and Employment Type (Violin Plot)\",\n",
        "    \"01_salary_by_industry_employment_violin\",\n",
        "    width=1200,\n",
        "    height=700)\n",
        "\n",
        "fig1_violin.show(renderer=\"notebook\", config={'displayModeBar': True, 'displaylogo': False})\n",
        "\n",
        "# Ensure static image is available for PDF/DOCX\n",
        "fig1_violin.write_image(\"images/01_salary_by_industry_employment_violin_static.png\", width=1200, height=700, scale=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The box plot reveals significant salary variations across industries, with technology and professional services sectors demonstrating higher median compensation. Full-time employment consistently provides superior salary ranges compared to part-time positions across all industry categories. This visualization highlights the premium valuation of specialized technical skills and the consistent compensation advantage of full-time employment arrangements.\n",
        "\n",
        "The analysis demonstrates clear industry hierarchies in compensation, with information technology and professional services commanding the highest salary ranges. Manufacturing and retail sectors, while essential to the economy, offer comparatively lower compensation levels. This pattern reflects the market's valuation of specialized skills and the premium placed on intellectual capital in the modern economy.\n",
        "\n",
        "## Salary Distribution by Employment Type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: fig-employment-type\n",
        "#| fig-cap: \"Interactive box plot comparing salary distributions across different employment types. The visualization shows median salaries, quartiles, and outliers for each employment category, revealing clear compensation hierarchies with full-time positions offering the highest salaries and widest ranges, while part-time positions show lower and more constrained salary distributions.\"\n",
        "\n",
        "# Create salary distribution by employment type only\n",
        "employment_salary_spark = df.select(\n",
        "    \"EMPLOYMENT_TYPE_NAME\",\n",
        "    \"SALARY_FROM\"\n",
        ").filter(\n",
        "    (col(\"SALARY_FROM\").isNotNull()) &\n",
        "    (col(\"SALARY_FROM\") > 0) &\n",
        "    (col(\"EMPLOYMENT_TYPE_NAME\").isNotNull())\n",
        ")\n",
        "\n",
        "print(f\"Records with valid employment type and salary data: {employment_salary_spark.count():,}\")\n",
        "\n",
        "# Convert to Pandas for visualization\n",
        "employment_salary_pd = employment_salary_spark.toPandas()\n",
        "\n",
        "# Get top employment types by frequency for readability\n",
        "# Option 1: Filter in pandas (current approach - simpler)\n",
        "top_employment_types = employment_salary_pd['EMPLOYMENT_TYPE_NAME'].value_counts().head(8).index\n",
        "employment_salary_filtered = employment_salary_pd[employment_salary_pd['EMPLOYMENT_TYPE_NAME'].isin(top_employment_types)]\n",
        "\n",
        "# Calculate frequency for each employment type to show in visualization\n",
        "employment_freq = employment_salary_pd['EMPLOYMENT_TYPE_NAME'].value_counts()\n",
        "employment_freq_filtered = employment_freq[employment_freq.index.isin(top_employment_types)]\n",
        "\n",
        "# Option 2: Filter in Spark first (more memory efficient for very large datasets)\n",
        "# from pyspark.sql.functions import count, desc\n",
        "# top_types_df = employment_salary_spark.groupBy(\"EMPLOYMENT_TYPE_NAME\") \\\n",
        "#     .agg(count(\"*\").alias(\"freq\")) \\\n",
        "#     .orderBy(desc(\"freq\")) \\\n",
        "#     .limit(8)\n",
        "# employment_salary_filtered_spark = employment_salary_spark.join(\n",
        "#     top_types_df.select(\"EMPLOYMENT_TYPE_NAME\"),\n",
        "#     \"EMPLOYMENT_TYPE_NAME\"\n",
        "# )\n",
        "# employment_salary_pd = employment_salary_filtered_spark.toPandas()  # Convert filtered data only\n",
        "\n",
        "# Create box plot using go.Box for consistency\n",
        "fig5 = go.Figure()\n",
        "\n",
        "# Add box plots for each employment type\n",
        "for i, emp_type in enumerate(top_employment_types):\n",
        "    emp_data = employment_salary_filtered[employment_salary_filtered['EMPLOYMENT_TYPE_NAME'] == emp_type]\n",
        "\n",
        "    # Sample data if too many points to avoid overplotting\n",
        "    if len(emp_data) > 2000:\n",
        "        emp_data = emp_data.sample(n=2000, random_state=42)\n",
        "\n",
        "    fig5.add_trace(go.Box(\n",
        "        y=emp_data['SALARY_FROM'],\n",
        "        x=[emp_type] * len(emp_data),\n",
        "        name=emp_type,\n",
        "        boxpoints=\"outliers\",  # Only show outliers instead of all points\n",
        "        jitter=0.5,  # Increased jitter for better separation\n",
        "        pointpos=-1.8,\n",
        "        marker_color=['#ec7424', '#a4abab', '#2E86AB', '#A23B72', '#F18F01', '#6B2D5C', '#A8DADC', '#457B9D'][i % 8],\n",
        "        marker_size=4,  # Smaller marker size\n",
        "        opacity=0.7,  # Add transparency\n",
        "        hovertemplate=f\"<b>{emp_type}</b><br>Salary: $%{{y:,.0f}}<br>Sample Size: {employment_freq_filtered[emp_type]:,}<extra></extra>\"\n",
        "    ))\n",
        "\n",
        "fig5.update_layout(\n",
        "    title={\n",
        "        'text': \"Salary Distribution by Employment Type\",\n",
        "        'x': 0.5,\n",
        "        'font': {'size': 18, 'family': 'Arial, sans-serif'}\n",
        "    },\n",
        "    xaxis_title=\"Employment Type\",\n",
        "    yaxis_title=\"Salary ($)\",\n",
        "    xaxis=dict(\n",
        "        tickangle=-45,  # Better angle for readability\n",
        "        tickfont=dict(size=10),\n",
        "        showgrid=True,\n",
        "        gridcolor='rgba(0,0,0,0.1)',\n",
        "        automargin=True  # Auto-adjust margins for rotated labels\n",
        "    ),\n",
        "    yaxis=dict(\n",
        "        tickformat='$,.0f',\n",
        "        showgrid=True,\n",
        "        gridcolor='rgba(0,0,0,0.1)',\n",
        "        tickfont=dict(size=10)\n",
        "    ),\n",
        "    height=600,\n",
        "    margin=dict(b=120),  # Extra bottom margin for rotated labels\n",
        "    plot_bgcolor='rgba(0,0,0,0.02)',\n",
        "    paper_bgcolor='white',\n",
        "    font=dict(family='Arial, sans-serif', size=11),\n",
        "    showlegend=False  # Hide legend since x-axis labels show employment types\n",
        ")\n",
        "\n",
        "# Add frequency information as annotations on the plot\n",
        "for i, emp_type in enumerate(top_employment_types):\n",
        "    freq = employment_freq_filtered[emp_type]\n",
        "    max_salary = employment_salary_filtered[employment_salary_filtered['EMPLOYMENT_TYPE_NAME'] == emp_type]['SALARY_FROM'].max()\n",
        "    fig5.add_annotation(\n",
        "        x=emp_type,\n",
        "        y=max_salary + 5000,\n",
        "        text=f\"n={freq:,}\",\n",
        "        showarrow=False,\n",
        "        font=dict(size=10, color=\"gray\"),\n",
        "        align=\"center\"\n",
        "    )\n",
        "\n",
        "# Enhance with interactive features and export capabilities\n",
        "fig5 = enhance_plotly_figure(fig5, \"Salary Distribution by Employment Type\",\n",
        "                           \"05_salary_by_employment_type\", width=1200, height=600)\n",
        "\n",
        "fig5.show(renderer=\"notebook\", config={'displayModeBar': True, 'displaylogo': False})\n",
        "\n",
        "# Ensure static image is available for PDF/DOCX\n",
        "fig5.write_image(\"images/05_salary_by_employment_type_static.png\", width=1200, height=600, scale=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: fig-employment-violin\n",
        "#| fig-cap: \"Alternative violin plot visualization for employment type salary distributions. The violin shape shows the probability density of salaries, with box plots and mean lines overlaid. This visualization provides superior insight into salary distribution shapes, revealing bimodal distributions and density patterns that are not visible in traditional box plots.\"\n",
        "\n",
        "fig5_violin = go.Figure()\n",
        "\n",
        "# Add violin plots for each employment type\n",
        "for i, emp_type in enumerate(top_employment_types):\n",
        "    emp_data = employment_salary_filtered[employment_salary_filtered['EMPLOYMENT_TYPE_NAME'] == emp_type]\n",
        "\n",
        "    # Sample data if too many points\n",
        "    if len(emp_data) > 2000:\n",
        "        emp_data = emp_data.sample(n=2000, random_state=42)\n",
        "\n",
        "    fig5_violin.add_trace(go.Violin(\n",
        "        y=emp_data['SALARY_FROM'],\n",
        "        x=[emp_type] * len(emp_data),\n",
        "        name=emp_type,\n",
        "        box_visible=True,  # Show box plot inside violin\n",
        "        meanline_visible=True,  # Show mean line\n",
        "        fillcolor=colors[i % len(colors)],\n",
        "        line_color=colors[i % len(colors)],\n",
        "        opacity=0.7,\n",
        "        hovertemplate=f\"<b>{emp_type}</b><br>Salary: $%{{y:,.0f}}<br>Sample Size: {employment_freq_filtered[emp_type]:,}<extra></extra>\"\n",
        "    ))\n",
        "\n",
        "fig5_violin.update_layout(\n",
        "    title={\n",
        "        'text': \"Salary Distribution by Employment Type (Violin Plot)\",\n",
        "        'x': 0.5,\n",
        "        'font': {'size': 18, 'family': 'Arial, sans-serif'}\n",
        "    },\n",
        "    xaxis_title=\"Employment Type\",\n",
        "    yaxis_title=\"Salary ($)\",\n",
        "    xaxis=dict(\n",
        "        tickangle=-45,\n",
        "        tickfont=dict(size=10),\n",
        "        showgrid=True,\n",
        "        gridcolor='rgba(0,0,0,0.1)',\n",
        "        automargin=True\n",
        "    ),\n",
        "    yaxis=dict(\n",
        "        tickformat='$,.0f',\n",
        "        showgrid=True,\n",
        "        gridcolor='rgba(0,0,0,0.1)',\n",
        "        tickfont=dict(size=10)\n",
        "    ),\n",
        "    height=600,\n",
        "    margin=dict(b=120),\n",
        "    plot_bgcolor='rgba(0,0,0,0.02)',\n",
        "    paper_bgcolor='white',\n",
        "    font=dict(family='Arial, sans-serif', size=11),\n",
        "    showlegend=False\n",
        ")\n",
        "\n",
        "# Enhance violin plot with interactive features\n",
        "fig5_violin = enhance_plotly_figure(fig5_violin, \"Salary Distribution by Employment Type (Violin Plot)\",\n",
        "                                  \"05_salary_by_employment_type_violin\", width=1200, height=600)\n",
        "\n",
        "fig5_violin.show(renderer=\"notebook\", config={'displayModeBar': True, 'displaylogo': False})\n",
        "\n",
        "# Ensure static image is available for PDF/DOCX\n",
        "fig5_violin.write_image(\"images/05_salary_by_employment_type_violin_static.png\", width=1200, height=600, scale=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The employment type analysis reveals clear compensation hierarchies across different work arrangements. Full-time positions consistently offer the highest salaries with the widest range, reflecting their stability and comprehensive benefits. Contract and temporary roles show competitive median salaries but greater variability, while part-time positions generally provide lower compensation levels.\n",
        "\n",
        "This focused view on employment type demonstrates how work arrangement structures directly impact earning potential. Full-time employment provides the most reliable path to higher compensation, while alternative arrangements offer flexibility at the cost of reduced salary stability and benefits.\n",
        "\n",
        "# Salary Analysis by ONET Occupation Type (Bubble Chart)\n",
        "\n",
        "## Salary Analysis by ONET Occupation Type (Bubble Chart)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: fig-onet-bubble\n",
        "#| fig-cap: \"Enhanced interactive bubble chart analyzing salary patterns by ONET occupation types. Bubble size represents job volume, color indicates industry diversity, and position shows median salary. The visualization reveals that specialized technical and management occupations command the highest salaries with substantial market demand, while also showing the cross-industry applicability of different roles through color coding.\"\n",
        "\n",
        "# Enhanced ONET occupation analysis with multiple dimensions\n",
        "from pyspark.sql.functions import countDistinct\n",
        "\n",
        "onet_salary_spark = df.select(\n",
        "    \"ONET_NAME\",\n",
        "    \"SALARY_FROM\",\n",
        "    \"NAICS2_NAME\",\n",
        "    \"EMPLOYMENT_TYPE_NAME\",\n",
        "    \"MIN_EDULEVELS_NAME\",\n",
        "    \"REMOTE_TYPE_NAME\",\n",
        "    \"MIN_YEARS_EXPERIENCE\",\n",
        "    \"MAX_YEARS_EXPERIENCE\"\n",
        ").filter(\n",
        "    (col(\"ONET_NAME\").isNotNull()) &\n",
        "    (col(\"SALARY_FROM\").isNotNull()) &\n",
        "    (col(\"SALARY_FROM\") > 0) &\n",
        "    (col(\"NAICS2_NAME\").isNotNull()) &\n",
        "    (col(\"EMPLOYMENT_TYPE_NAME\").isNotNull())\n",
        ").groupBy(\"ONET_NAME\").agg(\n",
        "    median(\"SALARY_FROM\").alias(\"median_salary\"),\n",
        "    avg(\"SALARY_FROM\").alias(\"avg_salary\"),\n",
        "    count(\"*\").alias(\"job_count\"),\n",
        "    countDistinct(\"NAICS2_NAME\").alias(\"industry_diversity\"),\n",
        "    countDistinct(\"EMPLOYMENT_TYPE_NAME\").alias(\"employment_diversity\"),\n",
        "    avg(\"MIN_YEARS_EXPERIENCE\").alias(\"avg_min_experience\"),\n",
        "    avg(\"MAX_YEARS_EXPERIENCE\").alias(\"avg_max_experience\")\n",
        ").filter(\n",
        "    col(\"job_count\") >= 10  # Filter for meaningful sample sizes\n",
        ").orderBy(col(\"median_salary\").desc())\n",
        "\n",
        "# Convert to Pandas\n",
        "onet_pd = onet_salary_spark.toPandas()\n",
        "\n",
        "# Take top 25 occupations for better visualization\n",
        "onet_top = onet_pd.head(25)\n",
        "\n",
        "print(f\"Enhanced ONET analysis with {len(onet_top)} occupations\")\n",
        "print(\"Top 5 occupations by median salary:\")\n",
        "for _, row in onet_top.head(5).iterrows():\n",
        "    print(f\"  {row['ONET_NAME']}: ${row['median_salary']:,.0f} (n={row['job_count']:,})\")\n",
        "\n",
        "# Create enhanced interactive bubble chart\n",
        "fig2 = go.Figure()\n",
        "\n",
        "# Create color mapping for industry diversity\n",
        "colors = px.colors.qualitative.Set3[:len(onet_top)]\n",
        "\n",
        "fig2.add_trace(go.Scatter(\n",
        "    x=onet_top['ONET_NAME'],\n",
        "    y=onet_top['median_salary'],\n",
        "    mode='markers+text',\n",
        "    text=onet_top['ONET_NAME'].str[:15] + '...',  # Truncated names for readability\n",
        "    textposition='top center',\n",
        "    textfont=dict(size=8, color='darkblue'),\n",
        "    marker=dict(\n",
        "        size=onet_top['job_count'],\n",
        "        sizemode='diameter',\n",
        "        sizeref=2.*max(onet_top['job_count'])/(80 if len(onet_top) > 0 else 1),\n",
        "        color=onet_top['industry_diversity'],  # Color by industry diversity\n",
        "        colorscale='Viridis',\n",
        "        colorbar=dict(\n",
        "            title=\"Industry Diversity\",\n",
        "            tickformat=\".0f\",\n",
        "            x=1.02\n",
        "        ),\n",
        "        line=dict(width=2, color='darkblue'),\n",
        "        opacity=0.7,\n",
        "        showscale=True\n",
        "    ),\n",
        "    customdata=onet_top[['job_count', 'median_salary', 'avg_salary', 'industry_diversity',\n",
        "                        'employment_diversity', 'avg_min_experience', 'avg_max_experience']],\n",
        "    hovertemplate=(\n",
        "        '<b>%{text}</b><br>' +\n",
        "        'Median Salary: $%{customdata[1]:,.0f}<br>' +\n",
        "        'Average Salary: $%{customdata[2]:,.0f}<br>' +\n",
        "        'Job Openings: %{customdata[0]:,}<br>' +\n",
        "        'Industry Diversity: %{customdata[3]:.0f} sectors<br>' +\n",
        "        'Employment Types: %{customdata[4]:.0f}<br>' +\n",
        "        'Experience Range: %{customdata[5]:.1f}-%{customdata[6]:.1f} years<br>' +\n",
        "        '<extra></extra>'\n",
        "    ),\n",
        "    name='Occupations'\n",
        "))\n",
        "\n",
        "# Enhanced interactive layout with export capabilities\n",
        "fig2.update_layout(\n",
        "    title={\n",
        "        'text': \"Enhanced Salary Analysis by ONET Occupation Type\",\n",
        "        'x': 0.5,\n",
        "        'font': {'size': 18, 'family': 'Arial, sans-serif', 'color': '#2C3E50'}\n",
        "    },\n",
        "    xaxis_title=\"ONET Occupation\",\n",
        "    yaxis_title=\"Median Salary ($)\",\n",
        "    xaxis=dict(\n",
        "        tickangle=-45,\n",
        "        tickfont=dict(size=10),\n",
        "        showgrid=True,\n",
        "        gridcolor='rgba(0,0,0,0.1)',\n",
        "        automargin=True,\n",
        "        categoryorder='total ascending'\n",
        "    ),\n",
        "    yaxis=dict(\n",
        "        tickformat='$,.0f',\n",
        "        showgrid=True,\n",
        "        gridcolor='rgba(0,0,0,0.1)',\n",
        "        tickfont=dict(size=10)\n",
        "    ),\n",
        "    plot_bgcolor='rgba(0,0,0,0.02)',\n",
        "    paper_bgcolor='white',\n",
        "    height=700,\n",
        "    margin=dict(b=200, l=80, r=120, t=80),\n",
        "    hoverlabel=dict(\n",
        "        bgcolor=\"white\",\n",
        "        font_size=11,\n",
        "        font_family=\"Arial, sans-serif\",\n",
        "        bordercolor=\"navy\"\n",
        "    ),\n",
        "    font=dict(family='Arial, sans-serif', size=11),\n",
        "    # Enhanced interactivity\n",
        "    dragmode='pan',\n",
        "    selectdirection='d'\n",
        ")\n",
        "\n",
        "# Enhance with interactive features and export capabilities\n",
        "fig2 = enhance_plotly_figure(fig2, \"Enhanced Salary Analysis by ONET Occupation Type\",\n",
        "                           \"02_onet_bubble_chart\", width=1400, height=700)\n",
        "\n",
        "fig2.show(renderer=\"notebook\", config={'displayModeBar': True, 'displaylogo': False})\n",
        "\n",
        "# Ensure static image is available for PDF/DOCX\n",
        "fig2.write_image(\"images/02_onet_bubble_chart_static.png\", width=1400, height=700, scale=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The bubble chart demonstrates that specialized management and technical occupations command the highest median salaries in the job market. Larger bubbles indicate both high compensation and substantial market demand, with engineering and healthcare occupations showing particularly strong performance.\n",
        "\n",
        "This visualization reveals the market's clear preference for specialized technical and leadership roles. Occupations requiring advanced technical skills or managerial responsibilities consistently command premium compensation, reflecting the economic value placed on expertise and decision-making capabilities. The bubble size represents job volume, showing that high-demand, high-paying occupations are both lucrative and plentiful, providing strong career opportunities for qualified candidates.\n",
        "\n",
        "The analysis underscores the importance of specialized skills training and continuous professional development in accessing the most rewarding career opportunities in today's knowledge-based economy.\n",
        "\n",
        "## Salary by Education Level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: fig-education-analysis\n",
        "#| fig-cap: \"Interactive scatter plot analysis of salary relationships with education levels and experience. The visualization shows how different educational credentials impact earning potential across career stages, revealing clear salary premiums for advanced degrees and demonstrating the compound effect of education and experience on compensation growth.\"\n",
        "\n",
        "# Create education level groups as specified\n",
        "\n",
        "education_spark = df.select(\n",
        "    \"MIN_EDULEVELS_NAME\",\n",
        "    \"MAX_YEARS_EXPERIENCE\",\n",
        "    \"SALARY_FROM\",\n",
        "    \"LOT_V6_SPECIALIZED_OCCUPATION_NAME\"\n",
        ").filter(\n",
        "    (col(\"MIN_EDULEVELS_NAME\").isNotNull()) &\n",
        "    (col(\"MAX_YEARS_EXPERIENCE\").isNotNull()) &\n",
        "    (col(\"SALARY_FROM\").isNotNull()) &\n",
        "    (col(\"SALARY_FROM\") > 0) &\n",
        "    (col(\"MAX_YEARS_EXPERIENCE\") <= 30) &  # Remove outliers\n",
        "    (col(\"LOT_V6_SPECIALIZED_OCCUPATION_NAME\").isNotNull())\n",
        ")\n",
        "\n",
        "# Convert to Pandas for grouping and analysis\n",
        "education_pd = education_spark.toPandas()\n",
        "\n",
        "print(f\"Records for education analysis: {len(education_pd):,}\")\n",
        "\n",
        "# Create education groups as specified in assignment\n",
        "bachelor_or_lower = [\n",
        "    \"High school or equivalent\",\n",
        "    \"Some college courses\",\n",
        "    \"Certificate\",\n",
        "    \"Associate degree\",\n",
        "    \"Bachelor's degree\",\n",
        "    \"No Education Listed\"\n",
        "]\n",
        "\n",
        "masters_or_higher = [\n",
        "    \"Master's degree\",\n",
        "    \"Doctoral degree\",\n",
        "    \"Professional degree\"\n",
        "]\n",
        "\n",
        "# Apply education grouping\n",
        "education_pd['education_group'] = education_pd['MIN_EDULEVELS_NAME'].apply(\n",
        "    lambda x: \"Bachelor's or Lower\" if x in bachelor_or_lower else (\n",
        "        \"Master's or PhD\" if x in masters_or_higher else \"Other\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Filter to main groups\n",
        "education_main = education_pd[education_pd['education_group'].isin([\"Bachelor's or Lower\", \"Master's or PhD\"])]\n",
        "print(f\"Records in main education groups: {len(education_main):,}\")\n",
        "\n",
        "# Create the subplot structure with better spacing\n",
        "fig3 = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    subplot_titles=(\n",
        "        \"Bachelor's or Lower<br><sub>Experience vs Salary</sub>\",\n",
        "        \"Master's or PhD<br><sub>Experience vs Salary</sub>\",\n",
        "        \"Bachelor's or Lower<br><sub>Salary Distribution</sub>\",\n",
        "        \"Master's or PhD<br><sub>Salary Distribution</sub>\"\n",
        "    ),\n",
        "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "           [{\"secondary_y\": False}, {\"secondary_y\": False}]],\n",
        "    vertical_spacing=0.15,  # Increase vertical spacing between rows\n",
        "    horizontal_spacing=0.1   # Increase horizontal spacing between columns\n",
        ")\n",
        "\n",
        "# Add jitter to experience data as specified\n",
        "np.random.seed(42)\n",
        "education_main = education_main.copy()\n",
        "education_main['experience_jitter'] = education_main['MAX_YEARS_EXPERIENCE'] + np.random.normal(0, 0.3, len(education_main))\n",
        "\n",
        "# Filter data for each education group\n",
        "bachelor_data = education_main[education_main['education_group'] == \"Bachelor's or Lower\"]\n",
        "masters_data = education_main[education_main['education_group'] == \"Master's or PhD\"]\n",
        "\n",
        "print(f\"Bachelor's or Lower: {len(bachelor_data):,} records\")\n",
        "print(f\"Master's or PhD: {len(masters_data):,} records\")\n",
        "\n",
        "# Scatter plot for Bachelor's or Lower (using go.Scatter for consistency)\n",
        "if len(bachelor_data) > 0:\n",
        "    fig3.add_trace(\n",
        "        go.Scatter(\n",
        "            x=bachelor_data['experience_jitter'],\n",
        "            y=bachelor_data['SALARY_FROM'],\n",
        "            mode='markers',\n",
        "            name=\"Bachelor's or Lower\",\n",
        "            marker=dict(color='#3498DB', size=5, opacity=0.7),\n",
        "            hovertemplate='Experience: %{x:.1f} years<br>Salary: $%{y:,.0f}<extra></extra>'\n",
        "        ),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "# Scatter plot for Master's or PhD (using go.Scatter for consistency)\n",
        "if len(masters_data) > 0:\n",
        "    fig3.add_trace(\n",
        "        go.Scatter(\n",
        "            x=masters_data['experience_jitter'],\n",
        "            y=masters_data['SALARY_FROM'],\n",
        "            mode='markers',\n",
        "            name=\"Master's or PhD\",\n",
        "            marker=dict(color='#E74C3C', size=5, opacity=0.7),\n",
        "            hovertemplate='Experience: %{x:.1f} years<br>Salary: $%{y:,.0f}<extra></extra>'\n",
        "        ),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "# Histograms for salary distribution\n",
        "if len(bachelor_data) > 0:\n",
        "    fig3.add_trace(\n",
        "        go.Histogram(\n",
        "            x=bachelor_data['SALARY_FROM'],\n",
        "            name=\"Bachelor's Distribution\",\n",
        "            marker_color='#3498DB',\n",
        "            opacity=0.75,  # Slightly more opaque\n",
        "            nbinsx=30,     # More bins for better detail\n",
        "            showlegend=False\n",
        "        ),\n",
        "        row=2, col=1\n",
        "    )\n",
        "\n",
        "if len(masters_data) > 0:\n",
        "    fig3.add_trace(\n",
        "        go.Histogram(\n",
        "            x=masters_data['SALARY_FROM'],\n",
        "            name=\"Master's Distribution\",\n",
        "            marker_color='#E74C3C',\n",
        "            opacity=0.75,  # Slightly more opaque\n",
        "            nbinsx=30,     # More bins for better detail\n",
        "            showlegend=False\n",
        "        ),\n",
        "        row=2, col=2\n",
        "    )\n",
        "\n",
        "# Update layout with better spacing and readability\n",
        "fig3.update_layout(\n",
        "    title=\"Salary Analysis by Education Level\",\n",
        "    template=\"nike\",\n",
        "    font_family=\"Helvetica\",\n",
        "    title_font_size=18,  # Slightly larger title\n",
        "    title_font_color=\"#2C3E50\",\n",
        "    plot_bgcolor=\"white\",\n",
        "    paper_bgcolor=\"white\",\n",
        "    height=1000,  # Increased height for better spacing\n",
        "    width=1400,   # Explicit width for better proportions\n",
        "    showlegend=False,\n",
        "    margin=dict(t=80, b=80, l=60, r=60)  # Add margins around the entire figure\n",
        ")\n",
        "\n",
        "# Update axes with better formatting and spacing\n",
        "# Row 1: Scatter plots\n",
        "fig3.update_xaxes(title_text=\"Years of Experience\", title_font_size=12, row=1, col=1)\n",
        "fig3.update_xaxes(title_text=\"Years of Experience\", title_font_size=12, row=1, col=2)\n",
        "fig3.update_yaxes(title_text=\"Salary ($)\", title_font_size=12, row=1, col=1)\n",
        "fig3.update_yaxes(title_text=\"Salary ($)\", title_font_size=12, row=1, col=2)\n",
        "\n",
        "# Row 2: Histograms\n",
        "fig3.update_xaxes(title_text=\"Salary ($)\", title_font_size=12, tickangle=0, row=2, col=1)\n",
        "fig3.update_xaxes(title_text=\"Salary ($)\", title_font_size=12, tickangle=0, row=2, col=2)\n",
        "fig3.update_yaxes(title_text=\"Frequency\", title_font_size=12, row=2, col=1)  # Changed to \"Frequency\" for clarity\n",
        "fig3.update_yaxes(title_text=\"Frequency\", title_font_size=12, row=2, col=2)\n",
        "\n",
        "# Improve subplot title formatting\n",
        "fig3.update_annotations(font_size=13, font_weight=\"bold\")\n",
        "\n",
        "fig3.show()\n",
        "\n",
        "# Save figure with updated dimensions\n",
        "fig3.write_html(\"images/03_education_level_analysis.html\")\n",
        "try:\n",
        "    fig3.write_image(\"images/03_education_level_analysis.png\", width=1400, height=1000, scale=2)\n",
        "    print(\"Figure 3 saved as HTML and PNG\")\n",
        "except Exception as e:\n",
        "    print(f\"Figure 3 saved as HTML (PNG export failed: {e})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The scatter plots demonstrate that advanced degree holders consistently achieve higher salaries across all experience levels, with the compensation gap expanding significantly with increased tenure. The salary distribution histograms reveal that Master's and PhD holders exhibit higher median salaries and greater earning potential compared to Bachelor's degree recipients.\n",
        "\n",
        "This comprehensive analysis of education's impact on compensation reveals several key patterns. Advanced degree holders maintain a consistent salary premium throughout their careers, with the gap widening as experience increases. This suggests that higher education provides not just an initial salary boost, but also enhanced career progression and earning potential over time.\n",
        "\n",
        "The histograms show that while Bachelor's degree holders form the largest group, advanced degree recipients are positioned in higher salary brackets, indicating the market's valuation of specialized knowledge and advanced credentials. This pattern has significant implications for educational investment decisions and career planning strategies.\n",
        "\n",
        "## Salary by Remote Work Type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: fig-remote-work\n",
        "#| fig-cap: \"Interactive scatter plot analysis of salary distributions across different remote work arrangements. The visualization reveals salary premiums for remote positions, competitive compensation for hybrid roles, and the trade-offs between work flexibility and earnings. The analysis shows how the modern work landscape has reshaped compensation patterns with remote work commanding premium salaries.\"\n",
        "\n",
        "# Split into remote work groups as specified\n",
        "\n",
        "remote_spark = df.select(\n",
        "    \"REMOTE_TYPE_NAME\",\n",
        "    \"MAX_YEARS_EXPERIENCE\",\n",
        "    \"SALARY_FROM\",\n",
        "    \"LOT_V6_SPECIALIZED_OCCUPATION_NAME\"\n",
        ").filter(\n",
        "    (col(\"MAX_YEARS_EXPERIENCE\").isNotNull()) &\n",
        "    (col(\"SALARY_FROM\").isNotNull()) &\n",
        "    (col(\"SALARY_FROM\") > 0) &\n",
        "    (col(\"MAX_YEARS_EXPERIENCE\") <= 30) &\n",
        "    (col(\"LOT_V6_SPECIALIZED_OCCUPATION_NAME\").isNotNull())\n",
        ")\n",
        "\n",
        "# Convert to Pandas\n",
        "remote_pd = remote_spark.toPandas()\n",
        "\n",
        "print(f\"Records for remote work analysis: {len(remote_pd):,}\")\n",
        "\n",
        "# Create remote work categories as specified in assignment\n",
        "def categorize_remote(remote_type):\n",
        "    if pd.isna(remote_type) or remote_type == '[None]' or remote_type == '' or remote_type is None:\n",
        "        return 'Onsite'\n",
        "    elif 'Remote' in str(remote_type):\n",
        "        return 'Remote'\n",
        "    elif 'Hybrid' in str(remote_type):\n",
        "        return 'Hybrid'\n",
        "    else:\n",
        "        return 'Onsite'\n",
        "\n",
        "remote_pd['remote_category'] = remote_pd['REMOTE_TYPE_NAME'].apply(categorize_remote)\n",
        "\n",
        "# Print distribution\n",
        "print(\"Remote work distribution:\")\n",
        "print(remote_pd['remote_category'].value_counts())\n",
        "\n",
        "# Create subplot structure\n",
        "fig4 = make_subplots(\n",
        "    rows=2, cols=3,\n",
        "    subplot_titles=(\n",
        "        \"Remote - Experience vs Salary\",\n",
        "        \"Hybrid - Experience vs Salary\",\n",
        "        \"Onsite - Experience vs Salary\",\n",
        "        \"Remote - Salary Distribution\",\n",
        "        \"Hybrid - Salary Distribution\",\n",
        "        \"Onsite - Salary Distribution\"\n",
        "    ),\n",
        "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "           [{\"secondary_y\": False}, {\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
        ")\n",
        "\n",
        "# Add jitter as specified\n",
        "np.random.seed(42)\n",
        "remote_pd = remote_pd.copy()\n",
        "remote_pd['experience_jitter'] = remote_pd['MAX_YEARS_EXPERIENCE'] + np.random.normal(0, 0.3, len(remote_pd))\n",
        "\n",
        "# Define colors for each remote type\n",
        "colors = {'Remote': '#27AE60', 'Hybrid': '#F39C12', 'Onsite': '#8E44AD'}\n",
        "\n",
        "# Create scatter plots and histograms for each remote type\n",
        "for i, remote_type in enumerate(['Remote', 'Hybrid', 'Onsite']):\n",
        "    data = remote_pd[remote_pd['remote_category'] == remote_type]\n",
        "\n",
        "    if len(data) > 0:\n",
        "        print(f\"{remote_type}: {len(data):,} records\")\n",
        "\n",
        "        # Scatter plot with jitter (using go.Scatter for consistency)\n",
        "        fig4.add_trace(\n",
        "            go.Scatter(\n",
        "                x=data['experience_jitter'],\n",
        "                y=data['SALARY_FROM'],\n",
        "                mode='markers',\n",
        "                name=f\"{remote_type}\",\n",
        "                marker=dict(color=colors[remote_type], size=4, opacity=0.6),\n",
        "                hovertemplate=f'{remote_type}<br>Experience: %{{x:.1f}} years<br>Salary: $%{{y:,.0f}}<extra></extra>',\n",
        "                showlegend=False\n",
        "            ),\n",
        "            row=1, col=i+1\n",
        "        )\n",
        "\n",
        "        # Histogram\n",
        "        fig4.add_trace(\n",
        "            go.Histogram(\n",
        "                x=data['SALARY_FROM'],\n",
        "                name=f\"{remote_type} Distribution\",\n",
        "                marker_color=colors[remote_type],\n",
        "                opacity=0.7,\n",
        "                nbinsx=20,\n",
        "                showlegend=False\n",
        "            ),\n",
        "            row=2, col=i+1\n",
        "        )\n",
        "\n",
        "# Update layout\n",
        "fig4.update_layout(\n",
        "    title=\"Salary Analysis by Remote Work Type\",\n",
        "    template=\"nike\",\n",
        "    font_family=\"Helvetica\",\n",
        "    title_font_size=16,\n",
        "    title_font_color=\"#2C3E50\",\n",
        "    plot_bgcolor=\"white\",\n",
        "    paper_bgcolor=\"white\",\n",
        "    height=800\n",
        ")\n",
        "\n",
        "# Update axes labels\n",
        "for i in range(1, 4):\n",
        "    fig4.update_xaxes(title_text=\"Years of Experience\", row=1, col=i)\n",
        "    fig4.update_yaxes(title_text=\"Salary ($)\", row=1, col=i)\n",
        "    fig4.update_xaxes(title_text=\"Salary ($)\", row=2, col=i)\n",
        "    fig4.update_yaxes(title_text=\"Count\", row=2, col=i)\n",
        "\n",
        "fig4.show()\n",
        "\n",
        "# Save figure\n",
        "fig4.write_html(\"images/04_remote_work_analysis.html\")\n",
        "try:\n",
        "    fig4.write_image(\"images/04_remote_work_analysis.png\", width=1400, height=800, scale=2)\n",
        "    print(\"Figure 4 saved as HTML and PNG\")\n",
        "except Exception as e:\n",
        "    print(f\"Figure 4 saved as HTML (PNG export failed: {e})\")\n",
        "\n",
        "# Summary statistics by remote type\n",
        "print(\"\\nSalary summary by remote work type:\")\n",
        "for remote_type in ['Remote', 'Hybrid', 'Onsite']:\n",
        "    data = remote_pd[remote_pd['remote_category'] == remote_type]['SALARY_FROM']\n",
        "    if len(data) > 0:\n",
        "        print(f\"{remote_type}: Mean=${data.mean():,.0f}, Median=${data.median():,.0f}, Count={len(data):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remote work positions demonstrate the highest salary potential with considerable variance, indicating premium compensation for distributed work capabilities. Hybrid arrangements provide competitive salaries while maintaining work flexibility, whereas onsite positions cluster around lower median compensation levels.\n",
        "\n",
        "The analysis reveals a clear hierarchy in remote work compensation, with fully remote positions commanding the highest salaries. This premium likely reflects the specialized skills required for effective remote collaboration and the market's recognition of the value of location-independent work capabilities.\n",
        "\n",
        "Hybrid arrangements offer a balanced approach, providing competitive compensation while maintaining some degree of workplace interaction. Onsite positions, while essential for many roles, generally offer lower compensation levels, reflecting the traditional employment model's baseline expectations.\n",
        "\n",
        "These findings have significant implications for work arrangement preferences and compensation negotiations in an increasingly flexible labor market."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
