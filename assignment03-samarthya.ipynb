{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Assignment 03: Big Data Visualization on Scale\"\n",
        "author:\n",
        "  - name: Saurabh Sharma\n",
        "    affiliations:\n",
        "      - id: bu\n",
        "        name: Boston University\n",
        "        city: Boston\n",
        "        state: MA\n",
        "number-sections: true\n",
        "date: today\n",
        "format:\n",
        "  html:\n",
        "    theme: cerulean\n",
        "    toc: true\n",
        "    lsof: true\n",
        "    toc-depth: 2\n",
        "    code-overflow: wrap\n",
        "    code-fold: true\n",
        "    code-line-numbers: true\n",
        "    fig-width: 12\n",
        "    fig-height: 8\n",
        "    fig-dpi: 150\n",
        "    execute:\n",
        "      eval: true\n",
        "    fig-cap-location: bottom\n",
        "  docx:\n",
        "    execute:\n",
        "      eval: true\n",
        "      echo: true\n",
        "    fig-format: svg\n",
        "    fig-width: 8\n",
        "    fig-height: 6\n",
        "    fig-dpi: 300\n",
        "    code-fold: true\n",
        "    code-overflow: wrap\n",
        "    code-line-numbers: true\n",
        "  pdf:\n",
        "    execute:\n",
        "      eval: true\n",
        "      echo: true\n",
        "    lof: true\n",
        "    fig-width: 8\n",
        "    fig-height: 6\n",
        "    fig-dpi: 300\n",
        "    fig-align: left\n",
        "    fig-format: svg\n",
        "    code-fold: true\n",
        "    code-overflow: wrap\n",
        "    code-line-numbers: true\n",
        "date-modified: today\n",
        "date-format: long\n",
        "execute:\n",
        "  echo: true\n",
        "  eval: true\n",
        "  cache: false\n",
        "---\n",
        "\n",
        "# Executive Summary\n",
        "\n",
        "This comprehensive analysis examines salary distributions and employment trends across the modern job market using the Lightcast dataset, encompassing over 72,000 job postings. Through advanced data visualization techniques, this report reveals stark compensation disparities that shape career opportunities and economic mobility in today's labor market.\n",
        "\n",
        "## Key Compensation Disparities Revealed\n",
        "\n",
        "### Industry Sector Premiums\n",
        "\n",
        "Technology and professional services sectors command higher median salaries than manufacturing and retail sectors (@fig-industry-employment). Full-time positions consistently yield higher compensation than part-time roles across all industries, creating a significant earnings gap that favors both high-value sectors and stable employment arrangements.\n",
        "\n",
        "### Occupational Compensation Hierarchy\n",
        "\n",
        "Specialized technical and management roles exhibit diverse median salaries ranging from $45,000 to $125,000, reflecting true market compensation patterns across different LOT occupation categories (@fig-onet-bubble). The enhanced bubble chart analysis, powered by occupation-specific salary imputation, reveals that engineering, healthcare, and technology occupations not only command the highest compensation but also represent the strongest job market demand across 25+ distinct occupation categories.\n",
        "\n",
        "### Education Investment Returns\n",
        "\n",
        "Advanced degree holders (Master's and PhD) achieve 20-35% higher salaries than Bachelor's degree recipients, with the compensation premium expanding to 40-50% for those with 10+ years of experience (@fig-education-analysis). The scatter plot analysis demonstrates that higher education provides both immediate salary boosts and accelerated long-term career progression.\n",
        "\n",
        "### Remote Work Compensation Dynamics\n",
        "\n",
        "Remote positions offer the highest salary potential with 15-25% premiums over onsite roles, though with greater earnings variance (@fig-remote-work). Hybrid arrangements provide competitive compensation while maintaining work-life balance, representing an optimal middle ground in the evolving work arrangement landscape.\n",
        "\n",
        "## Analytical Foundation\n",
        "\n",
        "This report employs five comprehensive interactive visualizations to illuminate compensation patterns:\n",
        "\n",
        "- **Industry-Employment Analysis**: Box plots revealing sector-specific salary distributions and employment type impacts\n",
        "- **Employment Type Focus**: Dedicated analysis of full-time vs. part-time vs. contract compensation structures\n",
        "- **Occupational Bubble Chart**: LOT occupation analysis showing compensation-demand relationships across 25+ diverse occupation categories with occupation-specific salary imputation\n",
        "<!-- Enhanced from ONET to LOT occupation classification for greater granularity -->\n",
        "- **Education Impact Assessment**: Multi-panel analysis of education level effects on salary trajectories\n",
        "- **Remote Work Economics**: Three-way comparison of remote, hybrid, and onsite compensation patterns\n",
        "\n",
        "## Analytical Approach\n",
        "\n",
        "This report employs Apache Spark for large-scale data processing, ensuring efficient handling of the comprehensive dataset.\n",
        "\n",
        "<!-- Updated to reflect improved imputation strategy using occupation-employment combinations -->\n",
        "\n",
        "# Introduction\n",
        "\n",
        "## Background and Context\n",
        "\n",
        "<!-- Google Copied -->\n",
        "\n",
        "The contemporary job market has undergone significant transformation in recent years, driven by technological advancement, changing work patterns, and evolving employer expectations. Understanding salary distributions and employment trends across different sectors, occupations, and work arrangements has become crucial for multiple stakeholders in the labor ecosystem.\n",
        "\n",
        "## Research Objectives\n",
        "\n",
        "This analysis aims to address four primary research questions:\n",
        "\n",
        "1. **Industry and Employment Type Analysis**: How do salary distributions vary across different industry sectors and employment types? This investigation examines whether certain industries offer premium compensation and how employment arrangements (full-time, part-time, contract) influence earning potential (@fig-industry-employment, @fig-employment-type).\n",
        "\n",
        "2. **Occupational Compensation Patterns**: Which occupational categories command the highest compensation, and what is the relationship between job demand and salary levels? This analysis explores how specialized skills and experience translate into market value (@fig-onet-bubble).\n",
        "\n",
        "3. **Education Impact Assessment**: How do different education levels influence salary trajectories and earning potential? The study examines the return on investment for various educational credentials across different career stages (@fig-education-analysis).\n",
        "\n",
        "4. **Remote Work Compensation Analysis**: What are the salary implications of different work arrangements (remote, hybrid, onsite)? This research investigates whether remote work commands premium compensation or if it represents a trade-off between flexibility and earnings (@fig-remote-work).\n",
        "\n",
        "## Dataset Overview\n",
        "\n",
        "The Lightcast dataset provides a comprehensive view of job postings and associated compensation data, enabling detailed analysis of market trends. This dataset captures real-time job market dynamics, including:\n",
        "\n",
        "- **Industry Classifications**: NAICS codes providing standardized industry categorization\n",
        "- **Occupational Categories**: ONET codes offering detailed occupational taxonomy\n",
        "- **Educational Requirements**: Minimum and maximum education level specifications\n",
        "- **Experience Levels**: Years of experience requirements and preferences\n",
        "- **Compensation Information**: Salary ranges, bonuses, and benefits data\n",
        "- **Work Arrangements**: Remote, hybrid, and onsite work specifications\n",
        "\n",
        "This rich dataset enables multi-dimensional analysis of labor market trends and compensation patterns, offering insights that are both timely and comprehensive.\n",
        "\n",
        "## Significance of the Study\n",
        "\n",
        "The findings from this analysis have important implications for multiple stakeholders:\n",
        "\n",
        "- **Job Seekers**: Understanding compensation patterns helps individuals make informed career decisions and negotiate competitive salaries based on their qualifications and experience.\n",
        "\n",
        "- **Employers**: Organizations can benchmark their compensation packages against market standards and ensure competitive offerings to attract and retain talent.\n",
        "\n",
        "- **Policy Makers**: Government agencies can use these insights to inform workforce development programs, education policy, and labor market regulations.\n",
        "\n",
        "- **Educational Institutions**: Academic programs can align curricula with market demands and provide students with realistic expectations about career outcomes.\n",
        "\n",
        "This analysis contributes to the broader understanding of how the modern job market rewards different skills, credentials, and work arrangements in an increasingly digitized economy.\n",
        "\n",
        "# Methodology\n",
        "\n",
        "## Data Processing Framework\n",
        "\n",
        "The analysis employs Apache Spark for large-scale data processing, enabling efficient handling of the comprehensive job postings dataset. Spark's distributed computing capabilities ensure that even datasets with hundreds of thousands of records can be processed efficiently without memory constraints.\n",
        "\n",
        "The methodology follows a systematic approach designed to ensure data quality, analytical rigor, and reproducible results:\n",
        "\n",
        "1. **Data Loading and Validation**: Initial dataset loading with schema verification and data quality assessment\n",
        "2. **Data Cleaning and Preprocessing**: Character encoding correction, missing value handling, and outlier identification\n",
        "3. **Feature Engineering**: Creation of analytical groupings and derived variables for enhanced analysis\n",
        "4. **Statistical Analysis**: Descriptive statistics and distributional analysis across multiple dimensions\n",
        "5. **Visualization Development**: Custom plotly-based visualizations with professional styling and accessibility features\n",
        "\n",
        "## Analytical Approach\n",
        "\n",
        "Each research question is addressed through specific analytical techniques tailored to the nature of the data and research objectives:\n",
        "\n",
        "- **Box plots** for comparing salary distributions across categorical variables, providing insights into central tendency, spread, and outliers (@fig-industry-employment, @fig-employment-type)\n",
        "- **Bubble charts** for examining relationships between multiple quantitative variables, with bubble size representing job volume and enhanced LOT occupation categorization (@fig-onet-bubble)\n",
        "<!-- Enhanced with LOT_OCCUPATION_NAME for 25+ distinct occupation categories -->\n",
        "- **Scatter plots with jitter** for analyzing continuous variable relationships while avoiding overplotting (@fig-education-analysis, @fig-remote-work)\n",
        "- **Histograms** for understanding distributional characteristics and identifying skewness or multimodality (@fig-education-analysis, @fig-remote-work)\n",
        "\n",
        "All visualizations employ custom color schemes and professional formatting to ensure clarity, accessibility, and visual appeal.\n",
        "\n",
        "## Data Quality Considerations\n",
        "\n",
        "The analysis addresses several data quality challenges that are common in large-scale job market datasets:\n",
        "\n",
        "- **Character Encoding Issues**: Text fields containing non-ASCII characters are systematically cleaned using `regex` patterns to ensure compatibility across different systems and visualization tools.\n",
        "\n",
        "- **Enhanced Missing Value Handling**: Rather than discarding records with missing salary information, we employ a `hierarchical occupation-employment` type specific median imputation strategy. This three-tier approach uses:\n",
        "  1. **Primary**: LOT_OCCUPATION_NAME + EMPLOYMENT_TYPE_NAME combination medians\n",
        "  2. **Fallback**: EMPLOYMENT_TYPE_NAME only medians\n",
        "  3. **Final**: Overall dataset median\n",
        "  This approach preserves sample size while maintaining realistic salary variation across different occupation categories.\n",
        "\n",
        "<!-- Upgraded from simple employment-type grouping to occupation-employment combinations for more accurate imputation -->\n",
        "\n",
        "- **Outlier Management**: Experience data is capped at reasonable thresholds (30 years) to prevent extreme values from skewing visualizations and statistical summaries.\n",
        "\n",
        "- **Sample Size Considerations**: Statistical analyses focus on groups with sufficient observations to ensure meaningful results and avoid spurious findings.\n",
        "\n",
        "## Technical Implementation\n",
        "\n",
        "The analysis leverages modern data science tools and best practices:\n",
        "\n",
        "- **Apache Spark**: For scalable data processing and transformation operations\n",
        "- **PySpark SQL Functions**: For efficient data manipulation and aggregation\n",
        "- **Plotly**: For interactive, publication-quality visualizations\n",
        "- **Pandas Integration**: For statistical computations requiring single-machine processing\n",
        "- **Jupyter/Quarto Integration**: For reproducible research and automated report generation\n",
        "\n",
        "<!-- Added enhanced imputation methodology and LOT occupation classification improvements -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: imports-included\n",
        "#| code-fold: true\n",
        "#| echo: false\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import warnings\n",
        "import builtins\n",
        "import kaleido\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: spark-session-initialization\n",
        "#| code-fold: true\n",
        "#| echo: false\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set default template for better appearance\n",
        "pio.templates.default = \"plotly_white\"\n",
        "\n",
        "# Custom Plotly Template\n",
        "# will test it later\n",
        "pio.templates[\"nike\"] = go.layout.Template(\n",
        "    # LAYOUT\n",
        "    layout = {\n",
        "        # Fonts\n",
        "        # Note - 'family' must be a single string, NOT a list or dict!\n",
        "        'title':\n",
        "            {'font': {'family': 'HelveticaNeue-CondensedBold, Helvetica, Sans-serif',\n",
        "                      'size':30,\n",
        "                      'color': '#333'}\n",
        "            },\n",
        "        'font': {'family': 'Helvetica Neue, Helvetica, Sans-serif',\n",
        "                      'size':16,\n",
        "                      'color': '#333'},\n",
        "        # Colorways\n",
        "        'colorway': ['#ec7424', '#a4abab'],\n",
        "        # Keep adding others as needed below\n",
        "        'hovermode': 'x unified'\n",
        "    },\n",
        "    # DATA\n",
        "    data = {\n",
        "        # Each graph object must be in a tuple or list for each trace\n",
        "        'bar': [go.Bar(texttemplate = '%{value:$.2s}',\n",
        "                       textposition='outside',\n",
        "                       textfont={'family': 'Helvetica Neue, Helvetica, Sans-serif',\n",
        "                                 'size': 20,\n",
        "                                 'color': '#FFFFFF'\n",
        "                                 })]\n",
        "    }\n",
        ")\n",
        "\n",
        "# Common modebar configuration\n",
        "MODEBAR_CONFIG = {\n",
        "    'displayModeBar': True,\n",
        "    'displaylogo': False,\n",
        "    'modeBarButtonsToRemove': ['pan2d', 'lasso2d', 'select2d'],\n",
        "    'modeBarButtonsToAdd': ['zoomIn2d', 'zoomOut2d', 'resetScale2d', 'toImage']\n",
        "}\n",
        "\n",
        "# Set up renderers\n",
        "pio.renderers[\"notebook\"].config = MODEBAR_CONFIG\n",
        "pio.renderers[\"png\"].config = {\n",
        "    'width': 1400,\n",
        "    'height': 800,\n",
        "    'scale': 2\n",
        "}\n",
        "pio.renderers[\"svg\"].config = {\n",
        "    'width': 1400,\n",
        "    'height': 800\n",
        "}\n",
        "\n",
        "# Create images directory\n",
        "os.makedirs('images', exist_ok=True)\n",
        "\n",
        "# Set up both renderers\n",
        "pio.renderers.default = \"svg+notebook\"  # Default to interactive for HTML\n",
        "\n",
        "# Centralized image generation function\n",
        "def generate_all_formats(fig, filename_base):\n",
        "    \"\"\"Generate all image formats (PNG, SVG, HTML) for a figure using configured renderers\"\"\"\n",
        "    try:\n",
        "        # Generate static images using pre-configured renderers\n",
        "        fig.write_image(f\"images/{filename_base}.png\")\n",
        "        fig.write_image(f\"images/{filename_base}.svg\")\n",
        "        \n",
        "        # Generate interactive HTML\n",
        "        fig.write_html(f\"images/{filename_base}.html\")\n",
        "        print(f\"All formats generated: {filename_base}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Image generation failed for {filename_base}: {e}\")\n",
        "        return False\n",
        "\n",
        "# Display function that adapts to output format\n",
        "def display_figure(fig, filename_base):\n",
        "    \"\"\"Display figure with appropriate renderer based on output format\"\"\"\n",
        "    # Generate all image formats first\n",
        "    generate_all_formats(fig, filename_base)\n",
        "    # Display static image (for PDF/DOCX)\n",
        "    print(f\"Static image available: images/{filename_base}.png\")\n",
        "    fig.show()\n",
        "\n",
        "# Initialize Spark Session with increased memory settings\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"LightcastData\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .config(\"spark.sql.debug.maxToStringFields\", \"1000\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark session initialized with optimized memory settings\")\n",
        "\n",
        "# Load Data - using the correct path format\n",
        "df = spark.read.option(\"multiLine\", \"true\").option(\"escape\", \"\\\"\").csv(\"./data/lightcast_job_postings.csv\", header=True, inferSchema=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data set loaded successfully"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: dataset-loaded-successfully\n",
        "#| echo: false\n",
        "\n",
        "print(\"Dataset loaded successfully\")\n",
        "print(f\"Total records: {df.count():,}\")\n",
        "print(f\"Total columns: {len(df.columns)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: helper-functions\n",
        "#| echo: true\n",
        "#| eval: true\n",
        "\n",
        "output_format = os.environ.get('QUARTO_OUTPUT_FORMAT', 'html')\n",
        "print(f\"Output format detected: {output_format}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: enhance-plotly-figure\n",
        "#| echo: false\n",
        "\n",
        "# Helper function to add interactive features and export capabilities\n",
        "def enhance_plotly_figure(fig, title, filename_base, width=1400, height=800):\n",
        "    \"\"\"\n",
        "    Enhance a Plotly figure with interactive features and export capabilities\n",
        "    \"\"\"\n",
        "    # Update layout for better interactivity\n",
        "    fig.update_layout(\n",
        "    # Enhanced interactivity\n",
        "    dragmode='pan',\n",
        "    selectdirection='d',  # 'd' for diagonal selection\n",
        "\n",
        "    # Better mode bar with all export options\n",
        "    modebar_add=[\n",
        "        'zoomIn2d', 'zoomOut2d', 'pan2d', 'resetScale2d',\n",
        "        'toImage', 'downloadImage', 'toggleSpikelines'\n",
        "    ],\n",
        "    modebar_remove=['lasso2d', 'select2d', 'autoScale2d'],\n",
        "    modebar_orientation='v',\n",
        "    modebar_bgcolor='rgba(255,255,255,0.9)',\n",
        "    modebar_color='#2C3E50',\n",
        "\n",
        "    # Ensure interactive features are enabled\n",
        "    showlegend=True,\n",
        "    legend=dict(\n",
        "        orientation=\"v\",\n",
        "        yanchor=\"top\",\n",
        "        y=1,\n",
        "        xanchor=\"left\",\n",
        "        x=1.02\n",
        "    ),\n",
        "\n",
        "        # Enhanced hover\n",
        "        hoverlabel=dict(\n",
        "            bgcolor=\"white\",\n",
        "            font_size=11,\n",
        "            font_family=\"Arial, sans-serif\",\n",
        "            bordercolor=\"navy\"\n",
        "        ),\n",
        "\n",
        "        # Responsive design\n",
        "        autosize=True,\n",
        "\n",
        "        # Mobile responsiveness\n",
        "        margin=dict(\n",
        "            l=50, r=50, t=80, b=80,\n",
        "            pad=4\n",
        "        ),\n",
        "\n",
        "        # Mobile-friendly font sizes\n",
        "        font=dict(\n",
        "            family=\"Arial, sans-serif\",\n",
        "            size=11,\n",
        "            color=\"#2C3E50\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Generate all image formats using centralized function\n",
        "    generate_all_formats(fig, filename_base)\n",
        "\n",
        "    return fig\n",
        "\n",
        "print(\"Helper functions loaded successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> debug element for information sake (Not required at the moment)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: debug-element-1\n",
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "df.printSchema()\n",
        "df.show(5, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Spark information carried over from the assignment 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: spark-sversion\n",
        "#| code-fold: true\n",
        "\n",
        "print(\"Spark Session Created Successfully!\")\n",
        "print(f\"Spark Version: {spark.version}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data cleansing and imputation\n",
        "\n",
        "Importing the functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: importing-the-functions\n",
        "#| code-fold: true\n",
        "\n",
        "# Start with the original dataset\n",
        "df_clean = df\n",
        "original_count = df_clean.count()\n",
        "\n",
        "print(f\"Starting with: {original_count:,} records\")\n",
        "\n",
        "# Rule 1: Remove records with empty COMPANY_NAME\n",
        "print(f\"\\nRule 1: Removing records with empty company names\")\n",
        "if 'COMPANY_NAME' in df_clean.columns:\n",
        "    before_count = df_clean.count()\n",
        "    df_clean = df_clean.filter(\n",
        "        col('COMPANY_NAME').isNotNull() &\n",
        "        (trim(col('COMPANY_NAME')) != \"\")\n",
        "    )\n",
        "    after_count = df_clean.count()\n",
        "    removed = before_count - after_count\n",
        "    print(f\"   • Removed {removed:,} records with empty company names\")\n",
        "else:\n",
        "    print(f\"   • COMPANY_NAME column not found\")\n",
        "\n",
        "\n",
        "# Rule 2: Enhanced Salary Imputation with Hierarchical Occupation-Employment Strategy\n",
        "print(f\"\\nRule 2: Imputing missing salary values with occupation + employment type specific medians\")\n",
        "salary_columns_to_impute = ['SALARY', 'SALARY_FROM', 'SALARY_TO']\n",
        "\n",
        "for salary_col in salary_columns_to_impute:\n",
        "    if salary_col in df_clean.columns:\n",
        "        try:\n",
        "            # --- Step 1: Clean unrealistic outliers first ---\n",
        "            print(f\"   • Cleaning extreme outliers and unrealistic values for {salary_col}\")\n",
        "            df_clean = df_clean.withColumn(salary_col,\n",
        "                when((col(salary_col) < 15000) | (col(salary_col) > 500000), None)\n",
        "                .otherwise(col(salary_col))\n",
        "            )\n",
        "\n",
        "            # Count missing before imputation for reporting\n",
        "            before_missing = df_clean.filter((col(salary_col).isNull()) | (col(salary_col) <= 0)).count()\n",
        "\n",
        "            # --- Step 2: Compute group medians using Spark (no Python UDFs) ---\n",
        "            # Primary: median by LOT_OCCUPATION_NAME + EMPLOYMENT_TYPE_NAME\n",
        "            occupation_employment_medians = df_clean.filter(\n",
        "                (col(salary_col).isNotNull()) & (col(salary_col) > 0) &\n",
        "                (col(\"LOT_OCCUPATION_NAME\").isNotNull()) & (col(\"EMPLOYMENT_TYPE_NAME\").isNotNull())\n",
        "            ).groupBy(\"LOT_OCCUPATION_NAME\", \"EMPLOYMENT_TYPE_NAME\") \\\n",
        "                .agg(median(salary_col).alias(\"occ_emp_median\")) \\\n",
        "                .filter(col(\"occ_emp_median\").isNotNull())\n",
        "\n",
        "            # Fallback: median by EMPLOYMENT_TYPE_NAME only\n",
        "            employment_medians = df_clean.filter(\n",
        "                (col(salary_col).isNotNull()) & (col(salary_col) > 0) &\n",
        "                (col(\"EMPLOYMENT_TYPE_NAME\").isNotNull())\n",
        "            ).groupBy(\"EMPLOYMENT_TYPE_NAME\") \\\n",
        "                .agg(median(salary_col).alias(\"emp_median\")) \\\n",
        "                .filter(col(\"emp_median\").isNotNull())\n",
        "\n",
        "            # Final fallback: overall (approximate) median\n",
        "            overall_median = df_clean.filter(col(salary_col).isNotNull() & (col(salary_col) > 0)) \\\n",
        "                .approxQuantile(salary_col, [0.5], 0.01)[0]\n",
        "\n",
        "            # Print diagnostics\n",
        "            print(f\"   • {salary_col} overall median: ${overall_median:,.2f}\")\n",
        "            print(f\"   • Occupation+Employment specific medians: {occupation_employment_medians.count():,} combinations\")\n",
        "            print(f\"   • Employment type fallback medians: {employment_medians.count():,} types\")\n",
        "\n",
        "            # --- Step 3: Left-join medians back onto main table and apply hierarchical coalescing ---\n",
        "            # Join on both LOT_OCCUPATION_NAME and EMPLOYMENT_TYPE_NAME for primary median\n",
        "            df_with_meds = df_clean.join(\n",
        "                occupation_employment_medians,\n",
        "                on=[\"LOT_OCCUPATION_NAME\", \"EMPLOYMENT_TYPE_NAME\"],\n",
        "                how=\"left\"\n",
        "            )\n",
        "\n",
        "            # Join employment-only medians as a fallback\n",
        "            df_with_meds = df_with_meds.join(\n",
        "                employment_medians,\n",
        "                on=[\"EMPLOYMENT_TYPE_NAME\"],\n",
        "                how=\"left\"\n",
        "            )\n",
        "\n",
        "            # Create imputed value using hierarchical coalesce: occ_emp_median -> emp_median -> overall_median\n",
        "            # Add a small random jitter to the imputed median to avoid too many identical values.\n",
        "            # Jitter magnitude is +/- 2.5% (controlled by jitter_scale)\n",
        "            jitter_scale = 0.025\n",
        "            seed = 42\n",
        "            imputed_expr = coalesce(col(\"occ_emp_median\"), col(\"emp_median\"), lit(float(overall_median)))\n",
        "            # Apply multiplicative jitter: imputed * (1 + (rand() - 0.5) * 2 * jitter_scale)\n",
        "            df_with_meds = df_with_meds.withColumn(\n",
        "                \"imputed_value\",\n",
        "                (imputed_expr * (1 + (rand(seed) - lit(0.5)) * lit(2.0 * jitter_scale))).cast(FloatType())\n",
        "            )\n",
        "\n",
        "            # Replace missing/invalid salary values with the imputed_value, keep others as-is\n",
        "            df_clean = df_with_meds.withColumn(\n",
        "                salary_col,\n",
        "                when((col(salary_col).isNull()) | (col(salary_col) <= 0), col(\"imputed_value\"))\n",
        "                .otherwise(col(salary_col))\n",
        "            ).drop(\"occ_emp_median\", \"emp_median\", \"imputed_value\")\n",
        "\n",
        "            # Count how many values were imputed (difference before/after)\n",
        "            after_missing = df_clean.filter((col(salary_col).isNull()) | (col(salary_col) <= 0)).count()\n",
        "            imputed_count = max(0, before_missing - after_missing)\n",
        "            print(f\"   • Imputed {imputed_count:,} missing/invalid {salary_col} values with hierarchical medians (with jitter)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   • Error in group-specific imputation for {salary_col}: {str(e)}\")\n",
        "            # Fallback to overall median (no jitter) if anything goes wrong\n",
        "            overall_median = df_clean.filter(col(salary_col).isNotNull() & (col(salary_col) > 0)) \\\n",
        "                .approxQuantile(salary_col, [0.5], 0.01)[0]\n",
        "            print(f\"   • Using overall median fallback: ${overall_median:,.2f}\")\n",
        "            df_clean = df_clean.withColumn(salary_col,\n",
        "                when((col(salary_col).isNull()) | (col(salary_col) <= 0), lit(float(overall_median))).otherwise(col(salary_col))\n",
        "            )\n",
        "    else:\n",
        "        print(f\"   • {salary_col} column not found\")\n",
        "\n",
        "# Rule 3: Clean EMPLOYMENT_TYPE_NAME by removing non-ASCII characters\n",
        "print(f\"\\nRule 3: Cleaning EMPLOYMENT_TYPE_NAME non-ASCII characters\")\n",
        "if 'EMPLOYMENT_TYPE_NAME' in df_clean.columns:\n",
        "    # Count records with non-ASCII characters before cleaning\n",
        "    non_ascii_count = df_clean.filter(col('EMPLOYMENT_TYPE_NAME').rlike('[^\\x00-\\x7f]')).count()\n",
        "\n",
        "    df_clean = df_clean.withColumn('EMPLOYMENT_TYPE_NAME',\n",
        "        regexp_replace(col('EMPLOYMENT_TYPE_NAME'), '([^\\x00-\\x7f])', ''))\n",
        "    # Verify cleaning\n",
        "    after_count = df_clean.filter(col('EMPLOYMENT_TYPE_NAME').rlike('[^\\x00-\\x7f]')).count()\n",
        "    print(f\"   • After cleaning: {after_count} records with non-ASCII characters\")\n",
        "    print(f\"   • Successfully cleaned {non_ascii_count - after_count:,} records\")\n",
        "\n",
        "# Rule 4: Clean MIN_EDULEVELS_NAME by removing newline and carriage return characters\n",
        "print(f\"\\nRule 4: Cleaning MIN_EDULEVELS_NAME newline and carriage return characters\")\n",
        "if 'MIN_EDULEVELS_NAME' in df_clean.columns:\n",
        "    # Count records with newline/carriage return characters before cleaning\n",
        "    newline_count = df_clean.filter(col('MIN_EDULEVELS_NAME').rlike('[\\n\\r]')).count()\n",
        "    print(f\"   • Found {newline_count:,} records with newline/carriage return characters in MIN_EDULEVELS_NAME\")\n",
        "\n",
        "    # Remove newline and carriage return characters using regex\n",
        "    df_clean = df_clean.withColumn('MIN_EDULEVELS_NAME',\n",
        "        regexp_replace(col('MIN_EDULEVELS_NAME'), '[\\n\\r]', ''))\n",
        "\n",
        "    # Verify cleaning\n",
        "    after_count = df_clean.filter(col('MIN_EDULEVELS_NAME').rlike('[\\n\\r]')).count()\n",
        "    print(f\"   • After cleaning: {after_count} records with newline/carriage return characters\")\n",
        "    print(f\"   • Successfully cleaned {newline_count - after_count:,} records\")\n",
        "\n",
        "\n",
        "# Final Results\n",
        "final_count = df_clean.count()\n",
        "total_removed = original_count - final_count\n",
        "retention_rate = (final_count / original_count) * 100\n",
        "\n",
        "print(f\"DATA CLEANING RESULTS SUMMARY\")\n",
        "print(f\"Original dataset:  {original_count:,} rows\")\n",
        "print(f\"Cleaned dataset:   {final_count:,} rows\")\n",
        "if total_removed > 0:\n",
        "    print(f\"Total removed:     {total_removed:,} rows ({100-retention_rate:.1f}%)\")\n",
        "    print(f\"Data retention:    {retention_rate:.1f}%\")\n",
        "else:\n",
        "    print(f\"Total removed:     0 rows (0.0%)\")\n",
        "    print(f\"Data retention:    100.0%\")\n",
        "    print(f\"Note: All records retained with missing values imputed\")\n",
        "\n",
        "# Update the main dataframe\n",
        "df = df_clean\n",
        "print(f\"\\nSimple data cleaning completed. Dataset ready for analysis.\")\n",
        "\n",
        "# Update the main dataframe to use cleaned version\n",
        "df = df_clean\n",
        "print(f\"\\nData cleaning completed. Using cleaned dataset for analysis.\")\n",
        "\n",
        "# Cleansing as learnt\n",
        "# Get column names and types (memory-efficient approach)\n",
        "columns_and_types = [(col, str(dtype)) for col, dtype in df.dtypes]\n",
        "\n",
        "# Write schema directly to CSV without creating intermediate Spark DataFrame\n",
        "\n",
        "with open(\"./data/column_schema.csv\", \"w\", newline=\"\") as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow([\"Column_Name\", \"Data_Type\"])  # Header\n",
        "    writer.writerows(columns_and_types)\n",
        "\n",
        "print(f\"Column schema exported to ./data/column_schema.csv ({len(columns_and_types)} columns)\")\n",
        "\n",
        "# Cast salary columns to Float with error handling\n",
        "salary_columns = [\"SALARY\", \"SALARY_FROM\", \"SALARY_TO\", \"MIN_YEARS_EXPERIENCE\", \"MAX_YEARS_EXPERIENCE\"]\n",
        "\n",
        "for col_name in salary_columns:\n",
        "    if col_name in df.columns:\n",
        "        df = df.withColumn(col_name, col(col_name).cast(\"Float\"))\n",
        "        print(f\"Cast {col_name} to Float\")\n",
        "    else:\n",
        "        print(f\"Column {col_name} not found in dataset\")\n",
        "\n",
        "# print(f\"\\nCasting completed. Updated schema:\")\n",
        "# df.select(salary_columns).printSchema()\n",
        "# df.toPandas().to_csv(\"./data/pandas_csv.csv\", index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::{.content-hidden unless-format=\"html\"}\n",
        "> Debug information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: debug-element-2\n",
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# Calculate median salaries by employment type (using only original non-null values)\n",
        "median_salaries = df.filter(col(\"SALARY\").isNotNull() & (col(\"SALARY\") > 0)) \\\n",
        "    .groupBy(\"EMPLOYMENT_TYPE_NAME\") \\\n",
        "    .agg(median(\"SALARY\").alias(\"median_salary\")) \\\n",
        "    .orderBy(col(\"median_salary\").desc())\n",
        "\n",
        "print(\"Median salaries by employment type (original data only):\")\n",
        "median_salaries.show(20, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "# Salary Distribution by Industry and Employment Type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-salary-histogram\n",
        "#| fig-cap: \"Histogram showing the distribution of salary data from a representative sample of the dataset.\"\n",
        "\n",
        "# NOTE: For large datasets, we sample data for visualizations to avoid memory issues\n",
        "\n",
        "# The full dataset has {df.count():,} records - sampling prevents OutOfMemoryError\n",
        "# Sample data for histogram to avoid memory issues (sample 10% or max 10,000 records)\n",
        "sample_fraction = builtins.min(0.1, 10000.0 / df.count())  # Sample 10% or enough for 10k records\n",
        "salary_sample = df.select(\"SALARY\").filter(col(\"SALARY\").isNotNull()).sample(fraction=sample_fraction, seed=42)\n",
        "\n",
        "# Create interactive histogram with Plotly\n",
        "fig = px.histogram(salary_sample.toPandas(), x=\"SALARY\", nbins=50,\n",
        "                   title=\"Salary Distribution (Sampled)\",\n",
        "                   labels={'SALARY': 'Salary ($)', 'count': 'Frequency'},\n",
        "                   color_discrete_sequence=['#1f77b4'])\n",
        "\n",
        "# Update layout for better appearance\n",
        "fig.update_layout(\n",
        "    bargap=0.1,\n",
        "    xaxis_title=\"Salary ($)\",\n",
        "    yaxis_title=\"Frequency\",\n",
        "    hovermode='x unified',\n",
        "    showlegend=False\n",
        ")\n",
        "\n",
        "# Add interactive features directly (since enhance_plotly_figure might not be available)\n",
        "fig.update_layout(\n",
        "    # Enhanced interactivity\n",
        "    dragmode='pan',\n",
        "    selectdirection='d',\n",
        "\n",
        "    # Better mode bar with all export options\n",
        "    modebar_add=[\n",
        "        'zoomIn2d', 'zoomOut2d', 'pan2d', 'resetScale2d',\n",
        "        'toImage', 'downloadImage', 'toggleSpikelines'\n",
        "    ],\n",
        "    modebar_remove=['lasso2d', 'select2d', 'autoScale2d'],\n",
        "    modebar_orientation='v',\n",
        "    modebar_bgcolor='rgba(255,255,255,0.9)',\n",
        "    modebar_color='#2C3E50',\n",
        "\n",
        "    # Enhanced hover\n",
        "    hoverlabel=dict(\n",
        "        bgcolor=\"white\",\n",
        "        font_size=11,\n",
        "        font_family=\"Arial, sans-serif\",\n",
        "        bordercolor=\"navy\"\n",
        "    ),\n",
        "\n",
        "    # Responsive design\n",
        "    autosize=True,\n",
        "\n",
        "    # Mobile responsiveness\n",
        "    margin=dict(l=50, r=50, t=80, b=80, pad=4),\n",
        "\n",
        "    # Mobile-friendly font sizes\n",
        "    font=dict(\n",
        "        family=\"Arial, sans-serif\",\n",
        "        size=11,\n",
        "        color=\"#2C3E50\"\n",
        "    )\n",
        ")\n",
        "\n",
        "display_figure(fig, \"salary_histogram\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Salary Distribution by Industry and Employment Type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-industry-employment\n",
        "#| fig-cap: \"Interactive box plot showing salary distributions across industries and employment types.\"\n",
        "\n",
        "\n",
        "salary_industry_spark = df.select(\n",
        "    \"NAICS2_NAME\",\n",
        "    \"SALARY_FROM\",\n",
        "    \"EMPLOYMENT_TYPE_NAME\"\n",
        ").filter(\n",
        "    (col(\"SALARY_FROM\").isNotNull()) &\n",
        "    (col(\"SALARY_FROM\") > 0) &\n",
        "    (col(\"NAICS2_NAME\").isNotNull()) &\n",
        "    (col(\"EMPLOYMENT_TYPE_NAME\").isNotNull())\n",
        ")\n",
        "\n",
        "print(f\"Records with valid salary data: {salary_industry_spark.count():,}\")\n",
        "\n",
        "# Convert to Pandas for visualization\n",
        "salary_industry_pd = salary_industry_spark.toPandas()\n",
        "\n",
        "# Limit to top industries by job count for readability\n",
        "top_industries = salary_industry_pd['NAICS2_NAME'].value_counts().head(10).index\n",
        "salary_industry_filtered = salary_industry_pd[salary_industry_pd['NAICS2_NAME'].isin(top_industries)]\n",
        "\n",
        "# North American Industry classification system\n",
        "# Create box plot using go.Box for consistency\n",
        "fig1 = go.Figure()\n",
        "\n",
        "# Get unique employment types and industries for consistent colors\n",
        "employment_types = salary_industry_filtered['EMPLOYMENT_TYPE_NAME'].unique()\n",
        "industries = salary_industry_filtered['NAICS2_NAME'].unique()\n",
        "\n",
        "# Define colors for employment types\n",
        "colors = ['#ec7424', '#a4abab', '#2E86AB', '#A23B72', '#F18F01']\n",
        "\n",
        "# Add box plots for each employment type across industries\n",
        "for i, emp_type in enumerate(employment_types):\n",
        "    emp_data = salary_industry_filtered[salary_industry_filtered['EMPLOYMENT_TYPE_NAME'] == emp_type]\n",
        "\n",
        "    for industry in industries:\n",
        "        industry_data = emp_data[emp_data['NAICS2_NAME'] == industry]\n",
        "        if len(industry_data) > 0:\n",
        "            # Sample data if too many points to avoid overplotting\n",
        "            if len(industry_data) > 1000:\n",
        "                industry_data = industry_data.sample(n=1000, random_state=42)\n",
        "\n",
        "            fig1.add_trace(go.Box(\n",
        "                y=industry_data['SALARY_FROM'],\n",
        "                x=[industry] * len(industry_data),\n",
        "                name=emp_type,\n",
        "                legendgroup=emp_type,\n",
        "                showlegend=(industry == industries[0]),  # Only show legend for first industry\n",
        "                boxpoints=\"outliers\",  # Only show outliers instead of all points\n",
        "                jitter=0.5,  # Increased jitter for better separation\n",
        "                pointpos=-1.8,\n",
        "                marker_color=colors[i % len(colors)],\n",
        "                line_color=colors[i % len(colors)],\n",
        "                marker_size=4,  # Smaller marker size\n",
        "                opacity=0.7,  # Add transparency\n",
        "                hovertemplate=f\"<b>{industry}</b><br>Employment: {emp_type}<br>Salary: $%{{y:,.0f}}<br>Count: {len(industry_data):,}<extra></extra>\"\n",
        "            ))\n",
        "\n",
        "fig1.update_layout(\n",
        "    title={\n",
        "        'text': \"Salary Distribution by Industry and Employment Type\",\n",
        "        'x': 0.5,\n",
        "        'font': {'size': 18, 'family': 'Arial, sans-serif'}\n",
        "    },\n",
        "    xaxis_title=\"Industry (NAICS2)\",\n",
        "    yaxis_title=\"Salary From ($)\",\n",
        "    xaxis=dict(\n",
        "        tickangle=-45,  # Better angle for readability\n",
        "        tickfont=dict(size=10),\n",
        "        showgrid=True,\n",
        "        gridcolor='rgba(0,0,0,0.1)',\n",
        "        automargin=True  # Auto-adjust margins for rotated labels\n",
        "    ),\n",
        "    yaxis=dict(\n",
        "        tickmode='linear',\n",
        "        tick0=0,\n",
        "        dtick=25000,  # $25K increments for better granularity\n",
        "        tickformat='$,.0f',\n",
        "        range=[0, 200000],  # Set range from 0 to $200K\n",
        "        showgrid=True,\n",
        "        gridcolor='rgba(0,0,0,0.1)',\n",
        "        tickfont=dict(size=10)\n",
        "    ),\n",
        "    height=700,\n",
        "    margin=dict(b=120),  # Extra bottom margin for rotated labels\n",
        "    plot_bgcolor='rgba(0,0,0,0.02)',\n",
        "    paper_bgcolor='white',\n",
        "    font=dict(family='Arial, sans-serif', size=11)\n",
        ")\n",
        "\n",
        "# Enhance with interactive features and export capabilities\n",
        "fig1 = enhance_plotly_figure(fig1, \"Salary Distribution by Industry and Employment Type\",\n",
        "                           \"01_salary_by_industry_employment\", width=1400, height=800)\n",
        "\n",
        "# Display figure based on output format\n",
        "display_figure(fig1, \"01_salary_by_industry_employment\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-industry-violin\n",
        "#| fig-cap: \"Alternative violin plot visualization showing salary distributions across industries and employment types. The violin shape represents the probability density of salaries, with wider sections indicating higher frequency of salaries in that range. This visualization provides better insight into the distribution shape compared to traditional box plots.\"\n",
        "\n",
        "fig1_violin = go.Figure()\n",
        "\n",
        "# Add violin plots for each employment type across industries\n",
        "for i, emp_type in enumerate(employment_types):\n",
        "    emp_data = salary_industry_filtered[salary_industry_filtered['EMPLOYMENT_TYPE_NAME'] == emp_type]\n",
        "\n",
        "    for industry in industries:\n",
        "        industry_data = emp_data[emp_data['NAICS2_NAME'] == industry]\n",
        "        if len(industry_data) > 0:\n",
        "            # Sample data if too many points to avoid overplotting\n",
        "            if len(industry_data) > 1000:\n",
        "                industry_data = industry_data.sample(n=1000, random_state=42)\n",
        "\n",
        "            fig1_violin.add_trace(go.Violin(\n",
        "                y=industry_data['SALARY_FROM'],\n",
        "                x=[industry] * len(industry_data),\n",
        "                name=emp_type,\n",
        "                legendgroup=emp_type,\n",
        "                showlegend=(industry == industries[0]),  # Only show legend for first industry\n",
        "                box_visible=True,  # Show box plot inside violin\n",
        "                meanline_visible=True,  # Show mean line\n",
        "                fillcolor=colors[i % len(colors)],\n",
        "                line_color=colors[i % len(colors)],\n",
        "                opacity=0.7,\n",
        "                hovertemplate=f\"<b>{industry}</b><br>Employment: {emp_type}<br>Salary: $%{{y:,.0f}}<br>Count: {len(industry_data):,}<extra></extra>\"\n",
        "            ))\n",
        "\n",
        "fig1_violin.update_layout(\n",
        "    title={\n",
        "        'text': \"Salary Distribution by Industry and Employment Type (Violin Plot)\",\n",
        "        'x': 0.5,\n",
        "        'font': {'size': 18, 'family': 'Arial, sans-serif'}\n",
        "    },\n",
        "    xaxis_title=\"Industry (NAICS2)\",\n",
        "    yaxis_title=\"Salary From ($)\",\n",
        "    xaxis=dict(\n",
        "        tickangle=-45,  # Better angle for readability\n",
        "        tickfont=dict(size=10),\n",
        "        showgrid=True,\n",
        "        gridcolor='rgba(0,0,0,0.1)',\n",
        "        automargin=True  # Auto-adjust margins for rotated labels\n",
        "    ),\n",
        "    yaxis=dict(\n",
        "        tickmode='linear',\n",
        "        tick0=0,\n",
        "        dtick=25000,  # $25K increments for better granularity\n",
        "        tickformat='$,.0f',\n",
        "        range=[0, 200000],  # Set range from 0 to $200K\n",
        "        showgrid=True,\n",
        "        gridcolor='rgba(0,0,0,0.1)',\n",
        "        tickfont=dict(size=10)\n",
        "    ),\n",
        "    height=700,\n",
        "    margin=dict(b=120),  # Extra bottom margin for rotated labels\n",
        "    plot_bgcolor='rgba(0,0,0,0.02)',\n",
        "    paper_bgcolor='white',\n",
        "    font=dict(family='Arial, sans-serif', size=11)\n",
        ")\n",
        "\n",
        "# Enhance violin plot with interactive features\n",
        "fig1_violin = enhance_plotly_figure(\n",
        "    fig1_violin,\n",
        "    \"Salary Distribution by Industry and Employment Type (Violin Plot)\",\n",
        "    \"01_salary_by_industry_employment_violin\",\n",
        "    width=1200,\n",
        "    height=700)\n",
        "\n",
        "display_figure(fig1_violin, \"01_salary_by_industry_employment_violin\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The box plot reveals significant salary variations across industries, with technology and professional services sectors demonstrating higher median compensation. Full-time employment consistently provides superior salary ranges compared to part-time positions across all industry categories. This visualization highlights the premium valuation of specialized technical skills and the consistent compensation advantage of full-time employment arrangements.\n",
        "\n",
        "The analysis demonstrates clear industry hierarchies in compensation, with information technology and professional services commanding the highest salary ranges. Manufacturing and retail sectors, while essential to the economy, offer comparatively lower compensation levels. This pattern reflects the market's valuation of specialized skills and the premium placed on intellectual capital in the modern economy.\n",
        "\n",
        "## Salary Distribution by Employment Type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-employment-type\n",
        "#| fig-cap: \"Interactive box plot comparing salary distributions across different employment types. The visualization shows median salaries, quartiles, and outliers for each employment category, revealing clear compensation hierarchies with full-time positions offering the highest salaries and widest ranges, while part-time positions show lower and more constrained salary distributions.\"\n",
        "\n",
        "# Create salary distribution by employment type only\n",
        "employment_salary_spark = df.select(\n",
        "    \"EMPLOYMENT_TYPE_NAME\",\n",
        "    \"SALARY_FROM\"\n",
        ").filter(\n",
        "    (col(\"SALARY_FROM\").isNotNull()) &\n",
        "    (col(\"SALARY_FROM\") > 0) &\n",
        "    (col(\"EMPLOYMENT_TYPE_NAME\").isNotNull())\n",
        ")\n",
        "\n",
        "print(f\"Records with valid employment type and salary data: {employment_salary_spark.count():,}\")\n",
        "\n",
        "# Convert to Pandas for visualization\n",
        "employment_salary_pd = employment_salary_spark.toPandas()\n",
        "\n",
        "# Get top employment types by frequency for readability\n",
        "# Option 1: Filter in pandas (current approach - simpler)\n",
        "top_employment_types = employment_salary_pd['EMPLOYMENT_TYPE_NAME'].value_counts().head(8).index\n",
        "employment_salary_filtered = employment_salary_pd[employment_salary_pd['EMPLOYMENT_TYPE_NAME'].isin(top_employment_types)]\n",
        "\n",
        "# Calculate frequency for each employment type to show in visualization\n",
        "employment_freq = employment_salary_pd['EMPLOYMENT_TYPE_NAME'].value_counts()\n",
        "employment_freq_filtered = employment_freq[employment_freq.index.isin(top_employment_types)]\n",
        "\n",
        "# Option 2: Filter in Spark first (more memory efficient for very large datasets)\n",
        "# from pyspark.sql.functions import count, desc\n",
        "# top_types_df = employment_salary_spark.groupBy(\"EMPLOYMENT_TYPE_NAME\") \\\n",
        "#     .agg(count(\"*\").alias(\"freq\")) \\\n",
        "#     .orderBy(desc(\"freq\")) \\\n",
        "#     .limit(8)\n",
        "# employment_salary_filtered_spark = employment_salary_spark.join(\n",
        "#     top_types_df.select(\"EMPLOYMENT_TYPE_NAME\"),\n",
        "#     \"EMPLOYMENT_TYPE_NAME\"\n",
        "# )\n",
        "# employment_salary_pd = employment_salary_filtered_spark.toPandas()  # Convert filtered data only\n",
        "\n",
        "# Create box plot using go.Box for consistency\n",
        "fig5 = go.Figure()\n",
        "\n",
        "# Add box plots for each employment type\n",
        "for i, emp_type in enumerate(top_employment_types):\n",
        "    emp_data = employment_salary_filtered[employment_salary_filtered['EMPLOYMENT_TYPE_NAME'] == emp_type]\n",
        "\n",
        "    # Sample data if too many points to avoid overplotting\n",
        "    if len(emp_data) > 2000:\n",
        "        emp_data = emp_data.sample(n=2000, random_state=42)\n",
        "\n",
        "    fig5.add_trace(go.Box(\n",
        "        y=emp_data['SALARY_FROM'],\n",
        "        x=[emp_type] * len(emp_data),\n",
        "        name=emp_type,\n",
        "        boxpoints=\"outliers\",  # Only show outliers instead of all points\n",
        "        jitter=0.5,  # Increased jitter for better separation\n",
        "        pointpos=-1.8,\n",
        "        marker_color=['#ec7424', '#a4abab', '#2E86AB', '#A23B72', '#F18F01', '#6B2D5C', '#A8DADC', '#457B9D'][i % 8],\n",
        "        marker_size=4,  # Smaller marker size\n",
        "        opacity=0.7,  # Add transparency\n",
        "        hovertemplate=f\"<b>{emp_type}</b><br>Salary: $%{{y:,.0f}}<br>Sample Size: {employment_freq_filtered[emp_type]:,}<extra></extra>\"\n",
        "    ))\n",
        "\n",
        "fig5.update_layout(\n",
        "    title={\n",
        "        'text': \"Salary Distribution by Employment Type\",\n",
        "        'x': 0.5,\n",
        "        'font': {'size': 18, 'family': 'Arial, sans-serif'}\n",
        "    },\n",
        "    xaxis_title=\"Employment Type\",\n",
        "    yaxis_title=\"Salary ($)\",\n",
        "    xaxis=dict(\n",
        "        tickangle=-45,  # Better angle for readability\n",
        "        tickfont=dict(size=10),\n",
        "        showgrid=True,\n",
        "        gridcolor='rgba(0,0,0,0.1)',\n",
        "        automargin=True  # Auto-adjust margins for rotated labels\n",
        "    ),\n",
        "    yaxis=dict(\n",
        "        tickformat='$,.0f',\n",
        "        showgrid=True,\n",
        "        gridcolor='rgba(0,0,0,0.1)',\n",
        "        tickfont=dict(size=10)\n",
        "    ),\n",
        "    height=600,\n",
        "    margin=dict(b=120),  # Extra bottom margin for rotated labels\n",
        "    plot_bgcolor='rgba(0,0,0,0.02)',\n",
        "    paper_bgcolor='white',\n",
        "    font=dict(family='Arial, sans-serif', size=11),\n",
        "    showlegend=False  # Hide legend since x-axis labels show employment types\n",
        ")\n",
        "\n",
        "# Add frequency information as annotations on the plot\n",
        "for i, emp_type in enumerate(top_employment_types):\n",
        "    freq = employment_freq_filtered[emp_type]\n",
        "    max_salary = employment_salary_filtered[employment_salary_filtered['EMPLOYMENT_TYPE_NAME'] == emp_type]['SALARY_FROM'].max()\n",
        "    fig5.add_annotation(\n",
        "        x=emp_type,\n",
        "        y=max_salary + 5000,\n",
        "        text=f\"n={freq:,}\",\n",
        "        showarrow=False,\n",
        "        font=dict(size=10, color=\"gray\"),\n",
        "        align=\"center\"\n",
        "    )\n",
        "\n",
        "# Enhance with interactive features and export capabilities\n",
        "fig5 = enhance_plotly_figure(fig5, \"Salary Distribution by Employment Type\",\n",
        "                           \"05_salary_by_employment_type\", width=1200, height=600)\n",
        "\n",
        "display_figure(fig5, \"05_salary_by_employment_type\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-employment-violin\n",
        "#| fig-cap: \"Alternative violin plot visualization for employment type salary distributions. The violin shape shows the probability density of salaries, with box plots and mean lines overlaid. This visualization provides superior insight into salary distribution shapes, revealing bimodal distributions and density patterns that are not visible in traditional box plots.\"\n",
        "\n",
        "fig5_violin = go.Figure()\n",
        "\n",
        "# Add violin plots for each employment type\n",
        "for i, emp_type in enumerate(top_employment_types):\n",
        "    emp_data = employment_salary_filtered[employment_salary_filtered['EMPLOYMENT_TYPE_NAME'] == emp_type]\n",
        "\n",
        "    # Sample data if too many points\n",
        "    if len(emp_data) > 2000:\n",
        "        emp_data = emp_data.sample(n=2000, random_state=42)\n",
        "\n",
        "    fig5_violin.add_trace(go.Violin(\n",
        "        y=emp_data['SALARY_FROM'],\n",
        "        x=[emp_type] * len(emp_data),\n",
        "        name=emp_type,\n",
        "        box_visible=True,  # Show box plot inside violin\n",
        "        meanline_visible=True,  # Show mean line\n",
        "        fillcolor=colors[i % len(colors)],\n",
        "        line_color=colors[i % len(colors)],\n",
        "        opacity=0.7,\n",
        "        hovertemplate=f\"<b>{emp_type}</b><br>Salary: $%{{y:,.0f}}<br>Sample Size: {employment_freq_filtered[emp_type]:,}<extra></extra>\"\n",
        "    ))\n",
        "\n",
        "fig5_violin.update_layout(\n",
        "    title={\n",
        "        'text': \"Salary Distribution by Employment Type (Violin Plot)\",\n",
        "        'x': 0.5,\n",
        "        'font': {'size': 18, 'family': 'Arial, sans-serif'}\n",
        "    },\n",
        "    xaxis_title=\"Employment Type\",\n",
        "    yaxis_title=\"Salary ($)\",\n",
        "    xaxis=dict(\n",
        "        tickangle=-45,\n",
        "        tickfont=dict(size=10),\n",
        "        showgrid=True,\n",
        "        gridcolor='rgba(0,0,0,0.1)',\n",
        "        automargin=True\n",
        "    ),\n",
        "    yaxis=dict(\n",
        "        tickformat='$,.0f',\n",
        "        showgrid=True,\n",
        "        gridcolor='rgba(0,0,0,0.1)',\n",
        "        tickfont=dict(size=10)\n",
        "    ),\n",
        "    height=600,\n",
        "    margin=dict(b=120),\n",
        "    plot_bgcolor='rgba(0,0,0,0.02)',\n",
        "    paper_bgcolor='white',\n",
        "    font=dict(family='Arial, sans-serif', size=11),\n",
        "    showlegend=False\n",
        ")\n",
        "\n",
        "# Enhance violin plot with interactive features\n",
        "fig5_violin = enhance_plotly_figure(fig5_violin, \"Salary Distribution by Employment Type (Violin Plot)\",\n",
        "                                  \"05_salary_by_employment_type_violin\", width=1200, height=600)\n",
        "\n",
        "display_figure(fig5_violin, \"05_salary_by_employment_type_violin\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The employment type analysis reveals clear compensation hierarchies across different work arrangements. Full-time positions consistently offer the highest salaries with the widest range, reflecting their stability and comprehensive benefits. Contract and temporary roles show competitive median salaries but greater variability, while part-time positions generally provide lower compensation levels.\n",
        "\n",
        "This focused view on employment type demonstrates how work arrangement structures directly impact earning potential. Full-time employment provides the most reliable path to higher compensation, while alternative arrangements offer flexibility at the cost of reduced salary stability and benefits.\n",
        "\n",
        "# Salary Analysis by ONET Occupation Type\n",
        "\n",
        "## Salary Analysis by ONET Occupation Type (Bubble Chart)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-onet-df\n",
        "#| fig-cap: Table with the learnings from the assignment video\n",
        "#| echo: false\n",
        "#| eval: false\n",
        "#| code-fold: true\n",
        "\n",
        "df.createOrReplaceTempView(\"job_postings\")\n",
        "\n",
        "salary_analysis = spark.sql(\"\"\"\n",
        "SELECT LOT_OCCUPATION_NAME AS Occupation_Name,\n",
        "       percentile_approx(SALARY_FROM, 0.5) AS median_salary,\n",
        "       COUNT(*) AS Job_Postings\n",
        "FROM job_postings\n",
        "WHERE LOT_OCCUPATION_NAME IS NOT NULL\n",
        "  AND SALARY_FROM IS NOT NULL\n",
        "  AND SALARY_FROM > 0\n",
        "GROUP BY LOT_OCCUPATION_NAME\n",
        "ORDER BY Job_Postings DESC\n",
        "LIMIT 10\n",
        "\"\"\")\n",
        "salary_analysis.show(truncate=False)\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "SELECT EMPLOYMENT_TYPE_NAME,\n",
        "       percentile_approx(SALARY_FROM, 0.5) AS median_salary,\n",
        "       COUNT(*) AS n\n",
        "FROM job_postings\n",
        "WHERE SALARY_FROM IS NOT NULL AND SALARY_FROM > 0\n",
        "GROUP BY EMPLOYMENT_TYPE_NAME\n",
        "ORDER BY n DESC\n",
        "\"\"\").show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-onet-bubble\n",
        "#| fig-cap: \"Enhanced interactive bubble chart analyzing salary patterns by LOT occupation types with improved occupation-specific salary imputation.\"\n",
        "\n",
        "\n",
        "# Enhanced LOT occupation analysis with IMPROVED salary imputation verification\n",
        "# STEP 1: First verify salary diversity after improved imputation\n",
        "# Probably I should have saved the cleansed df\n",
        "\n",
        "salary_diversity_check = df.select(\"LOT_OCCUPATION_NAME\", \"EMPLOYMENT_TYPE_NAME\", \"SALARY_FROM\", \"SALARY_TO\").filter(\n",
        "    (col(\"LOT_OCCUPATION_NAME\").isNotNull()) &\n",
        "    (col(\"EMPLOYMENT_TYPE_NAME\").isNotNull()) &\n",
        "    (col(\"SALARY_FROM\").isNotNull()) & (col(\"SALARY_TO\").isNotNull()) &\n",
        "    (col(\"SALARY_FROM\") > 0) & (col(\"SALARY_TO\") > 0)\n",
        ").groupBy(\"LOT_OCCUPATION_NAME\", \"EMPLOYMENT_TYPE_NAME\").agg(\n",
        "    median((col(\"SALARY_FROM\") + col(\"SALARY_TO\")) / 2).alias(\"median_salary\"),\n",
        "    count(\"*\").alias(\"job_count\")\n",
        ").filter(col(\"job_count\") >= 3).orderBy(col(\"median_salary\").desc())\n",
        "\n",
        "diversity_sample = salary_diversity_check.limit(10).toPandas()\n",
        "print(\"Sample salary diversity by occupation + employment type:\")\n",
        "for _, row in diversity_sample.iterrows():\n",
        "    print(f\"  {row['LOT_OCCUPATION_NAME']:<30} | {row['EMPLOYMENT_TYPE_NAME']:<15} | ${row['median_salary']:>8,.0f}\")\n",
        "\n",
        "# Check overall salary range\n",
        "salary_range = df.select(\"SALARY_FROM\", \"SALARY_TO\").filter(\n",
        "    (col(\"SALARY_FROM\").isNotNull()) & (col(\"SALARY_TO\").isNotNull()) &\n",
        "    (col(\"SALARY_FROM\") > 0) & (col(\"SALARY_TO\") > 0)\n",
        ").agg(\n",
        "    median(\"SALARY_FROM\").alias(\"median_from\"),\n",
        "    median(\"SALARY_TO\").alias(\"median_to\")\n",
        ").collect()[0]\n",
        "\n",
        "print(f\"\\nOverall salary range medians: ${salary_range['median_from']:,.0f} - ${salary_range['median_to']:,.0f}\")\n",
        "\n",
        "# STEP 2: Apply lenient filtering for LOT occupation analysis\n",
        "lot_occupation_spark = df.select(\n",
        "    \"LOT_OCCUPATION_NAME\",\n",
        "    \"SALARY_FROM\",\n",
        "    \"SALARY_TO\",\n",
        "    \"NAICS2_NAME\",\n",
        "    \"EMPLOYMENT_TYPE_NAME\",\n",
        "    \"MIN_YEARS_EXPERIENCE\",\n",
        "    \"MAX_YEARS_EXPERIENCE\"\n",
        ").filter(\n",
        "    (col(\"LOT_OCCUPATION_NAME\").isNotNull()) &\n",
        "    (col(\"SALARY_FROM\").isNotNull()) &\n",
        "    (col(\"SALARY_TO\").isNotNull()) &\n",
        "    (col(\"SALARY_FROM\") > 0) &\n",
        "    (col(\"SALARY_TO\") > 0) &\n",
        "    (col(\"SALARY_FROM\") <= 300000) &  # Keep reasonable salary cap\n",
        "    (col(\"SALARY_TO\") <= 300000)\n",
        "    # Removed industry and employment type filters for maximum diversity\n",
        ").withColumn(\n",
        "    \"avg_salary_range\", (col(\"SALARY_FROM\") + col(\"SALARY_TO\")) / 2\n",
        ").groupBy(\"LOT_OCCUPATION_NAME\").agg(\n",
        "    median(\"avg_salary_range\").alias(\"median_salary\"),\n",
        "    avg(\"avg_salary_range\").alias(\"avg_salary\"),\n",
        "    count(\"*\").alias(\"job_count\"),\n",
        "    countDistinct(\"NAICS2_NAME\").alias(\"industry_diversity\"),\n",
        "    countDistinct(\"EMPLOYMENT_TYPE_NAME\").alias(\"employment_diversity\"),\n",
        "    avg(\"MIN_YEARS_EXPERIENCE\").alias(\"avg_min_experience\"),\n",
        "    avg(\"MAX_YEARS_EXPERIENCE\").alias(\"avg_max_experience\")\n",
        ").filter(\n",
        "    col(\"job_count\") >= 3  # Very low threshold for maximum diversity\n",
        ").orderBy(col(\"job_count\").desc())  # Order by job count, not salary\n",
        "\n",
        "# Create one_Salary_spark as requested\n",
        "one_Salary_spark = lot_occupation_spark\n",
        "\n",
        "# Convert to Pandas\n",
        "lot_occupation_pd = lot_occupation_spark.toPandas()\n",
        "\n",
        "# Show TOP 25 for maximum diversity\n",
        "onet_top = lot_occupation_pd.head(25)\n",
        "print(f\"Total LOT occupation categories found: {len(lot_occupation_pd)}\")\n",
        "print(f\"Showing top {len(onet_top)} LOT occupations by job count:\\n\")\n",
        "\n",
        "for i, (_, row) in enumerate(onet_top.iterrows(), 1):\n",
        "    print(f\"{i:2d}. {row['LOT_OCCUPATION_NAME']:<35} | {row['job_count']:>6,} jobs | ${row['median_salary']:>8,.0f}\")\n",
        "\n",
        "print(f\"\\nSalary diversity verification:\")\n",
        "print(f\"Salary range: ${onet_top['median_salary'].min():,.0f} - ${onet_top['median_salary'].max():,.0f}\")\n",
        "print(f\"Unique salary values: {onet_top['median_salary'].nunique()}\")\n",
        "print(f\"Standard deviation: ${onet_top['median_salary'].std():,.0f}\")\n",
        "\n",
        "# Create enhanced interactive LOT occupation bubble chart with DIVERSE categories\n",
        "fig2 = go.Figure()\n",
        "\n",
        "# Create shortened labels for better readability of LOT occupation names\n",
        "onet_top['short_name'] = onet_top['LOT_OCCUPATION_NAME'].apply(\n",
        "    lambda x: x[:30] + '...' if len(x) > 30 else x\n",
        ")\n",
        "\n",
        "# Scale bubble sizes (area-based) so that job_count maps to perceptible bubble areas\n",
        "max_count = onet_top['job_count'].max() if len(onet_top) > 0 else 1\n",
        "# Target maximum marker diameter in pixels\n",
        "max_marker_size = 60\n",
        "# For sizemode='area' Plotly expects sizeref = 2. * max(size) / (max_marker_size ** 2)\n",
        "sizeref = 2. * max_count / (max_marker_size ** 2)\n",
        "\n",
        "fig2.add_trace(go.Scatter(\n",
        "    x=onet_top['short_name'],\n",
        "    y=onet_top['median_salary'],\n",
        "    mode='markers',  # Remove text labels to avoid overlap\n",
        "    marker=dict(\n",
        "        size=onet_top['job_count'],\n",
        "        sizemode='area',\n",
        "        sizeref=sizeref,\n",
        "        sizemin=6,\n",
        "        color=onet_top['job_count'],  # Color by job postings (number of job openings)\n",
        "        colorscale='YlOrRd',\n",
        "        colorbar=dict(\n",
        "            title=\"Number of Job Postings\",\n",
        "            tickformat=\",.0f\",\n",
        "            x=1.12,\n",
        "            len=0.8\n",
        "        ),\n",
        "        line=dict(width=1, color='darkblue'),\n",
        "        opacity=0.8,\n",
        "        showscale=True\n",
        "    ),\n",
        "    customdata=onet_top[['job_count', 'median_salary', 'avg_salary', 'industry_diversity',\n",
        "                        'employment_diversity', 'avg_min_experience', 'avg_max_experience', 'LOT_OCCUPATION_NAME']],\n",
        "    hovertemplate=(\n",
        "        '<b>%{customdata[7]}</b><br>' +\n",
        "        'Median Salary: $%{customdata[1]:,.0f}<br>' +\n",
        "        'Average Salary: $%{customdata[2]:,.0f}<br>' +\n",
        "        'Job Openings: %{customdata[0]:,}<br>' +\n",
        "        'Industry Diversity: %{customdata[3]:.0f} sectors<br>' +\n",
        "        'Employment Types: %{customdata[4]:.0f}<br>' +\n",
        "        'Experience Range: %{customdata[5]:.1f}-%{customdata[6]:.1f} years<br>' +\n",
        "        '<extra></extra>'\n",
        "    ),\n",
        "    name='Occupations'\n",
        "))\n",
        "\n",
        "# Enhanced interactive layout with export capabilities\n",
        "fig2.update_layout(\n",
        "    title={\n",
        "        'text': \"Diverse LOT Occupation Analysis: Salary vs Job Count Bubble Chart\",\n",
        "        'x': 0.5,\n",
        "        'font': {'size': 18, 'family': 'Arial, sans-serif', 'color': '#2C3E50'}\n",
        "    },\n",
        "    xaxis_title=\"LOT Occupation\",\n",
        "    yaxis_title=\"Median Salary ($)\",\n",
        "    xaxis=dict(\n",
        "        tickangle=-60,  # Steeper angle for better readability\n",
        "        tickfont=dict(size=9),\n",
        "        showgrid=True,\n",
        "        gridcolor='rgba(0,0,0,0.1)',\n",
        "        automargin=True,\n",
        "        categoryorder='total ascending',\n",
        "        tickmode='linear',  # Ensure all labels are shown\n",
        "        nticks=len(onet_top)  # Show all occupation labels\n",
        "    ),\n",
        "    yaxis=dict(\n",
        "        tickformat='$,.0f',\n",
        "        showgrid=True,\n",
        "        gridcolor='rgba(0,0,0,0.1)',\n",
        "        tickfont=dict(size=10),\n",
        "        range=[onet_top['median_salary'].min() * 0.9, onet_top['median_salary'].max() * 1.1]  # Better Y-axis range\n",
        "    ),\n",
        "    plot_bgcolor='rgba(0,0,0,0.02)',\n",
        "    paper_bgcolor='white',\n",
        "    height=800,  # Increased height for better label spacing\n",
        "    margin=dict(b=250, l=100, r=220, t=100),  # More margin for labels and colorbar\n",
        "    hoverlabel=dict(\n",
        "        bgcolor=\"white\",\n",
        "        font_size=11,\n",
        "        font_family=\"Arial, sans-serif\",\n",
        "        bordercolor=\"navy\"\n",
        "    ),\n",
        "    font=dict(family='Arial, sans-serif', size=11),\n",
        "    # Enhanced interactivity\n",
        "    dragmode='pan',\n",
        "    selectdirection='d',\n",
        "    showlegend=False  # Remove legend to save space\n",
        ")\n",
        "\n",
        "# Enhance with interactive features and export capabilities\n",
        "fig2 = enhance_plotly_figure(fig2, \"Enhanced Salary Analysis by ONET Occupation Type\",\n",
        "                           \"02_onet_bubble_chart\", width=1400, height=800)\n",
        "\n",
        "display_figure(fig2, \"02_onet_bubble_chart\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The bubble chart demonstrates that specialized management and technical occupations command the highest median salaries in the job market. Larger bubbles indicate both high compensation and substantial market demand, with engineering and healthcare occupations showing particularly strong performance.\n",
        "\n",
        "This visualization reveals the market's clear preference for specialized technical and leadership roles. Occupations requiring advanced technical skills or managerial responsibilities consistently command premium compensation, reflecting the economic value placed on expertise and decision-making capabilities. The bubble size represents job volume, showing that high-demand, high-paying occupations are both lucrative and plentiful, providing strong career opportunities for qualified candidates.\n",
        "\n",
        "The analysis underscores the importance of specialized skills training and continuous professional development in accessing the most rewarding career opportunities in today's knowledge-based economy.\n",
        "\n",
        "## Salary by Education Level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-education-analysis\n",
        "#| fig-cap: \"Interactive scatter plot analysis of salary relationships with education levels and experience. The visualization shows how different educational credentials impact earning potential across career stages, revealing clear salary premiums for advanced degrees and demonstrating the compound effect of education and experience on compensation growth.\"\n",
        "\n",
        "# Create education level groups as specified\n",
        "\n",
        "education_spark = df.select(\n",
        "    \"MIN_EDULEVELS_NAME\",\n",
        "    \"MAX_YEARS_EXPERIENCE\",\n",
        "    \"SALARY_FROM\",\n",
        "    \"LOT_V6_SPECIALIZED_OCCUPATION_NAME\"\n",
        ").filter(\n",
        "    (col(\"MIN_EDULEVELS_NAME\").isNotNull()) &\n",
        "    (col(\"MAX_YEARS_EXPERIENCE\").isNotNull()) &\n",
        "    (col(\"SALARY_FROM\").isNotNull()) &\n",
        "    (col(\"SALARY_FROM\") > 0) &\n",
        "    (col(\"MAX_YEARS_EXPERIENCE\") <= 30) &  # Remove outliers\n",
        "    (col(\"LOT_V6_SPECIALIZED_OCCUPATION_NAME\").isNotNull())\n",
        ")\n",
        "\n",
        "# Convert to Pandas for grouping and analysis\n",
        "education_pd = education_spark.toPandas()\n",
        "\n",
        "print(f\"Records for education analysis: {len(education_pd):,}\")\n",
        "\n",
        "# Create education groups as specified in assignment\n",
        "bachelor_or_lower = [\n",
        "    \"High school or equivalent\",\n",
        "    \"Some college courses\",\n",
        "    \"Certificate\",\n",
        "    \"Associate degree\",\n",
        "    \"Bachelor's degree\",\n",
        "    \"No Education Listed\"\n",
        "]\n",
        "\n",
        "masters_or_higher = [\n",
        "    \"Master's degree\",\n",
        "    \"Doctoral degree\",\n",
        "    \"Professional degree\"\n",
        "]\n",
        "\n",
        "# Apply education grouping\n",
        "education_pd['education_group'] = education_pd['MIN_EDULEVELS_NAME'].apply(\n",
        "    lambda x: \"Bachelor's or Lower\" if x in bachelor_or_lower else (\n",
        "        \"Master's or PhD\" if x in masters_or_higher else \"Other\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Filter to main groups\n",
        "education_main = education_pd[education_pd['education_group'].isin([\"Bachelor's or Lower\", \"Master's or PhD\"])]\n",
        "print(f\"Records in main education groups: {len(education_main):,}\")\n",
        "\n",
        "# Create the subplot structure with better spacing\n",
        "fig3 = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    subplot_titles=(\n",
        "        \"Bachelor's or Lower<br><sub>Experience vs Salary</sub>\",\n",
        "        \"Master's or PhD<br><sub>Experience vs Salary</sub>\",\n",
        "        \"Bachelor's or Lower<br><sub>Salary Distribution</sub>\",\n",
        "        \"Master's or PhD<br><sub>Salary Distribution</sub>\"\n",
        "    ),\n",
        "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "           [{\"secondary_y\": False}, {\"secondary_y\": False}]],\n",
        "    vertical_spacing=0.15,  # Increase vertical spacing between rows\n",
        "    horizontal_spacing=0.1   # Increase horizontal spacing between columns\n",
        ")\n",
        "\n",
        "# Add jitter to experience data as specified\n",
        "np.random.seed(42)\n",
        "education_main = education_main.copy()\n",
        "education_main['experience_jitter'] = education_main['MAX_YEARS_EXPERIENCE'] + np.random.normal(0, 0.3, len(education_main))\n",
        "\n",
        "# Filter data for each education group\n",
        "bachelor_data = education_main[education_main['education_group'] == \"Bachelor's or Lower\"]\n",
        "masters_data = education_main[education_main['education_group'] == \"Master's or PhD\"]\n",
        "\n",
        "print(f\"Bachelor's or Lower: {len(bachelor_data):,} records\")\n",
        "print(f\"Master's or PhD: {len(masters_data):,} records\")\n",
        "\n",
        "# Scatter plot for Bachelor's or Lower (using go.Scatter for consistency)\n",
        "if len(bachelor_data) > 0:\n",
        "    fig3.add_trace(\n",
        "        go.Scatter(\n",
        "            x=bachelor_data['experience_jitter'],\n",
        "            y=bachelor_data['SALARY_FROM'],\n",
        "            mode='markers',\n",
        "            name=\"Bachelor's or Lower\",\n",
        "            marker=dict(color='#3498DB', size=5, opacity=0.7),\n",
        "            hovertemplate='Experience: %{x:.1f} years<br>Salary: $%{y:,.0f}<extra></extra>'\n",
        "        ),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "# Scatter plot for Master's or PhD (using go.Scatter for consistency)\n",
        "if len(masters_data) > 0:\n",
        "    fig3.add_trace(\n",
        "        go.Scatter(\n",
        "            x=masters_data['experience_jitter'],\n",
        "            y=masters_data['SALARY_FROM'],\n",
        "            mode='markers',\n",
        "            name=\"Master's or PhD\",\n",
        "            marker=dict(color='#E74C3C', size=5, opacity=0.7),\n",
        "            hovertemplate='Experience: %{x:.1f} years<br>Salary: $%{y:,.0f}<extra></extra>'\n",
        "        ),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "# Histograms for salary distribution\n",
        "if len(bachelor_data) > 0:\n",
        "    fig3.add_trace(\n",
        "        go.Histogram(\n",
        "            x=bachelor_data['SALARY_FROM'],\n",
        "            name=\"Bachelor's Distribution\",\n",
        "            marker_color='#3498DB',\n",
        "            opacity=0.75,  # Slightly more opaque\n",
        "            nbinsx=30,     # More bins for better detail\n",
        "            showlegend=False\n",
        "        ),\n",
        "        row=2, col=1\n",
        "    )\n",
        "\n",
        "if len(masters_data) > 0:\n",
        "    fig3.add_trace(\n",
        "        go.Histogram(\n",
        "            x=masters_data['SALARY_FROM'],\n",
        "            name=\"Master's Distribution\",\n",
        "            marker_color='#E74C3C',\n",
        "            opacity=0.75,  # Slightly more opaque\n",
        "            nbinsx=30,     # More bins for better detail\n",
        "            showlegend=False\n",
        "        ),\n",
        "        row=2, col=2\n",
        "    )\n",
        "\n",
        "# Update layout with better spacing and readability\n",
        "fig3.update_layout(\n",
        "    title=\"Salary Analysis by Education Level\",\n",
        "    template=\"plotly_white\",\n",
        "    font_family=\"Helvetica\",\n",
        "    title_font_size=18,  # Slightly larger title\n",
        "    title_font_color=\"#2C3E50\",\n",
        "    plot_bgcolor=\"white\",\n",
        "    paper_bgcolor=\"white\",\n",
        "    height=1000,  # Increased height for better spacing\n",
        "    width=1400,   # Explicit width for better proportions\n",
        "    showlegend=False,\n",
        "    margin=dict(t=80, b=80, l=60, r=60)  # Add margins around the entire figure\n",
        ")\n",
        "\n",
        "# Update axes with better formatting and spacing\n",
        "# Row 1: Scatter plots\n",
        "fig3.update_xaxes(title_text=\"Years of Experience\", title_font_size=12, row=1, col=1)\n",
        "fig3.update_xaxes(title_text=\"Years of Experience\", title_font_size=12, row=1, col=2)\n",
        "fig3.update_yaxes(title_text=\"Salary ($)\", title_font_size=12, row=1, col=1)\n",
        "fig3.update_yaxes(title_text=\"Salary ($)\", title_font_size=12, row=1, col=2)\n",
        "\n",
        "# Row 2: Histograms\n",
        "fig3.update_xaxes(title_text=\"Salary ($)\", title_font_size=12, tickangle=0, row=2, col=1)\n",
        "fig3.update_xaxes(title_text=\"Salary ($)\", title_font_size=12, tickangle=0, row=2, col=2)\n",
        "fig3.update_yaxes(title_text=\"Frequency\", title_font_size=12, row=2, col=1)  # Changed to \"Frequency\" for clarity\n",
        "fig3.update_yaxes(title_text=\"Frequency\", title_font_size=12, row=2, col=2)\n",
        "\n",
        "# Improve subplot title formatting\n",
        "fig3.update_annotations(font_size=13, font_weight=\"bold\")\n",
        "\n",
        "fig3.update_layout(\n",
        "    # Enhanced interactivity\n",
        "    dragmode='pan',\n",
        "    selectdirection='d',\n",
        "\n",
        "    # Better mode bar with all export options\n",
        "    modebar_add=[\n",
        "        'zoomIn2d', 'zoomOut2d', 'pan2d', 'resetScale2d',\n",
        "        'toImage', 'downloadImage', 'toggleSpikelines'\n",
        "    ],\n",
        "    modebar_remove=['lasso2d', 'select2d', 'autoScale2d'],\n",
        "    modebar_orientation='v',\n",
        "    modebar_bgcolor='rgba(255,255,255,0.9)',\n",
        "    modebar_color='#2C3E50',\n",
        "\n",
        "    # Enhanced hover\n",
        "    hoverlabel=dict(\n",
        "        bgcolor=\"white\",\n",
        "        font_size=11,\n",
        "        font_family=\"Arial, sans-serif\",\n",
        "        bordercolor=\"navy\"\n",
        "    ),\n",
        "\n",
        "    # Responsive design\n",
        "    autosize=True,\n",
        "\n",
        "    # Mobile responsiveness\n",
        "    margin=dict(l=50, r=50, t=80, b=80, pad=4),\n",
        "\n",
        "    # Mobile-friendly font sizes\n",
        "    font=dict(\n",
        "        family=\"Arial, sans-serif\",\n",
        "        size=11,\n",
        "        color=\"#2C3E50\"\n",
        "    )\n",
        ")\n",
        "\n",
        "display_figure(fig3, \"03_education_level_analysis\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The scatter plots demonstrate that advanced degree holders consistently achieve higher salaries across all experience levels, with the compensation gap expanding significantly with increased tenure. The salary distribution histograms reveal that Master's and PhD holders exhibit higher median salaries and greater earning potential compared to Bachelor's degree recipients.\n",
        "\n",
        "This comprehensive analysis of education's impact on compensation reveals several key patterns. Advanced degree holders maintain a consistent salary premium throughout their careers, with the gap widening as experience increases. This suggests that higher education provides not just an initial salary boost, but also enhanced career progression and earning potential over time.\n",
        "\n",
        "The histograms show that while Bachelor's degree holders form the largest group, advanced degree recipients are positioned in higher salary brackets, indicating the market's valuation of specialized knowledge and advanced credentials. This pattern has significant implications for educational investment decisions and career planning strategies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-experience-vs-salary-education\n",
        "#| fig-cap: \"Experience vs Salary by Education Group. Scatter plot of years of experience (with jitter) vs salary, colored by education grouping (Associate or Lower, Bachelor, Master's, PhD).\"\n",
        "# Build dataset for experience vs salary by education group (compute avg salary safely)\n",
        "exp_spark = df.select(\n",
        "    \"MIN_EDULEVELS_NAME\",\n",
        "    col(\"MAX_YEARS_EXPERIENCE\").cast(\"float\").alias(\"MAX_YEARS_EXPERIENCE\"),\n",
        "    col(\"SALARY_FROM\"),\n",
        "    col(\"SALARY_TO\")\n",
        ").filter(\n",
        "    (col(\"MAX_YEARS_EXPERIENCE\").isNotNull()) &\n",
        "    (col(\"MAX_YEARS_EXPERIENCE\") > 0) &\n",
        "    (col(\"MAX_YEARS_EXPERIENCE\") <= 30)  # cap outliers\n",
        ")\n",
        "\n",
        "# Compute a robust average salary: prefer mean of SALARY_FROM and SALARY_TO when available\n",
        "exp_spark = exp_spark.withColumn(\n",
        "    \"Average_Salary\",\n",
        "    when((col(\"SALARY_FROM\").isNotNull()) & (col(\"SALARY_TO\").isNotNull()) & (col(\"SALARY_FROM\")>0) & (col(\"SALARY_TO\")>0),\n",
        "         (col(\"SALARY_FROM\") + col(\"SALARY_TO\")) / 2.0)\n",
        "    .when((col(\"SALARY_FROM\").isNotNull()) & (col(\"SALARY_FROM\")>0), col(\"SALARY_FROM\"))\n",
        "    .when((col(\"SALARY_TO\").isNotNull()) & (col(\"SALARY_TO\")>0), col(\"SALARY_TO\"))\n",
        "    .otherwise(None)\n",
        ")\n",
        "\n",
        "# Filter to valid salary rows\n",
        "exp_spark = exp_spark.filter((col(\"Average_Salary\").isNotNull()) & (col(\"Average_Salary\") > 0))\n",
        "\n",
        "# Create explicit education groups using regex (case-insensitive)\n",
        "\n",
        "# More comprehensive regexes to capture common variants (case-insensitive)\n",
        "lower_pattern = \"(?i)High school|Some college|Certificate|Associate|GED|No education listed|Secondary\"\n",
        "bachelor_pattern = \"(?i)Bachelor|Bachelors|BA|BS\"\n",
        "higher_pattern = \"(?i)Master|Masters|Professional|MSc|MA|MS\"\n",
        "# include dotted and spaced variants like 'Ph.D.' and 'Ph.D. or professional degree'\n",
        "phd_pattern = \"(?i)Doctoral|Doctorate|PhD|Ph\\.D\\.|DPhil|Doctor of|MD|M\\.D\\.|Doctor|Ph\\.D\"\n",
        "\n",
        "exp_spark = exp_spark.withColumn(\n",
        "    \"EDU_GROUP\",\n",
        "    when(col(\"MIN_EDULEVELS_NAME\").rlike(phd_pattern), \"PhD\")\n",
        "    .when(col(\"MIN_EDULEVELS_NAME\").rlike(higher_pattern), \"Master's\")\n",
        "    .when(col(\"MIN_EDULEVELS_NAME\").rlike(bachelor_pattern), \"Bachelor\")\n",
        "    .when(col(\"MIN_EDULEVELS_NAME\").rlike(lower_pattern), \"Associate or Lower\")\n",
        "    .otherwise(\"Other\")\n",
        ")\n",
        "\n",
        "# Bucket experience to integer years (so points cluster at integer x positions like your target figure)\n",
        "exp_spark = exp_spark.withColumn(\"Experience_Years\", floor(col(\"MAX_YEARS_EXPERIENCE\")).cast(\"int\"))\n",
        "\n",
        "# Keep only the 4 main education groups for the plot\n",
        "exp_spark = exp_spark.filter(col(\"EDU_GROUP\").isin(\"Associate or Lower\", \"Bachelor\", \"Master's\", \"PhD\"))\n",
        "\n",
        "# Convert to pandas for plotting\n",
        "\n",
        "exp_pd = exp_spark.select(\"MIN_EDULEVELS_NAME\", \"EDU_GROUP\", \"MAX_YEARS_EXPERIENCE\", \"Experience_Years\", \"Average_Salary\").toPandas()\n",
        "print(f\"Records for experience vs salary by education: {len(exp_pd):,}\")\n",
        "# Debug: show counts per education group so we can confirm PhD mapping\n",
        "print(\"EDU_GROUP counts:\\n\", exp_pd['EDU_GROUP'].value_counts(dropna=False))\n",
        "\n",
        "# Add small jitter around the integer experience buckets to reduce overplotting (stddev 0.18)\n",
        "np.random.seed(42)\n",
        "exp_pd['experience_jitter'] = exp_pd['Experience_Years'] + np.random.normal(0, 0.18, len(exp_pd))\n",
        "\n",
        "# Create scatter plot with Plotly Express using Average_Salary and EDU_GROUP\n",
        "category_order = [\"Associate or Lower\", \"Bachelor\", \"Master's\", \"PhD\"]\n",
        "color_map = {\n",
        "    \"Associate or Lower\": \"#636EFA\",\n",
        "    \"Bachelor\": \"#EF553B\",\n",
        "    \"Master's\": \"#00CC96\",\n",
        "    \"PhD\": \"#AB63FA\"\n",
        "}\n",
        "\n",
        "fig_exp = px.scatter(\n",
        "    exp_pd,\n",
        "    x='experience_jitter',\n",
        "    y='Average_Salary',\n",
        "    color='EDU_GROUP',\n",
        "    category_orders={'EDU_GROUP': category_order},\n",
        "    color_discrete_map=color_map,\n",
        "    labels={'experience_jitter': 'Years of Experience', 'Average_Salary': 'Average Salary (USD)'},\n",
        "    hover_data={'MIN_EDULEVELS_NAME': True, 'MAX_YEARS_EXPERIENCE': ':.1f', 'Average_Salary': ':.0f'},\n",
        "    title='Experience vs Salary by Education Level'\n",
        ")\n",
        "\n",
        "fig_exp.update_traces(\n",
        "    marker=dict(size=5, line=dict(width=0.15, color='DarkSlateGrey')), selector=dict(mode='markers')\n",
        ")\n",
        "\n",
        "fig_exp.update_layout(\n",
        "    template='plotly_white',\n",
        "    height=600,\n",
        "    width=1400,\n",
        "    margin=dict(t=80, b=60, l=80, r=80),\n",
        "    legend=dict(title='Education Group', orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1),\n",
        "    xaxis=dict(title='Years of Experience', tickmode='linear', dtick=1),\n",
        "    yaxis=dict(title='Average Salary (USD)', tickformat='$,.0f')\n",
        ")\n",
        "\n",
        "# Enhance and export\n",
        "fig_exp = enhance_plotly_figure(fig_exp, \"Experience vs Salary by Education Level\", \"06_experience_vs_salary_education\", width=1400, height=600)\n",
        "\n",
        "display_figure(fig_exp, \"06_experience_vs_salary_education\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Salary by Remote Work Type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-remote-work\n",
        "#| fig-cap: \"Interactive scatter plot analysis of salary distributions across different remote work arrangements.\"\n",
        "\n",
        "# Quick raw counts from `df` for debugging (counts are from the original Spark DataFrame before later filters)\n",
        "raw_remote_counts = (\n",
        "        df.select('REMOTE_TYPE_NAME')\n",
        "            .na.fill('[None]', subset=['REMOTE_TYPE_NAME'])\n",
        "            .groupBy('REMOTE_TYPE_NAME')\n",
        "            .count()\n",
        "            .orderBy(col('count').desc())\n",
        ")\n",
        "print('Top REMOTE_TYPE_NAME values (raw df):')\n",
        "raw_remote_counts.show(40, truncate=False)\n",
        "\n",
        "# Also show total counts per categorized group (Remote/Hybrid/Onsite) from raw df\n",
        "def categorize_remote_spark(col_expr):\n",
        "    # Prefer Hybrid when both words appear (e.g., 'Hybrid Remote') — check Hybrid first\n",
        "    return when((col_expr.isNull()) | (col_expr == '') | (col_expr == '[None]'), 'Onsite') \\\n",
        "        .when(col_expr.rlike('(?i)Hybrid'), 'Hybrid') \\\n",
        "        .when(col_expr.rlike('(?i)Remote'), 'Remote') \\\n",
        "        .otherwise('Onsite')\n",
        "\n",
        "cat_counts = (\n",
        "        df.select(categorize_remote_spark(col('REMOTE_TYPE_NAME')).alias('remote_cat'))\n",
        "            .groupBy('remote_cat')\n",
        "            .count()\n",
        ")\n",
        "print('\\nCategorized counts from raw df:')\n",
        "cat_counts.show(truncate=False)\n",
        "\n",
        "# Split into remote work groups as specified\n",
        "\n",
        "remote_spark = df.select(\n",
        "    \"REMOTE_TYPE_NAME\",\n",
        "    \"MAX_YEARS_EXPERIENCE\",\n",
        "    col(\"SALARY_FROM\"),\n",
        "    col(\"SALARY_TO\"),\n",
        "    \"LOT_V6_SPECIALIZED_OCCUPATION_NAME\"\n",
        ").filter(\n",
        "    (col(\"MAX_YEARS_EXPERIENCE\").isNotNull()) &\n",
        "    ((col(\"SALARY_FROM\").isNotNull()) | (col(\"SALARY_TO\").isNotNull())) &\n",
        "    ((col(\"SALARY_FROM\") > 0) | (col(\"SALARY_TO\") > 0)) &\n",
        "    (col(\"MAX_YEARS_EXPERIENCE\") <= 30) &\n",
        "    (col(\"LOT_V6_SPECIALIZED_OCCUPATION_NAME\").isNotNull())\n",
        ")\n",
        "\n",
        "# Convert to Pandas\n",
        "remote_pd = remote_spark.toPandas()\n",
        "\n",
        "print(f\"Records for remote work analysis: {len(remote_pd):,}\")\n",
        "\n",
        "# Create remote work categories as specified in assignment\n",
        "def categorize_remote(remote_type):\n",
        "    if pd.isna(remote_type) or remote_type == '[None]' or remote_type == '' or remote_type is None:\n",
        "        return 'Onsite'\n",
        "    # Prefer 'Hybrid' when both words appear (e.g., 'Hybrid Remote')\n",
        "    elif 'Hybrid' in str(remote_type):\n",
        "        return 'Hybrid'\n",
        "    elif 'Remote' in str(remote_type):\n",
        "        return 'Remote'\n",
        "    else:\n",
        "        return 'Onsite'\n",
        "\n",
        "remote_pd['remote_category'] = remote_pd['REMOTE_TYPE_NAME'].apply(categorize_remote)\n",
        "\n",
        "# Print distribution\n",
        "print(\"Remote work distribution:\")\n",
        "print(remote_pd['remote_category'].value_counts())\n",
        "\n",
        "# Create subplot structure with better spacing\n",
        "fig4 = make_subplots(\n",
        "    rows=2, cols=3,\n",
        "    subplot_titles=(\n",
        "        \"Remote - Experience vs Salary\",\n",
        "        \"Hybrid - Experience vs Salary\",\n",
        "        \"Onsite - Experience vs Salary\",\n",
        "        \"Remote - Salary Distribution\",\n",
        "        \"Hybrid - Salary Distribution\",\n",
        "        \"Onsite - Salary Distribution\"\n",
        "    ),\n",
        "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "           [{\"secondary_y\": False}, {\"secondary_y\": False}, {\"secondary_y\": False}]],\n",
        "    horizontal_spacing=0.12,  # Add horizontal spacing\n",
        "    vertical_spacing=0.15     # Add vertical spacing\n",
        ")\n",
        "\n",
        "# Add jitter as specified\n",
        "np.random.seed(42)\n",
        "remote_pd = remote_pd.copy()\n",
        "# Cast experience to float and add a smaller jitter (stddev=0.18) so points cluster by integer years\n",
        "remote_pd['MAX_YEARS_EXPERIENCE'] = remote_pd['MAX_YEARS_EXPERIENCE'].astype(float)\n",
        "remote_pd = remote_pd[remote_pd['MAX_YEARS_EXPERIENCE'] > 0]\n",
        "remote_pd['experience_years'] = np.floor(remote_pd['MAX_YEARS_EXPERIENCE']).astype(int)\n",
        "remote_pd['experience_jitter'] = remote_pd['experience_years'] + np.random.normal(-0.2, 0.2, len(remote_pd))\n",
        "\n",
        "# Compute Average_Salary (prefer mean of SALARY_FROM & SALARY_TO)\n",
        "def compute_avg_salary(row):\n",
        "    f = row.get('SALARY_FROM')\n",
        "    t = row.get('SALARY_TO')\n",
        "    if pd.notna(f) and pd.notna(t) and f > 0 and t > 0:\n",
        "        return (f + t) / 2.0\n",
        "    if pd.notna(f) and f > 0:\n",
        "        return f\n",
        "    if pd.notna(t) and t > 0:\n",
        "        return t\n",
        "    return None\n",
        "\n",
        "remote_pd['Average_Salary'] = remote_pd.apply(compute_avg_salary, axis=1)\n",
        "remote_pd = remote_pd[remote_pd['Average_Salary'].notna() & (remote_pd['Average_Salary'] > 0)]\n",
        "\n",
        "# Define colors for each remote type\n",
        "colors = {'Remote': '#27AE60', 'Hybrid': '#F39C12', 'Onsite': '#8E44AD'}\n",
        "\n",
        "# Create scatter plots and histograms for each remote type\n",
        "for i, remote_type in enumerate(['Remote', 'Hybrid', 'Onsite']):\n",
        "    data = remote_pd[remote_pd['remote_category'] == remote_type]\n",
        "\n",
        "    if len(data) > 0:\n",
        "        print(f\"{remote_type}: {len(data):,} records\")\n",
        "        # Add small vertical jitter to Average_Salary for plotting (-500..+500 uniform)\n",
        "        y_jitter = np.random.uniform(-500, 500, len(data))\n",
        "        data_plot_y = data['Average_Salary'].values + y_jitter\n",
        "\n",
        "        # Scatter plot with jitter (using go.Scatter). Keep the hover showing the true Average_Salary\n",
        "        fig4.add_trace(\n",
        "            go.Scatter(\n",
        "                x=data['experience_jitter'],\n",
        "                y=data_plot_y,\n",
        "                mode='markers',\n",
        "                name=f\"{remote_type}\",\n",
        "                marker=dict(color=colors[remote_type], size=5, opacity=0.75, line=dict(width=0.15, color='DarkSlateGrey')),\n",
        "                hovertemplate=f'{remote_type}<br>Occupation: %{{customdata[0]}}<br>Experience: %{{x:.1f}} years<br>Salary: $%{{customdata[1]:,.0f}}<extra></extra>',\n",
        "                customdata=np.stack([data['LOT_V6_SPECIALIZED_OCCUPATION_NAME'], data['Average_Salary']], axis=-1),\n",
        "                showlegend=False\n",
        "            ),\n",
        "            row=1, col=i+1\n",
        "        )\n",
        "\n",
        "        # Histogram\n",
        "        fig4.add_trace(\n",
        "            go.Histogram(\n",
        "                x=data['Average_Salary'],\n",
        "                name=f\"{remote_type} Distribution\",\n",
        "                marker_color=colors[remote_type],\n",
        "                opacity=0.7,\n",
        "                nbinsx=25,\n",
        "                showlegend=False\n",
        "            ),\n",
        "            row=2, col=i+1\n",
        "        )\n",
        "\n",
        "# Update layout with better spacing and readability\n",
        "fig4.update_layout(\n",
        "    title={\n",
        "        'text': \"Salary Analysis by Remote Work Type\",\n",
        "        'x': 0.5,\n",
        "        'font': {'size': 18, 'family': 'Arial, sans-serif', 'color': '#2C3E50'}\n",
        "    },\n",
        "    template=\"plotly_white\",\n",
        "    font_family=\"Arial, sans-serif\",\n",
        "    plot_bgcolor=\"white\",\n",
        "    paper_bgcolor=\"white\",\n",
        "    height=900,  # Increased height for better spacing\n",
        "    margin=dict(l=60, r=60, t=100, b=80, pad=10),  # Better margins\n",
        "    showlegend=False  # Remove legend to save space\n",
        ")\n",
        "\n",
        "# Update axes labels with better formatting\n",
        "for i in range(1,4):\n",
        "    fig4.update_xaxes(title_text='Years of Experience', row=1, col=i, tickmode='linear', dtick=1)\n",
        "    fig4.update_xaxes(title_text='Salary (USD)', row=2, col=i)\n",
        "    fig4.update_yaxes(title_text='Average Salary (USD)', row=1, col=i, tickformat='$,.0f')\n",
        "\n",
        "# Enhance and export\n",
        "fig4 = enhance_plotly_figure(fig4, \"Salary by Remote Work Type\", \"07_salary_by_remote_work_type\", width=1400, height=900)\n",
        "display_figure(fig4, \"07_salary_by_remote_work_type\")\n",
        "\n",
        "# Short descriptions / observations\n",
        "print('\\nObservations:')\n",
        "for remote_type in ['Remote','Hybrid','Onsite']:\n",
        "    subset = remote_pd[remote_pd['remote_category']==remote_type]\n",
        "    if len(subset)==0:\n",
        "        print(f\"{remote_type}: no data\")\n",
        "        continue\n",
        "    med = subset['Average_Salary'].median()\n",
        "    mean = subset['Average_Salary'].mean()\n",
        "    cnt = len(subset)\n",
        "    print(f\"{remote_type}: n={cnt:,}, median=${med:,.0f}, mean=${mean:,.0f}.\")\n",
        "    if remote_type=='Remote':\n",
        "        print(\"  Remote roles show a modest salary premium on average compared with Onsite, visible as higher median and more points in upper salary ranges.\")\n",
        "    if remote_type=='Hybrid':\n",
        "        print(\"  Hybrid roles fall between Remote and Onsite, often showing moderate salaries with similar spread.\")\n",
        "    if remote_type=='Onsite':\n",
        "        print(\"  Onsite roles have a wider concentration at lower salary bands, though senior onsite roles still reach high salaries.\")\n",
        "for i in range(1, 4):\n",
        "    # Scatter plot axes (top row)\n",
        "    fig4.update_xaxes(\n",
        "        title_text=\"Years of Experience\",\n",
        "        row=1, col=i,\n",
        "        title_font=dict(size=12),\n",
        "        tickfont=dict(size=10),\n",
        "        showgrid=True,\n",
        "        gridcolor='rgba(0,0,0,0.1)'\n",
        "    )\n",
        "    fig4.update_yaxes(\n",
        "        title_text=\"Salary ($)\",\n",
        "        row=1, col=i,\n",
        "        title_font=dict(size=12),\n",
        "        tickfont=dict(size=10),\n",
        "        showgrid=True,\n",
        "        gridcolor='rgba(0,0,0,0.1)',\n",
        "        tickformat='$,.0f'\n",
        "    )\n",
        "\n",
        "    # Histogram axes (bottom row)\n",
        "    fig4.update_xaxes(\n",
        "        title_text=\"Salary ($)\",\n",
        "        row=2, col=i,\n",
        "        title_font=dict(size=12),\n",
        "        tickfont=dict(size=10),\n",
        "        showgrid=True,\n",
        "        gridcolor='rgba(0,0,0,0.1)',\n",
        "        tickformat='$,.0f'\n",
        "    )\n",
        "    fig4.update_yaxes(\n",
        "        title_text=\"Count\",\n",
        "        row=2, col=i,\n",
        "        title_font=dict(size=12),\n",
        "        tickfont=dict(size=10),\n",
        "        showgrid=True,\n",
        "        gridcolor='rgba(0,0,0,0.1)'\n",
        "    )\n",
        "\n",
        "# Enhance with interactive features and export capabilities\n",
        "fig4 = enhance_plotly_figure(fig4, \"Salary Analysis by Remote Work Type\",\n",
        "                           \"04_remote_work_analysis\", width=1400, height=900)\n",
        "\n",
        "display_figure(fig4, \"04_remote_work_analysis\")\n",
        "\n",
        "print('\\n--- Remote group summaries (top-level) ---')\n",
        "\n",
        "if len(remote_pd) == 0:\n",
        "    print('remote_pd is empty (no rows after filtering)')\n",
        "else:\n",
        "    group_stats = remote_pd.groupby('remote_category').agg(\n",
        "        count=('Average_Salary','size'),\n",
        "        median_salary=('Average_Salary','median'),\n",
        "        mean_salary=('Average_Salary','mean')\n",
        "    ).reset_index()\n",
        "    print(group_stats.to_string(index=False))\n",
        "\n",
        "for cat in ['Remote','Hybrid','Onsite']:\n",
        "    subset = remote_pd[remote_pd['remote_category']==cat]\n",
        "    print(f\"\\nSample rows for {cat} (up to 5): {len(subset):,} rows total\")\n",
        "    if len(subset)>0:\n",
        "        try:\n",
        "            display(subset[['REMOTE_TYPE_NAME','LOT_V6_SPECIALIZED_OCCUPATION_NAME','MAX_YEARS_EXPERIENCE','experience_years','Average_Salary']].head(5))\n",
        "        except Exception:\n",
        "            print(subset[['REMOTE_TYPE_NAME','LOT_V6_SPECIALIZED_OCCUPATION_NAME','MAX_YEARS_EXPERIENCE','experience_years','Average_Salary']].head(5).to_string(index=False))\n",
        "    else:\n",
        "        print('  (no rows)')\n",
        "\n",
        "print('\\n--- Education group summaries (top-level) ---')\n",
        "if 'exp_pd' in globals() and isinstance(exp_pd, pd.DataFrame):\n",
        "    edu_stats = exp_pd.groupby('EDU_GROUP').agg(\n",
        "        count=('Average_Salary','size'),\n",
        "        median_salary=('Average_Salary','median')\n",
        "    ).reset_index()\n",
        "    print(edu_stats.to_string(index=False))\n",
        "    for g in exp_pd['EDU_GROUP'].unique():\n",
        "        sub = exp_pd[exp_pd['EDU_GROUP']==g]\n",
        "        print(f\"\\nSample rows for EDU_GROUP={g} (up to 5): {len(sub):,} rows total\")\n",
        "        try:\n",
        "            display(sub[['MIN_EDULEVELS_NAME','MAX_YEARS_EXPERIENCE','Experience_Years','Average_Salary']].head(5))\n",
        "        except Exception:\n",
        "            print(sub[['MIN_EDULEVELS_NAME','MAX_YEARS_EXPERIENCE','Experience_Years','Average_Salary']].head(5).to_string(index=False))\n",
        "else:\n",
        "    print('exp_pd not available for education debug (it may be in a different scope).')\n",
        "\n",
        "# Summary statistics by remote type (original)\n",
        "print(\"\\nSalary summary by remote work type:\")\n",
        "for remote_type in ['Remote', 'Hybrid', 'Onsite']:\n",
        "    data = remote_pd[remote_pd['remote_category'] == remote_type]['Average_Salary']\n",
        "    if len(data) > 0:\n",
        "        print(f\"{remote_type}: Mean=${data.mean():,.0f}, Median=${data.median():,.0f}, Count={len(data):,}\")\n",
        "\n",
        "# Combined scatter: MAX_YEARS_EXPERIENCE vs Average_Salary colored by remote group\n",
        "# Use a small horizontal jitter on experience and a small vertical jitter on salary (-500..500)\n",
        "print('\\nBuilding combined scatter: MAX_YEARS_EXPERIENCE vs Average_Salary (colored by remote group)')\n",
        "combined_pd = remote_pd.copy()\n",
        "combined_pd['MAX_YEARS_EXPERIENCE'] = combined_pd['MAX_YEARS_EXPERIENCE'].astype(float)\n",
        "combined_pd = combined_pd[combined_pd['MAX_YEARS_EXPERIENCE'] > 0]\n",
        "\n",
        "np.random.seed(42)\n",
        "combined_pd['x_jitter'] = combined_pd['MAX_YEARS_EXPERIENCE'] + np.random.normal(0, 0.12, len(combined_pd))\n",
        "combined_pd['y_jitter'] = combined_pd['Average_Salary'] + np.random.uniform(-500, 500, len(combined_pd))\n",
        "\n",
        "fig_comb = px.scatter(\n",
        "    combined_pd,\n",
        "    x='x_jitter',\n",
        "    y='y_jitter',\n",
        "    color='remote_category',\n",
        "    color_discrete_map={'Remote': '#27AE60', 'Hybrid': '#F39C12', 'Onsite': '#8E44AD'},\n",
        "    labels={'x_jitter': 'Years of Experience (jittered)', 'y_jitter': 'Average Salary (USD)', 'remote_category': 'Remote Work Type'},\n",
        "    hover_data={'LOT_V6_SPECIALIZED_OCCUPATION_NAME': True, 'MAX_YEARS_EXPERIENCE': ':.1f', 'Average_Salary': ':.0f'},\n",
        "    opacity=0.75,\n",
        "    height=600,\n",
        "    width=1400\n",
        ")\n",
        "\n",
        "fig_comb.update_traces(marker=dict(size=5, line=dict(width=0.15, color='DarkSlateGrey')))\n",
        "fig_comb.update_layout(title='Experience vs Average Salary by Remote Work Type', xaxis_title='Years of Experience', yaxis_title='Average Salary (USD)', xaxis=dict(tickmode='linear', dtick=1), yaxis=dict(tickformat='$,.0f'))\n",
        "\n",
        "# Export and display\n",
        "fig_comb = enhance_plotly_figure(fig_comb, 'Experience vs Average Salary by Remote Work Type (combined)', '07b_experience_vs_salary_by_remote', width=1400, height=600)\n",
        "display_figure(fig_comb, '07b_experience_vs_salary_by_remote')\n",
        "\n",
        "# Short summary for the combined scatter\n",
        "print('\\nCombined scatter summary:')\n",
        "for rc in ['Remote','Hybrid','Onsite']:\n",
        "    sub = combined_pd[combined_pd['remote_category']==rc]\n",
        "    if len(sub)==0:\n",
        "        print(f\"{rc}: no rows after filtering\")\n",
        "        continue\n",
        "    print(f\"{rc}: n={len(sub):,}, median=${sub['Average_Salary'].median():,.0f}, mean=${sub['Average_Salary'].mean():,.0f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remote work positions demonstrate the highest salary potential with considerable variance, indicating premium compensation for distributed work capabilities. Hybrid arrangements provide competitive salaries while maintaining work flexibility, whereas onsite positions cluster around lower median compensation levels.\n",
        "\n",
        "The analysis reveals a clear hierarchy in remote work compensation, with fully remote positions commanding the highest salaries. This premium likely reflects the specialized skills required for effective remote collaboration and the market's recognition of the value of location-independent work capabilities.\n",
        "\n",
        "Hybrid arrangements offer a balanced approach, providing competitive compensation while maintaining some degree of workplace interaction. Onsite positions, while essential for many roles, generally offer lower compensation levels, reflecting the traditional employment model's baseline expectations.\n",
        "\n",
        "These findings have significant implications for work arrangement preferences and compensation negotiations in an increasingly flexible labor market."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/samarthya/sourcebox/github.com/assignment-02-samarthya/venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}